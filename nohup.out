WARNING:tensorflow:From /home/gxdai/Focal-Contrastive-Loss/model.py:202: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.get_regularization_losses instead.
2018-10-22 10:22:13.520905: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-22 10:22:14.428501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 11.91GiB freeMemory: 54.06MiB
2018-10-22 10:22:14.428547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
ALL the args information
Namespace(batch_size=32, ckpt_dir='./models/siamese', class_num=5, display_step=20, dropout_keep_prob=0.5, embedding_size=128, eval_step=10, evaluation=0, focal_decay_factor=2.0, height=512, image_txt='/data1/Guoxian_Dai/CUB_200_2011/images.txt', label_txt='/data1/Guoxian_Dai/CUB_200_2011/image_class_labels.txt', learning_rate=0.001, learning_rate2=0.0001, learning_rate_decay_type='exponential', loss_type='contrastive_loss', margin=1.0, mode='train', momentum=0.9, num_epochs1=20, num_epochs2=10, num_epochs_per_decay=5, num_workers=4, optimizer='rmsprop', pair_type='vector', pretrained_model_path='./weights/inception_v3.ckpt', restore_ckpt=0, root_dir='/data1/Guoxian_Dai/CUB_200_2011/images', targetNum=1000, train_test_split_txt='/data1/Guoxian_Dai/CUB_200_2011/train_test_split.txt', weightFile='./models/my-model', weight_decay=0.0005, width=512, with_regularizer=False)
Configuration information
********************

optimimizer = rmsprop             
learning_rate =    0.00100
learning_rate_decay_type = exponential         
loss_type = contrastive_loss    
margin =                  1.0
with_regularizer =                    0
focal_decay_factor =                  2.0
training image number is 5864
testing image number is 5924
inputs.get_shape().as_list() = [None, 1, 1, 1000]
embedding_vector.get_shape().as_list() = [None, 1, 1, 256]
embedding_vector.get_shape().as_list() = [None, 1, 1, 64]
inputs.get_shape().as_list() = [None, 1, 1, 1000]
embedding_vector.get_shape().as_list() = [None, 1, 1, 256]
embedding_vector.get_shape().as_list() = [None, 1, 1, 64]

		***********************
		the pair_type is vector
		***********************


		************************
		*Build contrastive loss*
		******************************
Traceback (most recent call last):
  File "main.py", line 97, in <module>
    main(args)
  File "main.py", line 88, in main
    with tf.Session(config=config) as sess:
  File "/home/gxdai/py2/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1511, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/home/gxdai/py2/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 634, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
WARNING:tensorflow:From /home/gxdai/Focal-Contrastive-Loss/model.py:202: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.get_regularization_losses instead.
2018-10-22 10:27:27.404496: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-22 10:27:28.188127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0a:00.0
totalMemory: 11.91GiB freeMemory: 11.74GiB
2018-10-22 10:27:28.188169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2018-10-22 10:27:28.453386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-22 10:27:28.453461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2018-10-22 10:27:28.453474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2018-10-22 10:27:28.453939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11361 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:0a:00.0, compute capability: 6.1)
ALL the args information
Namespace(batch_size=32, ckpt_dir='./models/siamese', class_num=5, display_step=20, dropout_keep_prob=0.5, embedding_size=128, eval_step=10, evaluation=0, focal_decay_factor=2.0, height=512, image_txt='/data1/Guoxian_Dai/CUB_200_2011/images.txt', label_txt='/data1/Guoxian_Dai/CUB_200_2011/image_class_labels.txt', learning_rate=0.001, learning_rate2=0.0001, learning_rate_decay_type='exponential', loss_type='contrastive_loss', margin=1.0, mode='train', momentum=0.9, num_epochs1=20, num_epochs2=10, num_epochs_per_decay=5, num_workers=4, optimizer='rmsprop', pair_type='vector', pretrained_model_path='./weights/inception_v3.ckpt', restore_ckpt=0, root_dir='/data1/Guoxian_Dai/CUB_200_2011/images', targetNum=1000, train_test_split_txt='/data1/Guoxian_Dai/CUB_200_2011/train_test_split.txt', weightFile='./models/my-model', weight_decay=0.0005, width=512, with_regularizer=False)
Configuration information
********************

optimimizer = rmsprop             
learning_rate =    0.00100
learning_rate_decay_type = exponential         
loss_type = contrastive_loss    
margin =                  1.0
with_regularizer =                    0
focal_decay_factor =                  2.0
training image number is 5864
testing image number is 5924
inputs.get_shape().as_list() = [None, 1, 1, 1000]
embedding_vector.get_shape().as_list() = [None, 1, 1, 256]
embedding_vector.get_shape().as_list() = [None, 1, 1, 64]
inputs.get_shape().as_list() = [None, 1, 1, 1000]
embedding_vector.get_shape().as_list() = [None, 1, 1, 256]
embedding_vector.get_shape().as_list() = [None, 1, 1, 64]

		***********************
		the pair_type is vector
		***********************


		************************
		*Build contrastive loss*
		******************************
***	InceptionV3/Conv2d_1a_3x3/weights:0	******
***	InceptionV3/Conv2d_1a_3x3/BatchNorm/beta:0	******
***	InceptionV3/Conv2d_2a_3x3/weights:0	******
***	InceptionV3/Conv2d_2a_3x3/BatchNorm/beta:0	******
***	InceptionV3/Conv2d_2b_3x3/weights:0	******
***	InceptionV3/Conv2d_2b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Conv2d_3b_1x1/weights:0	******
***	InceptionV3/Conv2d_3b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Conv2d_4a_3x3/weights:0	******
***	InceptionV3/Conv2d_4a_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights:0	******
***	InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights:0	******
***	InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights:0	******
***	InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights:0	******
***	InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights:0	******
***	InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights:0	******
***	InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights:0	******
***	InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights:0	******
***	InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/weights:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights:0	******
***	InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights:0	******
***	InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/weights:0	******
***	InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/weights:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/weights:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0c_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0d_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights:0	******
***	InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/weights:0	******
***	InceptionV3/Mixed_7a/Branch_0/Conv2d_1a_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/weights:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/weights:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/weights:0	******
***	InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/weights:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_2/Conv2d_0d_3x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/weights:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0b_1x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/weights:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta:0	******
***	InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/weights:0	******
***	InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0	******
***	InceptionV3/AuxLogits/Conv2d_1b_1x1/weights:0	******
***	InceptionV3/AuxLogits/Conv2d_1b_1x1/BatchNorm/beta:0	******
***	InceptionV3/AuxLogits/Conv2d_2a_5x5/weights:0	******
***	InceptionV3/AuxLogits/Conv2d_2a_5x5/BatchNorm/beta:0	******
***	InceptionV3/AuxLogits/Conv2d_2b_1x1/weights:0	******
***	InceptionV3/AuxLogits/Conv2d_2b_1x1/biases:0	******
***	InceptionV3/Logits/Conv2d_1c_1x1/weights:0	******
***	InceptionV3/Logits/Conv2d_1c_1x1/biases:0	******
***	InceptionV3/embedding/Conv2d_1c_1x1/weights:0	******
***	InceptionV3/embedding/Conv2d_1c_1x1/weights:0	******
***	InceptionV3/embedding/Conv2d_1c_1x1/biases:0	******
***	InceptionV3/embedding/Conv2d_1c_1x1/biases:0	******
***	InceptionV3/embedding/Conv2d_1d_1x1/weights:0	******
***	InceptionV3/embedding/Conv2d_1d_1x1/weights:0	******
***	InceptionV3/embedding/Conv2d_1d_1x1/biases:0	******
***	InceptionV3/embedding/Conv2d_1d_1x1/biases:0	******
Recall@1: 0.29743
Recall@2: 0.40834
Recall@4: 0.53393
Recall@8: 0.66745
Recall@16: 0.79136
Recall@32: 0.88049
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 27.  94.  47.  94.  50. 106.  83. 107.  48.  33.  90. 113.  55.  41.
  53.  72.  44.  33.  14.  78.  58.  44.  42.  93.  60.  74.  66.  16.
  28.  74.  78.  77.  64.  89.  81.  47.  53.  33.  39.  65.  42.  41.
  30.  96.  89.  71.  91.  39. 117.  59.  77.  73.  48.  54.  18.  46.
  59.  72.  52.  26.  32.  81.  62.  75.  28.  29.  47.  92.  47.  44.
  73.  50.  76. 108.  61.  62.  80.  91.  99.  22.  78.  34.  75.  41.
  48.  16.   9.  85.  79.  38.  83.  37.  52.  55.  10.  11.  38.  80.
  96.  37.]
Purity is 0.228
count_cross = [[0. 0. 0. ... 0. 0. 0.]
 [0. 2. 0. ... 0. 0. 6.]
 [0. 0. 1. ... 0. 0. 0.]
 ...
 [0. 9. 5. ... 1. 0. 0.]
 [0. 0. 0. ... 0. 0. 6.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 1.98683
5924.0
5924
Entropy cluster is 4.50334
Entropy class is 4.60444
normalized_mutual_information is 0.43629
tp_and_fp = 205810.0
tp = 20019.0
fp is 185791.0
fn is 152731.0
RI is 0.9807043189762656
Precision is 0.09726932607745008
Recall is 0.11588422575976845
F_1 is 0.10576394759087067

normalized_mutual_information = 0.43629366693315863
RI = 0.9807043189762656
F_1 = 0.10576394759087067

The NN is 0.29743
The FT is 0.12791
The ST is 0.19360
The DCG is 0.50845
The E is 0.11098
The MAP 0.09559

2018-10-22 10:29:47.095760: Epoch [  0/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29028, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:29:57.102962: Epoch [  0/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29028, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:30:07.106327: Epoch [  0/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29029, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:30:17.173127: Epoch [  0/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29029, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:30:27.268290: Epoch [  0/1000] [100/183], total loss: 0.00000, regularization loss: 0.29029, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:30:37.364981: Epoch [  0/1000] [120/183], total loss: 0.00307, regularization loss: 0.29028, contrastive loss: 0.00307, Loss positive: 0.00000, Loss negative: 0.00307
2018-10-22 10:30:47.515846: Epoch [  0/1000] [140/183], total loss: 0.00014, regularization loss: 0.29028, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 10:30:57.701827: Epoch [  0/1000] [160/183], total loss: 0.00000, regularization loss: 0.29028, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:31:07.874505: Epoch [  0/1000] [180/183], total loss: 0.00064, regularization loss: 0.29028, contrastive loss: 0.00064, Loss positive: 0.00000, Loss negative: 0.00064
Recall@1: 0.07056
Recall@2: 0.11006
Recall@4: 0.18720
Recall@8: 0.29321
Recall@16: 0.43079
Recall@32: 0.59993
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 29.  28. 116. 107.  72.  30.  62.  31.  65.  57.  63.  81.  55.  58.
  44.  93.  70.  83.  95.  78.  64.  50.  50.  67.  60.  47.  30.  66.
  66.  61.  81.  43.  31.  73.  42.  55.  64. 111.  50.  44.  27.  54.
  65.  19.  87.  47.  54.  41.  76.  44.  41.  30.  61.  91. 103.  91.
  46. 108.  79.  42.  84.  47.  69.  57.  60.  33.  57.  42.  46.  57.
  69.  65.  56. 100.  39. 101.  55.  42.  68.  15.  16.  50.  29.  38.
  55.  73.  86.  20.  66.  64.  48.  33.  83.  97.  41.  61.  31.  81.
  33.  79.]
Purity is 0.117
count_cross = [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 1. 1. 0.]
 ...
 [0. 0. 1. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 3. 3. ... 0. 1. 0.]]
Mutual information is 1.31250
5924.0
5924
Entropy cluster is 4.52909
Entropy class is 4.60444
normalized_mutual_information is 0.28740
tp_and_fp = 198714.0
tp = 6438.0
fp is 192276.0
fn is 166312.0
RI is 0.9795605613019571
Precision is 0.032398321205350404
Recall is 0.037267727930535455
F_1 is 0.034662847543772746

normalized_mutual_information = 0.28740154896775394
RI = 0.9795605613019571
F_1 = 0.034662847543772746

The NN is 0.07056
The FT is 0.04282
The ST is 0.07334
The DCG is 0.40824
The E is 0.03439
The MAP 0.03189

2018-10-22 10:32:53.637476: Epoch [  1/1000] [ 20/183], total loss: 0.00258, regularization loss: 0.29028, contrastive loss: 0.00258, Loss positive: 0.00000, Loss negative: 0.00258
2018-10-22 10:33:03.738269: Epoch [  1/1000] [ 40/183], total loss: 0.00034, regularization loss: 0.29028, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 10:33:13.903756: Epoch [  1/1000] [ 60/183], total loss: 0.04459, regularization loss: 0.29028, contrastive loss: 0.04459, Loss positive: 0.03153, Loss negative: 0.01307
2018-10-22 10:33:24.093029: Epoch [  1/1000] [ 80/183], total loss: 0.00172, regularization loss: 0.29027, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 10:33:34.306193: Epoch [  1/1000] [100/183], total loss: 0.00075, regularization loss: 0.29027, contrastive loss: 0.00075, Loss positive: 0.00000, Loss negative: 0.00075
2018-10-22 10:33:44.513736: Epoch [  1/1000] [120/183], total loss: 0.00000, regularization loss: 0.29027, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:33:54.718783: Epoch [  1/1000] [140/183], total loss: 0.00090, regularization loss: 0.29027, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 10:34:04.955308: Epoch [  1/1000] [160/183], total loss: 0.08142, regularization loss: 0.29027, contrastive loss: 0.08142, Loss positive: 0.06984, Loss negative: 0.01158
2018-10-22 10:34:15.188704: Epoch [  1/1000] [180/183], total loss: 0.15240, regularization loss: 0.29027, contrastive loss: 0.15240, Loss positive: 0.15137, Loss negative: 0.00103
2018-10-22 10:34:39.161866: Epoch [  2/1000] [ 20/183], total loss: 0.04136, regularization loss: 0.29027, contrastive loss: 0.04136, Loss positive: 0.04012, Loss negative: 0.00124
2018-10-22 10:34:49.393309: Epoch [  2/1000] [ 40/183], total loss: 0.21839, regularization loss: 0.29027, contrastive loss: 0.21839, Loss positive: 0.21531, Loss negative: 0.00308
2018-10-22 10:34:59.562422: Epoch [  2/1000] [ 60/183], total loss: 0.13641, regularization loss: 0.29027, contrastive loss: 0.13641, Loss positive: 0.12889, Loss negative: 0.00752
2018-10-22 10:35:09.691310: Epoch [  2/1000] [ 80/183], total loss: 0.00438, regularization loss: 0.29027, contrastive loss: 0.00438, Loss positive: 0.00000, Loss negative: 0.00438
2018-10-22 10:35:19.876170: Epoch [  2/1000] [100/183], total loss: 0.00606, regularization loss: 0.29027, contrastive loss: 0.00606, Loss positive: 0.00000, Loss negative: 0.00606
2018-10-22 10:35:30.081464: Epoch [  2/1000] [120/183], total loss: 0.00000, regularization loss: 0.29027, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:35:40.368954: Epoch [  2/1000] [140/183], total loss: 0.01376, regularization loss: 0.29027, contrastive loss: 0.01376, Loss positive: 0.01235, Loss negative: 0.00141
2018-10-22 10:35:50.590202: Epoch [  2/1000] [160/183], total loss: 0.00000, regularization loss: 0.29027, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:36:00.844219: Epoch [  2/1000] [180/183], total loss: 0.00077, regularization loss: 0.29027, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 10:36:23.352372: Epoch [  3/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29027, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:36:33.467454: Epoch [  3/1000] [ 40/183], total loss: 0.00705, regularization loss: 0.29027, contrastive loss: 0.00705, Loss positive: 0.00000, Loss negative: 0.00705
2018-10-22 10:36:43.630359: Epoch [  3/1000] [ 60/183], total loss: 0.00014, regularization loss: 0.29027, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 10:36:53.850405: Epoch [  3/1000] [ 80/183], total loss: 0.00266, regularization loss: 0.29027, contrastive loss: 0.00266, Loss positive: 0.00000, Loss negative: 0.00266
2018-10-22 10:37:04.077053: Epoch [  3/1000] [100/183], total loss: 0.02895, regularization loss: 0.29027, contrastive loss: 0.02895, Loss positive: 0.02815, Loss negative: 0.00080
2018-10-22 10:37:14.325432: Epoch [  3/1000] [120/183], total loss: 0.00285, regularization loss: 0.29027, contrastive loss: 0.00285, Loss positive: 0.00000, Loss negative: 0.00285
2018-10-22 10:37:24.649927: Epoch [  3/1000] [140/183], total loss: 0.07570, regularization loss: 0.29027, contrastive loss: 0.07570, Loss positive: 0.07459, Loss negative: 0.00111
2018-10-22 10:37:35.032088: Epoch [  3/1000] [160/183], total loss: 0.00668, regularization loss: 0.29027, contrastive loss: 0.00668, Loss positive: 0.00000, Loss negative: 0.00668
2018-10-22 10:37:45.292068: Epoch [  3/1000] [180/183], total loss: 0.02530, regularization loss: 0.29027, contrastive loss: 0.02530, Loss positive: 0.01414, Loss negative: 0.01116
2018-10-22 10:38:07.530263: Epoch [  4/1000] [ 20/183], total loss: 0.00004, regularization loss: 0.29027, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 10:38:17.717332: Epoch [  4/1000] [ 40/183], total loss: 0.00358, regularization loss: 0.29026, contrastive loss: 0.00358, Loss positive: 0.00000, Loss negative: 0.00358
2018-10-22 10:38:27.915274: Epoch [  4/1000] [ 60/183], total loss: 0.00241, regularization loss: 0.29026, contrastive loss: 0.00241, Loss positive: 0.00000, Loss negative: 0.00241
2018-10-22 10:38:38.151377: Epoch [  4/1000] [ 80/183], total loss: 0.00867, regularization loss: 0.29026, contrastive loss: 0.00867, Loss positive: 0.00000, Loss negative: 0.00867
2018-10-22 10:38:48.368553: Epoch [  4/1000] [100/183], total loss: 0.00340, regularization loss: 0.29027, contrastive loss: 0.00340, Loss positive: 0.00000, Loss negative: 0.00340
2018-10-22 10:38:58.580593: Epoch [  4/1000] [120/183], total loss: 0.00243, regularization loss: 0.29026, contrastive loss: 0.00243, Loss positive: 0.00000, Loss negative: 0.00243
2018-10-22 10:39:09.052658: Epoch [  4/1000] [140/183], total loss: 0.00285, regularization loss: 0.29026, contrastive loss: 0.00285, Loss positive: 0.00000, Loss negative: 0.00285
2018-10-22 10:39:19.464916: Epoch [  4/1000] [160/183], total loss: 0.00000, regularization loss: 0.29026, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:39:29.810717: Epoch [  4/1000] [180/183], total loss: 0.10888, regularization loss: 0.29026, contrastive loss: 0.10888, Loss positive: 0.10594, Loss negative: 0.00294
2018-10-22 10:39:52.163938: Epoch [  5/1000] [ 20/183], total loss: 0.00433, regularization loss: 0.29026, contrastive loss: 0.00433, Loss positive: 0.00000, Loss negative: 0.00433
2018-10-22 10:40:02.333270: Epoch [  5/1000] [ 40/183], total loss: 0.00281, regularization loss: 0.29026, contrastive loss: 0.00281, Loss positive: 0.00000, Loss negative: 0.00281
2018-10-22 10:40:12.544936: Epoch [  5/1000] [ 60/183], total loss: 0.00001, regularization loss: 0.29026, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 10:40:22.743872: Epoch [  5/1000] [ 80/183], total loss: 0.07717, regularization loss: 0.29026, contrastive loss: 0.07717, Loss positive: 0.07460, Loss negative: 0.00257
2018-10-22 10:40:32.995636: Epoch [  5/1000] [100/183], total loss: 0.00210, regularization loss: 0.29026, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 10:40:43.219217: Epoch [  5/1000] [120/183], total loss: 0.00712, regularization loss: 0.29026, contrastive loss: 0.00712, Loss positive: 0.00000, Loss negative: 0.00712
2018-10-22 10:40:53.789157: Epoch [  5/1000] [140/183], total loss: 0.04842, regularization loss: 0.29026, contrastive loss: 0.04842, Loss positive: 0.04115, Loss negative: 0.00727
2018-10-22 10:41:04.059098: Epoch [  5/1000] [160/183], total loss: 0.00083, regularization loss: 0.29026, contrastive loss: 0.00083, Loss positive: 0.00000, Loss negative: 0.00083
2018-10-22 10:41:14.302505: Epoch [  5/1000] [180/183], total loss: 0.00003, regularization loss: 0.29026, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 10:41:38.311791: Epoch [  6/1000] [ 20/183], total loss: 0.00552, regularization loss: 0.29026, contrastive loss: 0.00552, Loss positive: 0.00000, Loss negative: 0.00552
2018-10-22 10:41:48.520969: Epoch [  6/1000] [ 40/183], total loss: 0.00245, regularization loss: 0.29025, contrastive loss: 0.00245, Loss positive: 0.00000, Loss negative: 0.00245
2018-10-22 10:41:58.741023: Epoch [  6/1000] [ 60/183], total loss: 0.10036, regularization loss: 0.29025, contrastive loss: 0.10036, Loss positive: 0.10006, Loss negative: 0.00030
2018-10-22 10:42:08.973994: Epoch [  6/1000] [ 80/183], total loss: 0.00857, regularization loss: 0.29025, contrastive loss: 0.00857, Loss positive: 0.00000, Loss negative: 0.00857
2018-10-22 10:42:19.308161: Epoch [  6/1000] [100/183], total loss: 0.00078, regularization loss: 0.29025, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 10:42:29.541354: Epoch [  6/1000] [120/183], total loss: 0.00250, regularization loss: 0.29026, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 10:42:40.111223: Epoch [  6/1000] [140/183], total loss: 0.00000, regularization loss: 0.29026, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:42:50.420945: Epoch [  6/1000] [160/183], total loss: 0.00102, regularization loss: 0.29025, contrastive loss: 0.00102, Loss positive: 0.00000, Loss negative: 0.00102
2018-10-22 10:43:00.803190: Epoch [  6/1000] [180/183], total loss: 0.00302, regularization loss: 0.29025, contrastive loss: 0.00302, Loss positive: 0.00000, Loss negative: 0.00302
2018-10-22 10:43:24.741653: Epoch [  7/1000] [ 20/183], total loss: 0.00272, regularization loss: 0.29025, contrastive loss: 0.00272, Loss positive: 0.00000, Loss negative: 0.00272
2018-10-22 10:43:34.934801: Epoch [  7/1000] [ 40/183], total loss: 0.00198, regularization loss: 0.29025, contrastive loss: 0.00198, Loss positive: 0.00000, Loss negative: 0.00198
2018-10-22 10:43:45.192828: Epoch [  7/1000] [ 60/183], total loss: 0.06829, regularization loss: 0.29025, contrastive loss: 0.06829, Loss positive: 0.06305, Loss negative: 0.00524
2018-10-22 10:43:55.415601: Epoch [  7/1000] [ 80/183], total loss: 0.06046, regularization loss: 0.29025, contrastive loss: 0.06046, Loss positive: 0.05691, Loss negative: 0.00355
2018-10-22 10:44:05.824558: Epoch [  7/1000] [100/183], total loss: 0.00001, regularization loss: 0.29025, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 10:44:16.065052: Epoch [  7/1000] [120/183], total loss: 0.00228, regularization loss: 0.29025, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 10:44:26.392742: Epoch [  7/1000] [140/183], total loss: 0.03356, regularization loss: 0.29025, contrastive loss: 0.03356, Loss positive: 0.02649, Loss negative: 0.00706
2018-10-22 10:44:36.650294: Epoch [  7/1000] [160/183], total loss: 0.00682, regularization loss: 0.29025, contrastive loss: 0.00682, Loss positive: 0.00000, Loss negative: 0.00682
2018-10-22 10:44:47.093908: Epoch [  7/1000] [180/183], total loss: 0.04538, regularization loss: 0.29025, contrastive loss: 0.04538, Loss positive: 0.03423, Loss negative: 0.01115
2018-10-22 10:45:09.536614: Epoch [  8/1000] [ 20/183], total loss: 0.00255, regularization loss: 0.29025, contrastive loss: 0.00255, Loss positive: 0.00000, Loss negative: 0.00255
2018-10-22 10:45:19.740050: Epoch [  8/1000] [ 40/183], total loss: 0.04956, regularization loss: 0.29025, contrastive loss: 0.04956, Loss positive: 0.04391, Loss negative: 0.00566
2018-10-22 10:45:29.905985: Epoch [  8/1000] [ 60/183], total loss: 0.05340, regularization loss: 0.29025, contrastive loss: 0.05340, Loss positive: 0.05170, Loss negative: 0.00171
2018-10-22 10:45:40.107571: Epoch [  8/1000] [ 80/183], total loss: 0.00113, regularization loss: 0.29025, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 10:45:50.372749: Epoch [  8/1000] [100/183], total loss: 0.07396, regularization loss: 0.29025, contrastive loss: 0.07396, Loss positive: 0.07394, Loss negative: 0.00002
2018-10-22 10:46:00.651822: Epoch [  8/1000] [120/183], total loss: 0.00174, regularization loss: 0.29024, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 10:46:10.937020: Epoch [  8/1000] [140/183], total loss: 0.01707, regularization loss: 0.29024, contrastive loss: 0.01707, Loss positive: 0.01367, Loss negative: 0.00340
2018-10-22 10:46:21.204444: Epoch [  8/1000] [160/183], total loss: 0.00804, regularization loss: 0.29024, contrastive loss: 0.00804, Loss positive: 0.00000, Loss negative: 0.00804
2018-10-22 10:46:31.491973: Epoch [  8/1000] [180/183], total loss: 0.00141, regularization loss: 0.29024, contrastive loss: 0.00141, Loss positive: 0.00000, Loss negative: 0.00141
2018-10-22 10:46:54.110334: Epoch [  9/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29024, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:47:04.308340: Epoch [  9/1000] [ 40/183], total loss: 0.00182, regularization loss: 0.29024, contrastive loss: 0.00182, Loss positive: 0.00000, Loss negative: 0.00182
2018-10-22 10:47:14.507345: Epoch [  9/1000] [ 60/183], total loss: 0.00541, regularization loss: 0.29024, contrastive loss: 0.00541, Loss positive: 0.00000, Loss negative: 0.00541
2018-10-22 10:47:24.750234: Epoch [  9/1000] [ 80/183], total loss: 0.00135, regularization loss: 0.29024, contrastive loss: 0.00135, Loss positive: 0.00000, Loss negative: 0.00135
2018-10-22 10:47:35.016184: Epoch [  9/1000] [100/183], total loss: 0.04257, regularization loss: 0.29024, contrastive loss: 0.04257, Loss positive: 0.03726, Loss negative: 0.00530
2018-10-22 10:47:45.251019: Epoch [  9/1000] [120/183], total loss: 0.00250, regularization loss: 0.29024, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 10:47:55.580348: Epoch [  9/1000] [140/183], total loss: 0.02551, regularization loss: 0.29024, contrastive loss: 0.02551, Loss positive: 0.02080, Loss negative: 0.00471
2018-10-22 10:48:05.793540: Epoch [  9/1000] [160/183], total loss: 0.00123, regularization loss: 0.29024, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 10:48:16.035988: Epoch [  9/1000] [180/183], total loss: 0.01645, regularization loss: 0.29024, contrastive loss: 0.01645, Loss positive: 0.01331, Loss negative: 0.00315
2018-10-22 10:48:36.608173: Epoch [ 10/1000] [ 20/183], total loss: 0.00156, regularization loss: 0.29024, contrastive loss: 0.00156, Loss positive: 0.00000, Loss negative: 0.00156
2018-10-22 10:48:46.774034: Epoch [ 10/1000] [ 40/183], total loss: 0.01241, regularization loss: 0.29024, contrastive loss: 0.01241, Loss positive: 0.00000, Loss negative: 0.01241
2018-10-22 10:48:56.902550: Epoch [ 10/1000] [ 60/183], total loss: 0.02291, regularization loss: 0.29023, contrastive loss: 0.02291, Loss positive: 0.02253, Loss negative: 0.00038
2018-10-22 10:49:07.460718: Epoch [ 10/1000] [ 80/183], total loss: 0.04577, regularization loss: 0.29024, contrastive loss: 0.04577, Loss positive: 0.04494, Loss negative: 0.00083
2018-10-22 10:49:17.695578: Epoch [ 10/1000] [100/183], total loss: 0.04865, regularization loss: 0.29024, contrastive loss: 0.04865, Loss positive: 0.04778, Loss negative: 0.00087
2018-10-22 10:49:27.982781: Epoch [ 10/1000] [120/183], total loss: 0.00408, regularization loss: 0.29024, contrastive loss: 0.00408, Loss positive: 0.00000, Loss negative: 0.00408
2018-10-22 10:49:38.172584: Epoch [ 10/1000] [140/183], total loss: 0.00101, regularization loss: 0.29024, contrastive loss: 0.00101, Loss positive: 0.00000, Loss negative: 0.00101
2018-10-22 10:49:48.454816: Epoch [ 10/1000] [160/183], total loss: 0.00181, regularization loss: 0.29023, contrastive loss: 0.00181, Loss positive: 0.00000, Loss negative: 0.00181
2018-10-22 10:49:58.665045: Epoch [ 10/1000] [180/183], total loss: 0.00333, regularization loss: 0.29023, contrastive loss: 0.00333, Loss positive: 0.00000, Loss negative: 0.00333
Recall@1: 0.12441
Recall@2: 0.20392
Recall@4: 0.30402
Recall@8: 0.43805
Recall@16: 0.58120
Recall@32: 0.73666
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 61.  86.  60.  27.  46.  69.  57.  79.  61.  60.  85.  70.  55.  38.
  49.  41.  36.  50.  59.  41.  65.  37.  58.  68.  86.  55.  64.  64.
  29.  22.  73.  79.  92.  51.  43.  41.  62.  70.  48.  57.  79.  53.
  35.  79.  30.  66.  59.  87.  69.  65.  54.  36.  82.  81.  42.  61.
  78.  51.  85.  57.  23.  33.  27.  45.  74.  78.  59.  63.  68.  83.
  82.  39.  73.  70.  50.  74.  48.  65.  41.  80.  60.  24.  66.  83.
  47.  69.  87.  68.  74. 101.  45.  79.  49.  61.  71.  45.  33.  70.
  42.  32.]
Purity is 0.163
count_cross = [[ 0.  0.  1. ...  0.  0.  0.]
 [ 0.  4.  5. ...  2.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  2.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [31.  0.  0. ...  0.  0.  0.]]
Mutual information is 1.65385
5924.0
5924
Entropy cluster is 4.55684
Entropy class is 4.60444
normalized_mutual_information is 0.36105
tp_and_fp = 188611.0
tp = 11220.0
fp is 177391.0
fn is 161530.0
RI is 0.9806815760622793
Precision is 0.05948751663476679
Recall is 0.0649493487698987
F_1 is 0.06209856625369091

normalized_mutual_information = 0.361051153216403
RI = 0.9806815760622793
F_1 = 0.06209856625369091

The NN is 0.12441
The FT is 0.07438
The ST is 0.12483
The DCG is 0.45296
The E is 0.05950
The MAP 0.05754

2018-10-22 10:51:25.341533: Epoch [ 11/1000] [ 20/183], total loss: 0.00081, regularization loss: 0.29023, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 10:51:35.390243: Epoch [ 11/1000] [ 40/183], total loss: 0.02033, regularization loss: 0.29023, contrastive loss: 0.02033, Loss positive: 0.01961, Loss negative: 0.00071
2018-10-22 10:51:45.482366: Epoch [ 11/1000] [ 60/183], total loss: 0.00043, regularization loss: 0.29023, contrastive loss: 0.00043, Loss positive: 0.00000, Loss negative: 0.00043
2018-10-22 10:51:55.573700: Epoch [ 11/1000] [ 80/183], total loss: 0.00006, regularization loss: 0.29023, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 10:52:05.706940: Epoch [ 11/1000] [100/183], total loss: 0.03184, regularization loss: 0.29023, contrastive loss: 0.03184, Loss positive: 0.02989, Loss negative: 0.00195
2018-10-22 10:52:15.838743: Epoch [ 11/1000] [120/183], total loss: 0.03736, regularization loss: 0.29023, contrastive loss: 0.03736, Loss positive: 0.03614, Loss negative: 0.00121
2018-10-22 10:52:26.106168: Epoch [ 11/1000] [140/183], total loss: 0.00042, regularization loss: 0.29023, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 10:52:36.439992: Epoch [ 11/1000] [160/183], total loss: 0.00488, regularization loss: 0.29023, contrastive loss: 0.00488, Loss positive: 0.00000, Loss negative: 0.00488
2018-10-22 10:52:46.769128: Epoch [ 11/1000] [180/183], total loss: 0.01622, regularization loss: 0.29023, contrastive loss: 0.01622, Loss positive: 0.01425, Loss negative: 0.00198
2018-10-22 10:53:08.567453: Epoch [ 12/1000] [ 20/183], total loss: 0.00169, regularization loss: 0.29023, contrastive loss: 0.00169, Loss positive: 0.00000, Loss negative: 0.00169
2018-10-22 10:53:18.658184: Epoch [ 12/1000] [ 40/183], total loss: 0.02193, regularization loss: 0.29023, contrastive loss: 0.02193, Loss positive: 0.02150, Loss negative: 0.00043
2018-10-22 10:53:28.802129: Epoch [ 12/1000] [ 60/183], total loss: 0.04250, regularization loss: 0.29023, contrastive loss: 0.04250, Loss positive: 0.03626, Loss negative: 0.00624
2018-10-22 10:53:38.955764: Epoch [ 12/1000] [ 80/183], total loss: 0.02744, regularization loss: 0.29023, contrastive loss: 0.02744, Loss positive: 0.02657, Loss negative: 0.00087
2018-10-22 10:53:49.216428: Epoch [ 12/1000] [100/183], total loss: 0.00173, regularization loss: 0.29023, contrastive loss: 0.00173, Loss positive: 0.00000, Loss negative: 0.00173
2018-10-22 10:53:59.405085: Epoch [ 12/1000] [120/183], total loss: 0.00010, regularization loss: 0.29023, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 10:54:09.664280: Epoch [ 12/1000] [140/183], total loss: 0.00177, regularization loss: 0.29023, contrastive loss: 0.00177, Loss positive: 0.00000, Loss negative: 0.00177
2018-10-22 10:54:19.892945: Epoch [ 12/1000] [160/183], total loss: 0.00044, regularization loss: 0.29023, contrastive loss: 0.00044, Loss positive: 0.00000, Loss negative: 0.00044
2018-10-22 10:54:30.133833: Epoch [ 12/1000] [180/183], total loss: 0.00078, regularization loss: 0.29023, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 10:54:50.716770: Epoch [ 13/1000] [ 20/183], total loss: 0.01035, regularization loss: 0.29023, contrastive loss: 0.01035, Loss positive: 0.00000, Loss negative: 0.01035
2018-10-22 10:55:00.854451: Epoch [ 13/1000] [ 40/183], total loss: 0.00128, regularization loss: 0.29023, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 10:55:11.027453: Epoch [ 13/1000] [ 60/183], total loss: 0.00053, regularization loss: 0.29023, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 10:55:21.220353: Epoch [ 13/1000] [ 80/183], total loss: 0.00104, regularization loss: 0.29023, contrastive loss: 0.00104, Loss positive: 0.00000, Loss negative: 0.00104
2018-10-22 10:55:31.482277: Epoch [ 13/1000] [100/183], total loss: 0.01952, regularization loss: 0.29023, contrastive loss: 0.01952, Loss positive: 0.01690, Loss negative: 0.00262
2018-10-22 10:55:41.704874: Epoch [ 13/1000] [120/183], total loss: 0.00148, regularization loss: 0.29023, contrastive loss: 0.00148, Loss positive: 0.00000, Loss negative: 0.00148
2018-10-22 10:55:51.938478: Epoch [ 13/1000] [140/183], total loss: 0.00005, regularization loss: 0.29023, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 10:56:02.392943: Epoch [ 13/1000] [160/183], total loss: 0.00133, regularization loss: 0.29023, contrastive loss: 0.00133, Loss positive: 0.00000, Loss negative: 0.00133
2018-10-22 10:56:12.640571: Epoch [ 13/1000] [180/183], total loss: 0.00252, regularization loss: 0.29022, contrastive loss: 0.00252, Loss positive: 0.00000, Loss negative: 0.00252
2018-10-22 10:56:33.211160: Epoch [ 14/1000] [ 20/183], total loss: 0.00032, regularization loss: 0.29023, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 10:56:43.346865: Epoch [ 14/1000] [ 40/183], total loss: 0.00056, regularization loss: 0.29023, contrastive loss: 0.00056, Loss positive: 0.00000, Loss negative: 0.00056
2018-10-22 10:56:53.480900: Epoch [ 14/1000] [ 60/183], total loss: 0.00203, regularization loss: 0.29023, contrastive loss: 0.00203, Loss positive: 0.00000, Loss negative: 0.00203
2018-10-22 10:57:03.804219: Epoch [ 14/1000] [ 80/183], total loss: 0.06635, regularization loss: 0.29023, contrastive loss: 0.06635, Loss positive: 0.06521, Loss negative: 0.00115
2018-10-22 10:57:13.968437: Epoch [ 14/1000] [100/183], total loss: 0.00344, regularization loss: 0.29023, contrastive loss: 0.00344, Loss positive: 0.00000, Loss negative: 0.00344
2018-10-22 10:57:24.317403: Epoch [ 14/1000] [120/183], total loss: 0.00000, regularization loss: 0.29023, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:57:34.823085: Epoch [ 14/1000] [140/183], total loss: 0.00010, regularization loss: 0.29022, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 10:57:45.032314: Epoch [ 14/1000] [160/183], total loss: 0.00370, regularization loss: 0.29022, contrastive loss: 0.00370, Loss positive: 0.00000, Loss negative: 0.00370
2018-10-22 10:57:55.276186: Epoch [ 14/1000] [180/183], total loss: 0.01846, regularization loss: 0.29022, contrastive loss: 0.01846, Loss positive: 0.01805, Loss negative: 0.00040
2018-10-22 10:58:15.824641: Epoch [ 15/1000] [ 20/183], total loss: 0.00747, regularization loss: 0.29022, contrastive loss: 0.00747, Loss positive: 0.00000, Loss negative: 0.00747
2018-10-22 10:58:25.929562: Epoch [ 15/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29022, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 10:58:36.031100: Epoch [ 15/1000] [ 60/183], total loss: 0.05970, regularization loss: 0.29022, contrastive loss: 0.05970, Loss positive: 0.05246, Loss negative: 0.00725
2018-10-22 10:58:46.266126: Epoch [ 15/1000] [ 80/183], total loss: 0.00120, regularization loss: 0.29022, contrastive loss: 0.00120, Loss positive: 0.00000, Loss negative: 0.00120
2018-10-22 10:58:56.512150: Epoch [ 15/1000] [100/183], total loss: 0.00071, regularization loss: 0.29022, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 10:59:06.753384: Epoch [ 15/1000] [120/183], total loss: 0.00035, regularization loss: 0.29022, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 10:59:16.946953: Epoch [ 15/1000] [140/183], total loss: 0.00090, regularization loss: 0.29022, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 10:59:27.198344: Epoch [ 15/1000] [160/183], total loss: 0.00398, regularization loss: 0.29022, contrastive loss: 0.00398, Loss positive: 0.00000, Loss negative: 0.00398
2018-10-22 10:59:37.421822: Epoch [ 15/1000] [180/183], total loss: 0.00067, regularization loss: 0.29022, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 10:59:58.099590: Epoch [ 16/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29023, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:00:08.215445: Epoch [ 16/1000] [ 40/183], total loss: 0.00090, regularization loss: 0.29023, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 11:00:18.352210: Epoch [ 16/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29023, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:00:28.528861: Epoch [ 16/1000] [ 80/183], total loss: 0.04929, regularization loss: 0.29023, contrastive loss: 0.04929, Loss positive: 0.04197, Loss negative: 0.00732
2018-10-22 11:00:38.784246: Epoch [ 16/1000] [100/183], total loss: 0.00024, regularization loss: 0.29022, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 11:00:49.019196: Epoch [ 16/1000] [120/183], total loss: 0.00173, regularization loss: 0.29022, contrastive loss: 0.00173, Loss positive: 0.00000, Loss negative: 0.00173
2018-10-22 11:00:59.267628: Epoch [ 16/1000] [140/183], total loss: 0.00395, regularization loss: 0.29022, contrastive loss: 0.00395, Loss positive: 0.00000, Loss negative: 0.00395
2018-10-22 11:01:09.512149: Epoch [ 16/1000] [160/183], total loss: 0.03043, regularization loss: 0.29022, contrastive loss: 0.03043, Loss positive: 0.02620, Loss negative: 0.00423
2018-10-22 11:01:19.971932: Epoch [ 16/1000] [180/183], total loss: 0.00176, regularization loss: 0.29022, contrastive loss: 0.00176, Loss positive: 0.00000, Loss negative: 0.00176
2018-10-22 11:01:41.459090: Epoch [ 17/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29022, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:01:51.600207: Epoch [ 17/1000] [ 40/183], total loss: 0.06622, regularization loss: 0.29022, contrastive loss: 0.06622, Loss positive: 0.06622, Loss negative: 0.00000
2018-10-22 11:02:01.708803: Epoch [ 17/1000] [ 60/183], total loss: 0.00003, regularization loss: 0.29022, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 11:02:11.823990: Epoch [ 17/1000] [ 80/183], total loss: 0.00498, regularization loss: 0.29022, contrastive loss: 0.00498, Loss positive: 0.00000, Loss negative: 0.00498
2018-10-22 11:02:22.172998: Epoch [ 17/1000] [100/183], total loss: 0.00018, regularization loss: 0.29023, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 11:02:32.442867: Epoch [ 17/1000] [120/183], total loss: 0.00164, regularization loss: 0.29022, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 11:02:42.687966: Epoch [ 17/1000] [140/183], total loss: 0.06488, regularization loss: 0.29022, contrastive loss: 0.06488, Loss positive: 0.06399, Loss negative: 0.00090
2018-10-22 11:02:52.931953: Epoch [ 17/1000] [160/183], total loss: 0.04509, regularization loss: 0.29022, contrastive loss: 0.04509, Loss positive: 0.04351, Loss negative: 0.00159
2018-10-22 11:03:03.163641: Epoch [ 17/1000] [180/183], total loss: 0.00002, regularization loss: 0.29022, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 11:03:24.666283: Epoch [ 18/1000] [ 20/183], total loss: 0.04610, regularization loss: 0.29022, contrastive loss: 0.04610, Loss positive: 0.04286, Loss negative: 0.00324
2018-10-22 11:03:34.786687: Epoch [ 18/1000] [ 40/183], total loss: 0.00732, regularization loss: 0.29022, contrastive loss: 0.00732, Loss positive: 0.00000, Loss negative: 0.00732
2018-10-22 11:03:44.891634: Epoch [ 18/1000] [ 60/183], total loss: 0.00060, regularization loss: 0.29022, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 11:03:55.276970: Epoch [ 18/1000] [ 80/183], total loss: 0.00033, regularization loss: 0.29022, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 11:04:05.670090: Epoch [ 18/1000] [100/183], total loss: 0.00001, regularization loss: 0.29022, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 11:04:15.873717: Epoch [ 18/1000] [120/183], total loss: 0.00000, regularization loss: 0.29022, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:04:26.231728: Epoch [ 18/1000] [140/183], total loss: 0.01623, regularization loss: 0.29022, contrastive loss: 0.01623, Loss positive: 0.01345, Loss negative: 0.00278
2018-10-22 11:04:36.472347: Epoch [ 18/1000] [160/183], total loss: 0.00771, regularization loss: 0.29022, contrastive loss: 0.00771, Loss positive: 0.00000, Loss negative: 0.00771
2018-10-22 11:04:46.805770: Epoch [ 18/1000] [180/183], total loss: 0.05539, regularization loss: 0.29022, contrastive loss: 0.05539, Loss positive: 0.05314, Loss negative: 0.00225
2018-10-22 11:05:07.419062: Epoch [ 19/1000] [ 20/183], total loss: 0.00020, regularization loss: 0.29022, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 11:05:17.538362: Epoch [ 19/1000] [ 40/183], total loss: 0.00259, regularization loss: 0.29022, contrastive loss: 0.00259, Loss positive: 0.00000, Loss negative: 0.00259
2018-10-22 11:05:27.657961: Epoch [ 19/1000] [ 60/183], total loss: 0.00108, regularization loss: 0.29022, contrastive loss: 0.00108, Loss positive: 0.00000, Loss negative: 0.00108
2018-10-22 11:05:37.806636: Epoch [ 19/1000] [ 80/183], total loss: 0.00057, regularization loss: 0.29022, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 11:05:48.052824: Epoch [ 19/1000] [100/183], total loss: 0.00017, regularization loss: 0.29022, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 11:05:58.291064: Epoch [ 19/1000] [120/183], total loss: 0.00060, regularization loss: 0.29022, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 11:06:08.838380: Epoch [ 19/1000] [140/183], total loss: 0.00892, regularization loss: 0.29022, contrastive loss: 0.00892, Loss positive: 0.00000, Loss negative: 0.00892
2018-10-22 11:06:19.172672: Epoch [ 19/1000] [160/183], total loss: 0.00270, regularization loss: 0.29022, contrastive loss: 0.00270, Loss positive: 0.00000, Loss negative: 0.00270
2018-10-22 11:06:29.437031: Epoch [ 19/1000] [180/183], total loss: 0.00256, regularization loss: 0.29022, contrastive loss: 0.00256, Loss positive: 0.00000, Loss negative: 0.00256
2018-10-22 11:06:50.050126: Epoch [ 20/1000] [ 20/183], total loss: 0.00424, regularization loss: 0.29022, contrastive loss: 0.00424, Loss positive: 0.00000, Loss negative: 0.00424
2018-10-22 11:07:00.163973: Epoch [ 20/1000] [ 40/183], total loss: 0.01279, regularization loss: 0.29022, contrastive loss: 0.01279, Loss positive: 0.01279, Loss negative: 0.00000
2018-10-22 11:07:10.299966: Epoch [ 20/1000] [ 60/183], total loss: 0.00115, regularization loss: 0.29022, contrastive loss: 0.00115, Loss positive: 0.00000, Loss negative: 0.00115
2018-10-22 11:07:20.417317: Epoch [ 20/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29022, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 11:07:30.667801: Epoch [ 20/1000] [100/183], total loss: 0.00241, regularization loss: 0.29022, contrastive loss: 0.00241, Loss positive: 0.00000, Loss negative: 0.00241
2018-10-22 11:07:40.975698: Epoch [ 20/1000] [120/183], total loss: 0.00011, regularization loss: 0.29022, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 11:07:51.222139: Epoch [ 20/1000] [140/183], total loss: 0.02469, regularization loss: 0.29022, contrastive loss: 0.02469, Loss positive: 0.02351, Loss negative: 0.00117
2018-10-22 11:08:01.492715: Epoch [ 20/1000] [160/183], total loss: 0.00210, regularization loss: 0.29022, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 11:08:11.734115: Epoch [ 20/1000] [180/183], total loss: 0.00007, regularization loss: 0.29022, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
Recall@1: 0.14365
Recall@2: 0.22654
Recall@4: 0.33474
Recall@8: 0.47519
Recall@16: 0.62441
Recall@32: 0.77481
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [32. 64. 50. 61. 79. 70. 62. 37. 71. 77. 62. 47. 43. 68. 57. 52. 95. 49.
 71. 44. 58. 87. 88. 50. 49. 66. 55. 49. 85. 54. 42. 65. 54. 59. 73. 60.
 57. 55. 45. 52. 62. 61. 58. 41. 80. 51. 39. 49. 89. 68. 48. 76. 70. 64.
 54. 35. 56. 49. 62. 59. 60. 30. 49. 79. 47. 68. 41. 50. 30. 61. 51. 64.
 79. 78. 42. 46. 34. 60. 38. 55. 83. 59. 36. 85. 33. 53. 76. 42. 68. 78.
 58. 65. 64. 56. 96. 59. 74. 83. 81. 48.]
Purity is 0.160
count_cross = [[0. 0. 1. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 1. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 1. 1.]
 [0. 2. 1. ... 0. 1. 1.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 1.68259
5924.0
5924
Entropy cluster is 4.57186
Entropy class is 4.60444
normalized_mutual_information is 0.36672
tp_and_fp = 184144.0
tp = 11385.0
fp is 172759.0
fn is 161365.0
RI is 0.9809550040281748
Precision is 0.06182661395429664
Recall is 0.06590448625180897
F_1 is 0.0638004561578508

normalized_mutual_information = 0.3667246593619587
RI = 0.9809550040281748
F_1 = 0.0638004561578508

The NN is 0.14365
The FT is 0.08039
The ST is 0.13231
The DCG is 0.45891
The E is 0.06563
The MAP 0.06025

2018-10-22 11:09:37.323791: Epoch [ 21/1000] [ 20/183], total loss: 0.07410, regularization loss: 0.29022, contrastive loss: 0.07410, Loss positive: 0.07409, Loss negative: 0.00001
2018-10-22 11:09:47.358720: Epoch [ 21/1000] [ 40/183], total loss: 0.05926, regularization loss: 0.29022, contrastive loss: 0.05926, Loss positive: 0.05741, Loss negative: 0.00185
2018-10-22 11:09:57.441417: Epoch [ 21/1000] [ 60/183], total loss: 0.00098, regularization loss: 0.29022, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 11:10:07.545732: Epoch [ 21/1000] [ 80/183], total loss: 0.00520, regularization loss: 0.29022, contrastive loss: 0.00520, Loss positive: 0.00000, Loss negative: 0.00520
2018-10-22 11:10:17.632980: Epoch [ 21/1000] [100/183], total loss: 0.00223, regularization loss: 0.29022, contrastive loss: 0.00223, Loss positive: 0.00000, Loss negative: 0.00223
2018-10-22 11:10:27.735300: Epoch [ 21/1000] [120/183], total loss: 0.00245, regularization loss: 0.29022, contrastive loss: 0.00245, Loss positive: 0.00000, Loss negative: 0.00245
2018-10-22 11:10:37.858802: Epoch [ 21/1000] [140/183], total loss: 0.00205, regularization loss: 0.29022, contrastive loss: 0.00205, Loss positive: 0.00000, Loss negative: 0.00205
2018-10-22 11:10:48.153297: Epoch [ 21/1000] [160/183], total loss: 0.00085, regularization loss: 0.29022, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 11:10:58.332155: Epoch [ 21/1000] [180/183], total loss: 0.00588, regularization loss: 0.29022, contrastive loss: 0.00588, Loss positive: 0.00000, Loss negative: 0.00588
2018-10-22 11:11:19.688348: Epoch [ 22/1000] [ 20/183], total loss: 0.00186, regularization loss: 0.29022, contrastive loss: 0.00186, Loss positive: 0.00000, Loss negative: 0.00186
2018-10-22 11:11:29.819772: Epoch [ 22/1000] [ 40/183], total loss: 0.00137, regularization loss: 0.29022, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 11:11:39.930385: Epoch [ 22/1000] [ 60/183], total loss: 0.01576, regularization loss: 0.29022, contrastive loss: 0.01576, Loss positive: 0.01568, Loss negative: 0.00007
2018-10-22 11:11:50.059969: Epoch [ 22/1000] [ 80/183], total loss: 0.09990, regularization loss: 0.29022, contrastive loss: 0.09990, Loss positive: 0.09972, Loss negative: 0.00018
2018-10-22 11:12:00.263736: Epoch [ 22/1000] [100/183], total loss: 0.02219, regularization loss: 0.29022, contrastive loss: 0.02219, Loss positive: 0.02219, Loss negative: 0.00000
2018-10-22 11:12:10.554990: Epoch [ 22/1000] [120/183], total loss: 0.00001, regularization loss: 0.29022, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 11:12:20.741903: Epoch [ 22/1000] [140/183], total loss: 0.00031, regularization loss: 0.29022, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 11:12:31.038900: Epoch [ 22/1000] [160/183], total loss: 0.00001, regularization loss: 0.29022, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 11:12:41.241303: Epoch [ 22/1000] [180/183], total loss: 0.00033, regularization loss: 0.29022, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 11:13:03.550287: Epoch [ 23/1000] [ 20/183], total loss: 0.00071, regularization loss: 0.29022, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 11:13:13.693347: Epoch [ 23/1000] [ 40/183], total loss: 0.00827, regularization loss: 0.29022, contrastive loss: 0.00827, Loss positive: 0.00000, Loss negative: 0.00827
2018-10-22 11:13:23.860246: Epoch [ 23/1000] [ 60/183], total loss: 0.00076, regularization loss: 0.29022, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 11:13:34.060706: Epoch [ 23/1000] [ 80/183], total loss: 0.00007, regularization loss: 0.29022, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 11:13:44.410316: Epoch [ 23/1000] [100/183], total loss: 0.00004, regularization loss: 0.29022, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 11:13:54.685965: Epoch [ 23/1000] [120/183], total loss: 0.03509, regularization loss: 0.29022, contrastive loss: 0.03509, Loss positive: 0.03427, Loss negative: 0.00082
2018-10-22 11:14:04.934847: Epoch [ 23/1000] [140/183], total loss: 0.00267, regularization loss: 0.29022, contrastive loss: 0.00267, Loss positive: 0.00000, Loss negative: 0.00267
2018-10-22 11:14:15.162170: Epoch [ 23/1000] [160/183], total loss: 0.00528, regularization loss: 0.29022, contrastive loss: 0.00528, Loss positive: 0.00000, Loss negative: 0.00528
2018-10-22 11:14:25.422213: Epoch [ 23/1000] [180/183], total loss: 0.00000, regularization loss: 0.29022, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:14:47.212350: Epoch [ 24/1000] [ 20/183], total loss: 0.00190, regularization loss: 0.29022, contrastive loss: 0.00190, Loss positive: 0.00000, Loss negative: 0.00190
2018-10-22 11:14:57.361575: Epoch [ 24/1000] [ 40/183], total loss: 0.00038, regularization loss: 0.29022, contrastive loss: 0.00038, Loss positive: 0.00000, Loss negative: 0.00038
2018-10-22 11:15:07.510454: Epoch [ 24/1000] [ 60/183], total loss: 0.00633, regularization loss: 0.29022, contrastive loss: 0.00633, Loss positive: 0.00000, Loss negative: 0.00633
2018-10-22 11:15:17.723175: Epoch [ 24/1000] [ 80/183], total loss: 0.00288, regularization loss: 0.29021, contrastive loss: 0.00288, Loss positive: 0.00000, Loss negative: 0.00288
2018-10-22 11:15:27.966545: Epoch [ 24/1000] [100/183], total loss: 0.00469, regularization loss: 0.29022, contrastive loss: 0.00469, Loss positive: 0.00000, Loss negative: 0.00469
2018-10-22 11:15:38.179477: Epoch [ 24/1000] [120/183], total loss: 0.03974, regularization loss: 0.29022, contrastive loss: 0.03974, Loss positive: 0.03908, Loss negative: 0.00066
2018-10-22 11:15:48.543602: Epoch [ 24/1000] [140/183], total loss: 0.00243, regularization loss: 0.29022, contrastive loss: 0.00243, Loss positive: 0.00000, Loss negative: 0.00243
2018-10-22 11:15:58.789508: Epoch [ 24/1000] [160/183], total loss: 0.00016, regularization loss: 0.29022, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 11:16:09.020123: Epoch [ 24/1000] [180/183], total loss: 0.00001, regularization loss: 0.29022, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 11:16:31.187254: Epoch [ 25/1000] [ 20/183], total loss: 0.03327, regularization loss: 0.29022, contrastive loss: 0.03327, Loss positive: 0.03095, Loss negative: 0.00232
2018-10-22 11:16:41.330640: Epoch [ 25/1000] [ 40/183], total loss: 0.01951, regularization loss: 0.29021, contrastive loss: 0.01951, Loss positive: 0.01704, Loss negative: 0.00247
2018-10-22 11:16:51.465480: Epoch [ 25/1000] [ 60/183], total loss: 0.00212, regularization loss: 0.29022, contrastive loss: 0.00212, Loss positive: 0.00000, Loss negative: 0.00212
2018-10-22 11:17:01.611105: Epoch [ 25/1000] [ 80/183], total loss: 0.00088, regularization loss: 0.29021, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 11:17:11.821330: Epoch [ 25/1000] [100/183], total loss: 0.00000, regularization loss: 0.29022, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:17:22.046142: Epoch [ 25/1000] [120/183], total loss: 0.00106, regularization loss: 0.29022, contrastive loss: 0.00106, Loss positive: 0.00000, Loss negative: 0.00106
2018-10-22 11:17:32.302149: Epoch [ 25/1000] [140/183], total loss: 0.00181, regularization loss: 0.29021, contrastive loss: 0.00181, Loss positive: 0.00000, Loss negative: 0.00181
2018-10-22 11:17:42.499939: Epoch [ 25/1000] [160/183], total loss: 0.04958, regularization loss: 0.29021, contrastive loss: 0.04958, Loss positive: 0.04837, Loss negative: 0.00121
2018-10-22 11:17:52.786414: Epoch [ 25/1000] [180/183], total loss: 0.00020, regularization loss: 0.29021, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 11:18:15.526632: Epoch [ 26/1000] [ 20/183], total loss: 0.00244, regularization loss: 0.29021, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 11:18:25.663083: Epoch [ 26/1000] [ 40/183], total loss: 0.00086, regularization loss: 0.29021, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 11:18:35.808159: Epoch [ 26/1000] [ 60/183], total loss: 0.03434, regularization loss: 0.29021, contrastive loss: 0.03434, Loss positive: 0.02807, Loss negative: 0.00627
2018-10-22 11:18:46.004802: Epoch [ 26/1000] [ 80/183], total loss: 0.04205, regularization loss: 0.29022, contrastive loss: 0.04205, Loss positive: 0.04045, Loss negative: 0.00160
2018-10-22 11:18:56.249630: Epoch [ 26/1000] [100/183], total loss: 0.02141, regularization loss: 0.29021, contrastive loss: 0.02141, Loss positive: 0.01951, Loss negative: 0.00190
2018-10-22 11:19:06.537286: Epoch [ 26/1000] [120/183], total loss: 0.00020, regularization loss: 0.29021, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 11:19:16.841208: Epoch [ 26/1000] [140/183], total loss: 0.03925, regularization loss: 0.29021, contrastive loss: 0.03925, Loss positive: 0.03215, Loss negative: 0.00709
2018-10-22 11:19:27.064183: Epoch [ 26/1000] [160/183], total loss: 0.00224, regularization loss: 0.29021, contrastive loss: 0.00224, Loss positive: 0.00000, Loss negative: 0.00224
2018-10-22 11:19:37.313218: Epoch [ 26/1000] [180/183], total loss: 0.00054, regularization loss: 0.29022, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 11:20:00.164887: Epoch [ 27/1000] [ 20/183], total loss: 0.00140, regularization loss: 0.29021, contrastive loss: 0.00140, Loss positive: 0.00000, Loss negative: 0.00140
2018-10-22 11:20:10.325025: Epoch [ 27/1000] [ 40/183], total loss: 0.04543, regularization loss: 0.29021, contrastive loss: 0.04543, Loss positive: 0.04289, Loss negative: 0.00254
2018-10-22 11:20:20.500789: Epoch [ 27/1000] [ 60/183], total loss: 0.00347, regularization loss: 0.29021, contrastive loss: 0.00347, Loss positive: 0.00000, Loss negative: 0.00347
2018-10-22 11:20:30.668657: Epoch [ 27/1000] [ 80/183], total loss: 0.04194, regularization loss: 0.29021, contrastive loss: 0.04194, Loss positive: 0.03799, Loss negative: 0.00396
2018-10-22 11:20:40.927541: Epoch [ 27/1000] [100/183], total loss: 0.00063, regularization loss: 0.29021, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 11:20:51.136159: Epoch [ 27/1000] [120/183], total loss: 0.00125, regularization loss: 0.29021, contrastive loss: 0.00125, Loss positive: 0.00000, Loss negative: 0.00125
2018-10-22 11:21:01.357889: Epoch [ 27/1000] [140/183], total loss: 0.02839, regularization loss: 0.29021, contrastive loss: 0.02839, Loss positive: 0.02839, Loss negative: 0.00000
2018-10-22 11:21:11.626434: Epoch [ 27/1000] [160/183], total loss: 0.00317, regularization loss: 0.29021, contrastive loss: 0.00317, Loss positive: 0.00000, Loss negative: 0.00317
2018-10-22 11:21:21.845025: Epoch [ 27/1000] [180/183], total loss: 0.04243, regularization loss: 0.29021, contrastive loss: 0.04243, Loss positive: 0.03909, Loss negative: 0.00334
2018-10-22 11:21:44.727705: Epoch [ 28/1000] [ 20/183], total loss: 0.00418, regularization loss: 0.29021, contrastive loss: 0.00418, Loss positive: 0.00000, Loss negative: 0.00418
2018-10-22 11:21:54.872150: Epoch [ 28/1000] [ 40/183], total loss: 0.01758, regularization loss: 0.29021, contrastive loss: 0.01758, Loss positive: 0.01720, Loss negative: 0.00038
2018-10-22 11:22:05.035320: Epoch [ 28/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29021, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:22:15.205105: Epoch [ 28/1000] [ 80/183], total loss: 0.03942, regularization loss: 0.29022, contrastive loss: 0.03942, Loss positive: 0.03855, Loss negative: 0.00087
2018-10-22 11:22:25.456203: Epoch [ 28/1000] [100/183], total loss: 0.00756, regularization loss: 0.29021, contrastive loss: 0.00756, Loss positive: 0.00000, Loss negative: 0.00756
2018-10-22 11:22:35.686314: Epoch [ 28/1000] [120/183], total loss: 0.00229, regularization loss: 0.29021, contrastive loss: 0.00229, Loss positive: 0.00000, Loss negative: 0.00229
2018-10-22 11:22:45.934286: Epoch [ 28/1000] [140/183], total loss: 0.00005, regularization loss: 0.29021, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 11:22:56.490975: Epoch [ 28/1000] [160/183], total loss: 0.00078, regularization loss: 0.29021, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 11:23:06.846476: Epoch [ 28/1000] [180/183], total loss: 0.00241, regularization loss: 0.29022, contrastive loss: 0.00241, Loss positive: 0.00000, Loss negative: 0.00241
2018-10-22 11:23:31.832684: Epoch [ 29/1000] [ 20/183], total loss: 0.05226, regularization loss: 0.29021, contrastive loss: 0.05226, Loss positive: 0.04878, Loss negative: 0.00348
2018-10-22 11:23:42.063528: Epoch [ 29/1000] [ 40/183], total loss: 0.00037, regularization loss: 0.29021, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 11:23:52.223356: Epoch [ 29/1000] [ 60/183], total loss: 0.00002, regularization loss: 0.29021, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 11:24:02.446409: Epoch [ 29/1000] [ 80/183], total loss: 0.05492, regularization loss: 0.29021, contrastive loss: 0.05492, Loss positive: 0.05210, Loss negative: 0.00282
2018-10-22 11:24:12.682251: Epoch [ 29/1000] [100/183], total loss: 0.03990, regularization loss: 0.29021, contrastive loss: 0.03990, Loss positive: 0.03840, Loss negative: 0.00150
2018-10-22 11:24:23.022174: Epoch [ 29/1000] [120/183], total loss: 0.00474, regularization loss: 0.29021, contrastive loss: 0.00474, Loss positive: 0.00000, Loss negative: 0.00474
2018-10-22 11:24:33.256458: Epoch [ 29/1000] [140/183], total loss: 0.00025, regularization loss: 0.29021, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 11:24:43.534629: Epoch [ 29/1000] [160/183], total loss: 0.03121, regularization loss: 0.29021, contrastive loss: 0.03121, Loss positive: 0.02955, Loss negative: 0.00166
2018-10-22 11:24:54.041340: Epoch [ 29/1000] [180/183], total loss: 0.00105, regularization loss: 0.29021, contrastive loss: 0.00105, Loss positive: 0.00000, Loss negative: 0.00105
2018-10-22 11:25:18.087066: Epoch [ 30/1000] [ 20/183], total loss: 0.00295, regularization loss: 0.29021, contrastive loss: 0.00295, Loss positive: 0.00000, Loss negative: 0.00295
2018-10-22 11:25:28.234967: Epoch [ 30/1000] [ 40/183], total loss: 0.00144, regularization loss: 0.29021, contrastive loss: 0.00144, Loss positive: 0.00000, Loss negative: 0.00144
2018-10-22 11:25:38.399114: Epoch [ 30/1000] [ 60/183], total loss: 0.00076, regularization loss: 0.29021, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 11:25:48.664013: Epoch [ 30/1000] [ 80/183], total loss: 0.00054, regularization loss: 0.29021, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 11:25:58.855706: Epoch [ 30/1000] [100/183], total loss: 0.02576, regularization loss: 0.29021, contrastive loss: 0.02576, Loss positive: 0.02560, Loss negative: 0.00016
2018-10-22 11:26:09.104541: Epoch [ 30/1000] [120/183], total loss: 0.00069, regularization loss: 0.29021, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 11:26:19.694327: Epoch [ 30/1000] [140/183], total loss: 0.04767, regularization loss: 0.29021, contrastive loss: 0.04767, Loss positive: 0.04765, Loss negative: 0.00002
2018-10-22 11:26:30.165801: Epoch [ 30/1000] [160/183], total loss: 0.00018, regularization loss: 0.29021, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 11:26:40.553908: Epoch [ 30/1000] [180/183], total loss: 0.00437, regularization loss: 0.29021, contrastive loss: 0.00437, Loss positive: 0.00000, Loss negative: 0.00437
Recall@1: 0.16188
Recall@2: 0.25473
Recall@4: 0.37373
Recall@8: 0.51772
Recall@16: 0.67083
Recall@32: 0.80149
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 73.  71.  57.  70.  70.  63.  62.  33.  76.  63.  50.  99.  81.  51.
  76.  39.  52.  45.  62.  39.  25.  79.  77.  73.  78.  44.  39.  13.
  93.  42.  72.  66.  63.  45.  69.  74.  64.  30.  69.  38.  34.  77.
  68.  78.  77.  53.  67.  69.  24.  96.  70.  63.  90.  37.  31.  29.
  72.  68. 112.  38.  80.  47.  72.  42.  81.  57.  95.  45.  64.  67.
  39.  49.  62.  34.  76.  44.  59.  55.  61.  48. 104.  52.  32.  66.
  47.  54.  75.  84.  39.  38.  20.  36.  68.  52.  66.  58.  64.  48.
  54.  52.]
Purity is 0.189
count_cross = [[ 0.  4.  3. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0. 11.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 3.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  1. ...  1.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  1.]]
Mutual information is 1.80265
5924.0
5924
Entropy cluster is 4.54967
Entropy class is 4.60444
normalized_mutual_information is 0.39385
tp_and_fp = 191163.0
tp = 14246.0
fp is 176917.0
fn is 158504.0
RI is 0.9808810753077731
Precision is 0.07452278945193369
Recall is 0.08246599131693198
F_1 is 0.0782934382668385

normalized_mutual_information = 0.393845332589924
RI = 0.9808810753077731
F_1 = 0.0782934382668385

The NN is 0.16188
The FT is 0.09827
The ST is 0.16021
The DCG is 0.47963
The E is 0.08000
The MAP 0.07771

2018-10-22 11:28:11.047908: Epoch [ 31/1000] [ 20/183], total loss: 0.00161, regularization loss: 0.29021, contrastive loss: 0.00161, Loss positive: 0.00000, Loss negative: 0.00161
2018-10-22 11:28:21.082197: Epoch [ 31/1000] [ 40/183], total loss: 0.00040, regularization loss: 0.29021, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 11:28:31.158022: Epoch [ 31/1000] [ 60/183], total loss: 0.00004, regularization loss: 0.29021, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 11:28:41.236767: Epoch [ 31/1000] [ 80/183], total loss: 0.08701, regularization loss: 0.29021, contrastive loss: 0.08701, Loss positive: 0.08455, Loss negative: 0.00247
2018-10-22 11:28:51.367895: Epoch [ 31/1000] [100/183], total loss: 0.00197, regularization loss: 0.29021, contrastive loss: 0.00197, Loss positive: 0.00000, Loss negative: 0.00197
2018-10-22 11:29:01.504430: Epoch [ 31/1000] [120/183], total loss: 0.00192, regularization loss: 0.29021, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 11:29:11.698919: Epoch [ 31/1000] [140/183], total loss: 0.00167, regularization loss: 0.29021, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 11:29:21.948042: Epoch [ 31/1000] [160/183], total loss: 0.03009, regularization loss: 0.29021, contrastive loss: 0.03009, Loss positive: 0.02880, Loss negative: 0.00129
2018-10-22 11:29:32.156384: Epoch [ 31/1000] [180/183], total loss: 0.14550, regularization loss: 0.29021, contrastive loss: 0.14550, Loss positive: 0.14385, Loss negative: 0.00165
2018-10-22 11:29:53.767834: Epoch [ 32/1000] [ 20/183], total loss: 0.00295, regularization loss: 0.29021, contrastive loss: 0.00295, Loss positive: 0.00000, Loss negative: 0.00295
2018-10-22 11:30:03.872887: Epoch [ 32/1000] [ 40/183], total loss: 0.00596, regularization loss: 0.29021, contrastive loss: 0.00596, Loss positive: 0.00000, Loss negative: 0.00596
2018-10-22 11:30:13.970194: Epoch [ 32/1000] [ 60/183], total loss: 0.01928, regularization loss: 0.29021, contrastive loss: 0.01928, Loss positive: 0.01473, Loss negative: 0.00456
2018-10-22 11:30:24.085430: Epoch [ 32/1000] [ 80/183], total loss: 0.00164, regularization loss: 0.29021, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 11:30:34.301650: Epoch [ 32/1000] [100/183], total loss: 0.00244, regularization loss: 0.29021, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 11:30:44.746721: Epoch [ 32/1000] [120/183], total loss: 0.00160, regularization loss: 0.29021, contrastive loss: 0.00160, Loss positive: 0.00000, Loss negative: 0.00160
2018-10-22 11:30:55.111486: Epoch [ 32/1000] [140/183], total loss: 0.00347, regularization loss: 0.29021, contrastive loss: 0.00347, Loss positive: 0.00000, Loss negative: 0.00347
2018-10-22 11:31:05.352343: Epoch [ 32/1000] [160/183], total loss: 0.00524, regularization loss: 0.29021, contrastive loss: 0.00524, Loss positive: 0.00000, Loss negative: 0.00524
2018-10-22 11:31:15.581534: Epoch [ 32/1000] [180/183], total loss: 0.07169, regularization loss: 0.29021, contrastive loss: 0.07169, Loss positive: 0.07013, Loss negative: 0.00155
2018-10-22 11:31:37.052606: Epoch [ 33/1000] [ 20/183], total loss: 0.00378, regularization loss: 0.29021, contrastive loss: 0.00378, Loss positive: 0.00000, Loss negative: 0.00378
2018-10-22 11:31:47.181214: Epoch [ 33/1000] [ 40/183], total loss: 0.00448, regularization loss: 0.29021, contrastive loss: 0.00448, Loss positive: 0.00000, Loss negative: 0.00448
2018-10-22 11:31:57.310050: Epoch [ 33/1000] [ 60/183], total loss: 0.03699, regularization loss: 0.29021, contrastive loss: 0.03699, Loss positive: 0.03616, Loss negative: 0.00083
2018-10-22 11:32:07.511557: Epoch [ 33/1000] [ 80/183], total loss: 0.03685, regularization loss: 0.29021, contrastive loss: 0.03685, Loss positive: 0.02567, Loss negative: 0.01118
2018-10-22 11:32:17.762190: Epoch [ 33/1000] [100/183], total loss: 0.00058, regularization loss: 0.29021, contrastive loss: 0.00058, Loss positive: 0.00000, Loss negative: 0.00058
2018-10-22 11:32:27.986668: Epoch [ 33/1000] [120/183], total loss: 0.00022, regularization loss: 0.29021, contrastive loss: 0.00022, Loss positive: 0.00000, Loss negative: 0.00022
2018-10-22 11:32:38.330186: Epoch [ 33/1000] [140/183], total loss: 0.10579, regularization loss: 0.29021, contrastive loss: 0.10579, Loss positive: 0.10491, Loss negative: 0.00088
2018-10-22 11:32:48.609470: Epoch [ 33/1000] [160/183], total loss: 0.00004, regularization loss: 0.29021, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 11:32:58.799134: Epoch [ 33/1000] [180/183], total loss: 0.00216, regularization loss: 0.29021, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 11:33:20.259454: Epoch [ 34/1000] [ 20/183], total loss: 0.02216, regularization loss: 0.29021, contrastive loss: 0.02216, Loss positive: 0.02125, Loss negative: 0.00091
2018-10-22 11:33:30.398767: Epoch [ 34/1000] [ 40/183], total loss: 0.02725, regularization loss: 0.29021, contrastive loss: 0.02725, Loss positive: 0.02502, Loss negative: 0.00223
2018-10-22 11:33:40.516233: Epoch [ 34/1000] [ 60/183], total loss: 0.02218, regularization loss: 0.29021, contrastive loss: 0.02218, Loss positive: 0.02212, Loss negative: 0.00006
2018-10-22 11:33:50.717122: Epoch [ 34/1000] [ 80/183], total loss: 0.02106, regularization loss: 0.29021, contrastive loss: 0.02106, Loss positive: 0.02018, Loss negative: 0.00088
2018-10-22 11:34:00.990170: Epoch [ 34/1000] [100/183], total loss: 0.00062, regularization loss: 0.29021, contrastive loss: 0.00062, Loss positive: 0.00000, Loss negative: 0.00062
2018-10-22 11:34:11.231992: Epoch [ 34/1000] [120/183], total loss: 0.00270, regularization loss: 0.29021, contrastive loss: 0.00270, Loss positive: 0.00000, Loss negative: 0.00270
2018-10-22 11:34:21.435026: Epoch [ 34/1000] [140/183], total loss: 0.00485, regularization loss: 0.29021, contrastive loss: 0.00485, Loss positive: 0.00000, Loss negative: 0.00485
2018-10-22 11:34:31.681482: Epoch [ 34/1000] [160/183], total loss: 0.00278, regularization loss: 0.29021, contrastive loss: 0.00278, Loss positive: 0.00000, Loss negative: 0.00278
2018-10-22 11:34:41.928998: Epoch [ 34/1000] [180/183], total loss: 0.00164, regularization loss: 0.29021, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 11:35:02.461954: Epoch [ 35/1000] [ 20/183], total loss: 0.00311, regularization loss: 0.29021, contrastive loss: 0.00311, Loss positive: 0.00000, Loss negative: 0.00311
2018-10-22 11:35:12.579465: Epoch [ 35/1000] [ 40/183], total loss: 0.04578, regularization loss: 0.29021, contrastive loss: 0.04578, Loss positive: 0.04342, Loss negative: 0.00236
2018-10-22 11:35:22.722859: Epoch [ 35/1000] [ 60/183], total loss: 0.04131, regularization loss: 0.29021, contrastive loss: 0.04131, Loss positive: 0.03942, Loss negative: 0.00189
2018-10-22 11:35:32.968145: Epoch [ 35/1000] [ 80/183], total loss: 0.00111, regularization loss: 0.29021, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 11:35:43.155333: Epoch [ 35/1000] [100/183], total loss: 0.00115, regularization loss: 0.29021, contrastive loss: 0.00115, Loss positive: 0.00000, Loss negative: 0.00115
2018-10-22 11:35:53.417655: Epoch [ 35/1000] [120/183], total loss: 0.00761, regularization loss: 0.29021, contrastive loss: 0.00761, Loss positive: 0.00000, Loss negative: 0.00761
2018-10-22 11:36:03.636266: Epoch [ 35/1000] [140/183], total loss: 0.03135, regularization loss: 0.29021, contrastive loss: 0.03135, Loss positive: 0.02637, Loss negative: 0.00498
2018-10-22 11:36:13.894668: Epoch [ 35/1000] [160/183], total loss: 0.00426, regularization loss: 0.29021, contrastive loss: 0.00426, Loss positive: 0.00000, Loss negative: 0.00426
2018-10-22 11:36:24.160369: Epoch [ 35/1000] [180/183], total loss: 0.04302, regularization loss: 0.29021, contrastive loss: 0.04302, Loss positive: 0.04195, Loss negative: 0.00107
2018-10-22 11:36:45.016834: Epoch [ 36/1000] [ 20/183], total loss: 0.00059, regularization loss: 0.29021, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 11:36:55.163393: Epoch [ 36/1000] [ 40/183], total loss: 0.00178, regularization loss: 0.29021, contrastive loss: 0.00178, Loss positive: 0.00000, Loss negative: 0.00178
2018-10-22 11:37:05.316545: Epoch [ 36/1000] [ 60/183], total loss: 0.00229, regularization loss: 0.29021, contrastive loss: 0.00229, Loss positive: 0.00000, Loss negative: 0.00229
2018-10-22 11:37:15.623133: Epoch [ 36/1000] [ 80/183], total loss: 0.02048, regularization loss: 0.29021, contrastive loss: 0.02048, Loss positive: 0.01398, Loss negative: 0.00650
2018-10-22 11:37:25.871079: Epoch [ 36/1000] [100/183], total loss: 0.01720, regularization loss: 0.29021, contrastive loss: 0.01720, Loss positive: 0.01599, Loss negative: 0.00121
2018-10-22 11:37:36.115479: Epoch [ 36/1000] [120/183], total loss: 0.00087, regularization loss: 0.29021, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 11:37:46.403578: Epoch [ 36/1000] [140/183], total loss: 0.00248, regularization loss: 0.29021, contrastive loss: 0.00248, Loss positive: 0.00000, Loss negative: 0.00248
2018-10-22 11:37:56.571907: Epoch [ 36/1000] [160/183], total loss: 0.02848, regularization loss: 0.29021, contrastive loss: 0.02848, Loss positive: 0.02683, Loss negative: 0.00165
2018-10-22 11:38:06.810593: Epoch [ 36/1000] [180/183], total loss: 0.05841, regularization loss: 0.29021, contrastive loss: 0.05841, Loss positive: 0.05823, Loss negative: 0.00018
2018-10-22 11:38:27.354475: Epoch [ 37/1000] [ 20/183], total loss: 0.02204, regularization loss: 0.29021, contrastive loss: 0.02204, Loss positive: 0.01781, Loss negative: 0.00422
2018-10-22 11:38:37.462821: Epoch [ 37/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29021, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:38:47.617006: Epoch [ 37/1000] [ 60/183], total loss: 0.00007, regularization loss: 0.29021, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 11:38:57.807671: Epoch [ 37/1000] [ 80/183], total loss: 0.00094, regularization loss: 0.29021, contrastive loss: 0.00094, Loss positive: 0.00000, Loss negative: 0.00094
2018-10-22 11:39:08.065974: Epoch [ 37/1000] [100/183], total loss: 0.00107, regularization loss: 0.29021, contrastive loss: 0.00107, Loss positive: 0.00000, Loss negative: 0.00107
2018-10-22 11:39:18.340588: Epoch [ 37/1000] [120/183], total loss: 0.07864, regularization loss: 0.29020, contrastive loss: 0.07864, Loss positive: 0.07768, Loss negative: 0.00096
2018-10-22 11:39:28.548747: Epoch [ 37/1000] [140/183], total loss: 0.00243, regularization loss: 0.29020, contrastive loss: 0.00243, Loss positive: 0.00000, Loss negative: 0.00243
2018-10-22 11:39:38.895591: Epoch [ 37/1000] [160/183], total loss: 0.01007, regularization loss: 0.29020, contrastive loss: 0.01007, Loss positive: 0.00000, Loss negative: 0.01007
2018-10-22 11:39:49.150576: Epoch [ 37/1000] [180/183], total loss: 0.00292, regularization loss: 0.29020, contrastive loss: 0.00292, Loss positive: 0.00000, Loss negative: 0.00292
2018-10-22 11:40:09.813223: Epoch [ 38/1000] [ 20/183], total loss: 0.04127, regularization loss: 0.29020, contrastive loss: 0.04127, Loss positive: 0.03654, Loss negative: 0.00473
2018-10-22 11:40:19.966464: Epoch [ 38/1000] [ 40/183], total loss: 0.00508, regularization loss: 0.29020, contrastive loss: 0.00508, Loss positive: 0.00000, Loss negative: 0.00508
2018-10-22 11:40:30.063796: Epoch [ 38/1000] [ 60/183], total loss: 0.00255, regularization loss: 0.29020, contrastive loss: 0.00255, Loss positive: 0.00000, Loss negative: 0.00255
2018-10-22 11:40:40.219298: Epoch [ 38/1000] [ 80/183], total loss: 0.00548, regularization loss: 0.29020, contrastive loss: 0.00548, Loss positive: 0.00000, Loss negative: 0.00548
2018-10-22 11:40:50.574461: Epoch [ 38/1000] [100/183], total loss: 0.00079, regularization loss: 0.29020, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 11:41:00.725894: Epoch [ 38/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:41:10.948396: Epoch [ 38/1000] [140/183], total loss: 0.00539, regularization loss: 0.29020, contrastive loss: 0.00539, Loss positive: 0.00000, Loss negative: 0.00539
2018-10-22 11:41:21.208346: Epoch [ 38/1000] [160/183], total loss: 0.02101, regularization loss: 0.29021, contrastive loss: 0.02101, Loss positive: 0.02093, Loss negative: 0.00008
2018-10-22 11:41:31.462673: Epoch [ 38/1000] [180/183], total loss: 0.09158, regularization loss: 0.29021, contrastive loss: 0.09158, Loss positive: 0.09103, Loss negative: 0.00055
2018-10-22 11:41:52.846288: Epoch [ 39/1000] [ 20/183], total loss: 0.00012, regularization loss: 0.29020, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 11:42:02.960028: Epoch [ 39/1000] [ 40/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 11:42:13.093694: Epoch [ 39/1000] [ 60/183], total loss: 0.01590, regularization loss: 0.29020, contrastive loss: 0.01590, Loss positive: 0.01590, Loss negative: 0.00000
2018-10-22 11:42:23.236637: Epoch [ 39/1000] [ 80/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 11:42:33.502552: Epoch [ 39/1000] [100/183], total loss: 0.00207, regularization loss: 0.29020, contrastive loss: 0.00207, Loss positive: 0.00000, Loss negative: 0.00207
2018-10-22 11:42:43.729774: Epoch [ 39/1000] [120/183], total loss: 0.00910, regularization loss: 0.29020, contrastive loss: 0.00910, Loss positive: 0.00000, Loss negative: 0.00910
2018-10-22 11:42:54.077588: Epoch [ 39/1000] [140/183], total loss: 0.00150, regularization loss: 0.29020, contrastive loss: 0.00150, Loss positive: 0.00000, Loss negative: 0.00150
2018-10-22 11:43:04.300654: Epoch [ 39/1000] [160/183], total loss: 0.00284, regularization loss: 0.29020, contrastive loss: 0.00284, Loss positive: 0.00000, Loss negative: 0.00284
2018-10-22 11:43:14.568377: Epoch [ 39/1000] [180/183], total loss: 0.01890, regularization loss: 0.29020, contrastive loss: 0.01890, Loss positive: 0.01810, Loss negative: 0.00080
2018-10-22 11:43:36.154064: Epoch [ 40/1000] [ 20/183], total loss: 0.00005, regularization loss: 0.29021, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 11:43:46.269004: Epoch [ 40/1000] [ 40/183], total loss: 0.00270, regularization loss: 0.29021, contrastive loss: 0.00270, Loss positive: 0.00000, Loss negative: 0.00270
2018-10-22 11:43:56.425187: Epoch [ 40/1000] [ 60/183], total loss: 0.00173, regularization loss: 0.29021, contrastive loss: 0.00173, Loss positive: 0.00000, Loss negative: 0.00173
2018-10-22 11:44:06.567853: Epoch [ 40/1000] [ 80/183], total loss: 0.00025, regularization loss: 0.29021, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 11:44:16.831539: Epoch [ 40/1000] [100/183], total loss: 0.00000, regularization loss: 0.29021, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:44:27.052450: Epoch [ 40/1000] [120/183], total loss: 0.00066, regularization loss: 0.29021, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 11:44:37.422173: Epoch [ 40/1000] [140/183], total loss: 0.00060, regularization loss: 0.29021, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 11:44:47.630745: Epoch [ 40/1000] [160/183], total loss: 0.00063, regularization loss: 0.29020, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 11:44:57.912207: Epoch [ 40/1000] [180/183], total loss: 0.00067, regularization loss: 0.29020, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
Recall@1: 0.17353
Recall@2: 0.26958
Recall@4: 0.39619
Recall@8: 0.53815
Recall@16: 0.68332
Recall@32: 0.81212
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 78.  74.  48.  51.  83.  54.  69.  70.  76.  77.  70.  55.  74.  67.
  27.  36.  91.  45.  50.  30.  36.  36.  41.  47.  43.  48.  59.  68.
  20.  89.  67.  60.  97.  38.  47.  39.  53.  40.  45.  51.  66.  75.
  73.  50.  56.  70.  47.  51.  58.  99. 100.  43.  40.  46.  49.  74.
  50.  77.  67.  63.  71.  48.  66.  35.  46.  61.  45.  76.  70.  68.
  77.  73.  77.  61.  64.  67.  85.  89.  40.  62.  21.  60.  59.  40.
  43.  63.  55.  44.  68.  40.  81.  65.  68.  74.  91.  39.  61.  80.
  56.  32.]
Purity is 0.189
count_cross = [[0. 0. 0. ... 0. 0. 2.]
 [0. 0. 0. ... 1. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 3.]
 [0. 0. 0. ... 0. 0. 5.]
 [0. 0. 3. ... 1. 0. 0.]]
Mutual information is 1.81630
5924.0
5924
Entropy cluster is 4.56052
Entropy class is 4.60444
normalized_mutual_information is 0.39636
tp_and_fp = 187758.0
tp = 14381.0
fp is 173377.0
fn is 158369.0
RI is 0.9810905495155418
Precision is 0.0765932743212007
Recall is 0.08324746743849494
F_1 is 0.07978186337057708

normalized_mutual_information = 0.39635791404635673
RI = 0.9810905495155418
F_1 = 0.07978186337057708

The NN is 0.17353
The FT is 0.09831
The ST is 0.16014
The DCG is 0.48022
The E is 0.08087
The MAP 0.07657

2018-10-22 11:46:23.233162: Epoch [ 41/1000] [ 20/183], total loss: 0.00266, regularization loss: 0.29020, contrastive loss: 0.00266, Loss positive: 0.00000, Loss negative: 0.00266
2018-10-22 11:46:33.310291: Epoch [ 41/1000] [ 40/183], total loss: 0.00460, regularization loss: 0.29020, contrastive loss: 0.00460, Loss positive: 0.00000, Loss negative: 0.00460
2018-10-22 11:46:43.370074: Epoch [ 41/1000] [ 60/183], total loss: 0.07920, regularization loss: 0.29020, contrastive loss: 0.07920, Loss positive: 0.07919, Loss negative: 0.00001
2018-10-22 11:46:53.472674: Epoch [ 41/1000] [ 80/183], total loss: 0.05574, regularization loss: 0.29020, contrastive loss: 0.05574, Loss positive: 0.05563, Loss negative: 0.00011
2018-10-22 11:47:03.552865: Epoch [ 41/1000] [100/183], total loss: 0.02902, regularization loss: 0.29020, contrastive loss: 0.02902, Loss positive: 0.02595, Loss negative: 0.00307
2018-10-22 11:47:13.689306: Epoch [ 41/1000] [120/183], total loss: 0.00080, regularization loss: 0.29020, contrastive loss: 0.00080, Loss positive: 0.00000, Loss negative: 0.00080
2018-10-22 11:47:23.834249: Epoch [ 41/1000] [140/183], total loss: 0.00080, regularization loss: 0.29020, contrastive loss: 0.00080, Loss positive: 0.00000, Loss negative: 0.00080
2018-10-22 11:47:34.046530: Epoch [ 41/1000] [160/183], total loss: 0.03833, regularization loss: 0.29020, contrastive loss: 0.03833, Loss positive: 0.03603, Loss negative: 0.00230
2018-10-22 11:47:44.272401: Epoch [ 41/1000] [180/183], total loss: 0.00097, regularization loss: 0.29020, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 11:48:04.784228: Epoch [ 42/1000] [ 20/183], total loss: 0.04818, regularization loss: 0.29020, contrastive loss: 0.04818, Loss positive: 0.04499, Loss negative: 0.00319
2018-10-22 11:48:14.922775: Epoch [ 42/1000] [ 40/183], total loss: 0.00282, regularization loss: 0.29020, contrastive loss: 0.00282, Loss positive: 0.00000, Loss negative: 0.00282
2018-10-22 11:48:25.035785: Epoch [ 42/1000] [ 60/183], total loss: 0.00680, regularization loss: 0.29020, contrastive loss: 0.00680, Loss positive: 0.00000, Loss negative: 0.00680
2018-10-22 11:48:35.162564: Epoch [ 42/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:48:45.368328: Epoch [ 42/1000] [100/183], total loss: 0.00072, regularization loss: 0.29020, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 11:48:55.560981: Epoch [ 42/1000] [120/183], total loss: 0.00069, regularization loss: 0.29020, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 11:49:05.923092: Epoch [ 42/1000] [140/183], total loss: 0.00857, regularization loss: 0.29020, contrastive loss: 0.00857, Loss positive: 0.00000, Loss negative: 0.00857
2018-10-22 11:49:16.174950: Epoch [ 42/1000] [160/183], total loss: 0.00151, regularization loss: 0.29020, contrastive loss: 0.00151, Loss positive: 0.00000, Loss negative: 0.00151
2018-10-22 11:49:26.360288: Epoch [ 42/1000] [180/183], total loss: 0.00069, regularization loss: 0.29020, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 11:49:47.086710: Epoch [ 43/1000] [ 20/183], total loss: 0.00214, regularization loss: 0.29020, contrastive loss: 0.00214, Loss positive: 0.00000, Loss negative: 0.00214
2018-10-22 11:49:57.202261: Epoch [ 43/1000] [ 40/183], total loss: 0.02783, regularization loss: 0.29020, contrastive loss: 0.02783, Loss positive: 0.02783, Loss negative: 0.00000
2018-10-22 11:50:07.345923: Epoch [ 43/1000] [ 60/183], total loss: 0.00065, regularization loss: 0.29020, contrastive loss: 0.00065, Loss positive: 0.00000, Loss negative: 0.00065
2018-10-22 11:50:17.658147: Epoch [ 43/1000] [ 80/183], total loss: 0.02408, regularization loss: 0.29020, contrastive loss: 0.02408, Loss positive: 0.02363, Loss negative: 0.00045
2018-10-22 11:50:27.812910: Epoch [ 43/1000] [100/183], total loss: 0.03558, regularization loss: 0.29020, contrastive loss: 0.03558, Loss positive: 0.03199, Loss negative: 0.00359
2018-10-22 11:50:38.050555: Epoch [ 43/1000] [120/183], total loss: 0.00329, regularization loss: 0.29020, contrastive loss: 0.00329, Loss positive: 0.00000, Loss negative: 0.00329
2018-10-22 11:50:48.392036: Epoch [ 43/1000] [140/183], total loss: 0.02462, regularization loss: 0.29020, contrastive loss: 0.02462, Loss positive: 0.02360, Loss negative: 0.00102
2018-10-22 11:50:58.637394: Epoch [ 43/1000] [160/183], total loss: 0.00197, regularization loss: 0.29020, contrastive loss: 0.00197, Loss positive: 0.00000, Loss negative: 0.00197
2018-10-22 11:51:08.882546: Epoch [ 43/1000] [180/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 11:51:30.166695: Epoch [ 44/1000] [ 20/183], total loss: 0.00325, regularization loss: 0.29020, contrastive loss: 0.00325, Loss positive: 0.00000, Loss negative: 0.00325
2018-10-22 11:51:40.333872: Epoch [ 44/1000] [ 40/183], total loss: 0.00010, regularization loss: 0.29020, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 11:51:50.460774: Epoch [ 44/1000] [ 60/183], total loss: 0.00098, regularization loss: 0.29020, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 11:52:00.855747: Epoch [ 44/1000] [ 80/183], total loss: 0.00515, regularization loss: 0.29020, contrastive loss: 0.00515, Loss positive: 0.00000, Loss negative: 0.00515
2018-10-22 11:52:11.031335: Epoch [ 44/1000] [100/183], total loss: 0.00021, regularization loss: 0.29020, contrastive loss: 0.00021, Loss positive: 0.00000, Loss negative: 0.00021
2018-10-22 11:52:21.297738: Epoch [ 44/1000] [120/183], total loss: 0.00310, regularization loss: 0.29020, contrastive loss: 0.00310, Loss positive: 0.00000, Loss negative: 0.00310
2018-10-22 11:52:31.748321: Epoch [ 44/1000] [140/183], total loss: 0.00474, regularization loss: 0.29020, contrastive loss: 0.00474, Loss positive: 0.00000, Loss negative: 0.00474
2018-10-22 11:52:41.976083: Epoch [ 44/1000] [160/183], total loss: 0.03331, regularization loss: 0.29020, contrastive loss: 0.03331, Loss positive: 0.03127, Loss negative: 0.00204
2018-10-22 11:52:52.310714: Epoch [ 44/1000] [180/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 11:53:13.770346: Epoch [ 45/1000] [ 20/183], total loss: 0.00220, regularization loss: 0.29020, contrastive loss: 0.00220, Loss positive: 0.00000, Loss negative: 0.00220
2018-10-22 11:53:23.878875: Epoch [ 45/1000] [ 40/183], total loss: 0.00071, regularization loss: 0.29020, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 11:53:34.006931: Epoch [ 45/1000] [ 60/183], total loss: 0.00198, regularization loss: 0.29020, contrastive loss: 0.00198, Loss positive: 0.00000, Loss negative: 0.00198
2018-10-22 11:53:44.406455: Epoch [ 45/1000] [ 80/183], total loss: 0.00052, regularization loss: 0.29020, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 11:53:54.610672: Epoch [ 45/1000] [100/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 11:54:04.814276: Epoch [ 45/1000] [120/183], total loss: 0.00279, regularization loss: 0.29020, contrastive loss: 0.00279, Loss positive: 0.00000, Loss negative: 0.00279
2018-10-22 11:54:15.063749: Epoch [ 45/1000] [140/183], total loss: 0.01527, regularization loss: 0.29020, contrastive loss: 0.01527, Loss positive: 0.01270, Loss negative: 0.00257
2018-10-22 11:54:25.344262: Epoch [ 45/1000] [160/183], total loss: 0.00239, regularization loss: 0.29020, contrastive loss: 0.00239, Loss positive: 0.00000, Loss negative: 0.00239
2018-10-22 11:54:35.549523: Epoch [ 45/1000] [180/183], total loss: 0.01132, regularization loss: 0.29020, contrastive loss: 0.01132, Loss positive: 0.00000, Loss negative: 0.01132
2018-10-22 11:54:56.101539: Epoch [ 46/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29021, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:55:06.230960: Epoch [ 46/1000] [ 40/183], total loss: 0.00070, regularization loss: 0.29020, contrastive loss: 0.00070, Loss positive: 0.00000, Loss negative: 0.00070
2018-10-22 11:55:16.379024: Epoch [ 46/1000] [ 60/183], total loss: 0.00208, regularization loss: 0.29020, contrastive loss: 0.00208, Loss positive: 0.00000, Loss negative: 0.00208
2018-10-22 11:55:26.573382: Epoch [ 46/1000] [ 80/183], total loss: 0.00366, regularization loss: 0.29020, contrastive loss: 0.00366, Loss positive: 0.00000, Loss negative: 0.00366
2018-10-22 11:55:36.757994: Epoch [ 46/1000] [100/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 11:55:47.004326: Epoch [ 46/1000] [120/183], total loss: 0.05456, regularization loss: 0.29020, contrastive loss: 0.05456, Loss positive: 0.05206, Loss negative: 0.00250
2018-10-22 11:55:57.285545: Epoch [ 46/1000] [140/183], total loss: 0.05854, regularization loss: 0.29020, contrastive loss: 0.05854, Loss positive: 0.05809, Loss negative: 0.00045
2018-10-22 11:56:07.477018: Epoch [ 46/1000] [160/183], total loss: 0.00124, regularization loss: 0.29020, contrastive loss: 0.00124, Loss positive: 0.00000, Loss negative: 0.00124
2018-10-22 11:56:17.929164: Epoch [ 46/1000] [180/183], total loss: 0.05220, regularization loss: 0.29020, contrastive loss: 0.05220, Loss positive: 0.05048, Loss negative: 0.00172
2018-10-22 11:56:38.840076: Epoch [ 47/1000] [ 20/183], total loss: 0.00111, regularization loss: 0.29020, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 11:56:48.956764: Epoch [ 47/1000] [ 40/183], total loss: 0.00325, regularization loss: 0.29020, contrastive loss: 0.00325, Loss positive: 0.00000, Loss negative: 0.00325
2018-10-22 11:56:59.099155: Epoch [ 47/1000] [ 60/183], total loss: 0.00158, regularization loss: 0.29020, contrastive loss: 0.00158, Loss positive: 0.00000, Loss negative: 0.00158
2018-10-22 11:57:09.260589: Epoch [ 47/1000] [ 80/183], total loss: 0.03014, regularization loss: 0.29020, contrastive loss: 0.03014, Loss positive: 0.02845, Loss negative: 0.00169
2018-10-22 11:57:19.467371: Epoch [ 47/1000] [100/183], total loss: 0.00093, regularization loss: 0.29020, contrastive loss: 0.00093, Loss positive: 0.00000, Loss negative: 0.00093
2018-10-22 11:57:29.838780: Epoch [ 47/1000] [120/183], total loss: 0.00155, regularization loss: 0.29020, contrastive loss: 0.00155, Loss positive: 0.00000, Loss negative: 0.00155
2018-10-22 11:57:40.063312: Epoch [ 47/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:57:50.331403: Epoch [ 47/1000] [160/183], total loss: 0.00141, regularization loss: 0.29020, contrastive loss: 0.00141, Loss positive: 0.00000, Loss negative: 0.00141
2018-10-22 11:58:00.524339: Epoch [ 47/1000] [180/183], total loss: 0.00097, regularization loss: 0.29020, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 11:58:21.215416: Epoch [ 48/1000] [ 20/183], total loss: 0.02562, regularization loss: 0.29020, contrastive loss: 0.02562, Loss positive: 0.02411, Loss negative: 0.00151
2018-10-22 11:58:31.323744: Epoch [ 48/1000] [ 40/183], total loss: 0.00040, regularization loss: 0.29020, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 11:58:41.427437: Epoch [ 48/1000] [ 60/183], total loss: 0.00435, regularization loss: 0.29020, contrastive loss: 0.00435, Loss positive: 0.00000, Loss negative: 0.00435
2018-10-22 11:58:51.569731: Epoch [ 48/1000] [ 80/183], total loss: 0.00406, regularization loss: 0.29020, contrastive loss: 0.00406, Loss positive: 0.00000, Loss negative: 0.00406
2018-10-22 11:59:01.892679: Epoch [ 48/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 11:59:12.132720: Epoch [ 48/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 11:59:22.482854: Epoch [ 48/1000] [140/183], total loss: 0.00187, regularization loss: 0.29020, contrastive loss: 0.00187, Loss positive: 0.00000, Loss negative: 0.00187
2018-10-22 11:59:32.722085: Epoch [ 48/1000] [160/183], total loss: 0.00145, regularization loss: 0.29020, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 11:59:42.928616: Epoch [ 48/1000] [180/183], total loss: 0.00435, regularization loss: 0.29020, contrastive loss: 0.00435, Loss positive: 0.00000, Loss negative: 0.00435
2018-10-22 12:00:03.528865: Epoch [ 49/1000] [ 20/183], total loss: 0.00042, regularization loss: 0.29020, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 12:00:13.674302: Epoch [ 49/1000] [ 40/183], total loss: 0.04617, regularization loss: 0.29020, contrastive loss: 0.04617, Loss positive: 0.04220, Loss negative: 0.00397
2018-10-22 12:00:23.778388: Epoch [ 49/1000] [ 60/183], total loss: 0.00766, regularization loss: 0.29020, contrastive loss: 0.00766, Loss positive: 0.00000, Loss negative: 0.00766
2018-10-22 12:00:33.912518: Epoch [ 49/1000] [ 80/183], total loss: 0.02712, regularization loss: 0.29020, contrastive loss: 0.02712, Loss positive: 0.02672, Loss negative: 0.00040
2018-10-22 12:00:44.305755: Epoch [ 49/1000] [100/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 12:00:54.572069: Epoch [ 49/1000] [120/183], total loss: 0.02387, regularization loss: 0.29020, contrastive loss: 0.02387, Loss positive: 0.02299, Loss negative: 0.00088
2018-10-22 12:01:04.753828: Epoch [ 49/1000] [140/183], total loss: 0.09092, regularization loss: 0.29020, contrastive loss: 0.09092, Loss positive: 0.08715, Loss negative: 0.00377
2018-10-22 12:01:14.993606: Epoch [ 49/1000] [160/183], total loss: 0.02671, regularization loss: 0.29020, contrastive loss: 0.02671, Loss positive: 0.02429, Loss negative: 0.00241
2018-10-22 12:01:25.376544: Epoch [ 49/1000] [180/183], total loss: 0.00098, regularization loss: 0.29020, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 12:01:46.803999: Epoch [ 50/1000] [ 20/183], total loss: 0.02548, regularization loss: 0.29020, contrastive loss: 0.02548, Loss positive: 0.02345, Loss negative: 0.00203
2018-10-22 12:01:56.924209: Epoch [ 50/1000] [ 40/183], total loss: 0.00108, regularization loss: 0.29020, contrastive loss: 0.00108, Loss positive: 0.00000, Loss negative: 0.00108
2018-10-22 12:02:07.051353: Epoch [ 50/1000] [ 60/183], total loss: 0.03151, regularization loss: 0.29020, contrastive loss: 0.03151, Loss positive: 0.03012, Loss negative: 0.00139
2018-10-22 12:02:17.209575: Epoch [ 50/1000] [ 80/183], total loss: 0.02545, regularization loss: 0.29020, contrastive loss: 0.02545, Loss positive: 0.02497, Loss negative: 0.00048
2018-10-22 12:02:27.400481: Epoch [ 50/1000] [100/183], total loss: 0.00190, regularization loss: 0.29020, contrastive loss: 0.00190, Loss positive: 0.00000, Loss negative: 0.00190
2018-10-22 12:02:37.635155: Epoch [ 50/1000] [120/183], total loss: 0.00393, regularization loss: 0.29020, contrastive loss: 0.00393, Loss positive: 0.00000, Loss negative: 0.00393
2018-10-22 12:02:47.875067: Epoch [ 50/1000] [140/183], total loss: 0.00382, regularization loss: 0.29020, contrastive loss: 0.00382, Loss positive: 0.00000, Loss negative: 0.00382
2018-10-22 12:02:58.134955: Epoch [ 50/1000] [160/183], total loss: 0.00174, regularization loss: 0.29020, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 12:03:08.372707: Epoch [ 50/1000] [180/183], total loss: 0.00075, regularization loss: 0.29020, contrastive loss: 0.00075, Loss positive: 0.00000, Loss negative: 0.00075
Recall@1: 0.18839
Recall@2: 0.28629
Recall@4: 0.40749
Recall@8: 0.55081
Recall@16: 0.68974
Recall@32: 0.81009
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [59. 49. 52. 27. 59. 99. 66. 70. 48. 46. 49. 66. 79. 18. 37. 69. 66. 32.
 54. 65. 77. 34. 47. 52. 49. 63. 84. 89. 51. 72. 90. 64. 56. 63. 37. 39.
 70. 58. 53. 73. 65. 79. 93. 67. 52. 41. 59. 42. 38. 61. 69. 40. 74. 74.
 83. 70. 50. 73. 30. 39. 69. 61. 61. 63. 62. 35. 67. 58. 38. 77. 44. 57.
 67. 78. 62. 53. 57. 52. 64. 73. 58. 59. 53. 54. 55. 71. 60. 95. 64. 65.
 40. 51. 54. 68. 71. 51. 40. 52. 72. 63.]
Purity is 0.187
count_cross = [[0. 0. 0. ... 0. 0. 0.]
 [0. 1. 2. ... 1. 4. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 2. 0.]
 [0. 0. 0. ... 2. 7. 0.]]
Mutual information is 1.81385
5924.0
5924
Entropy cluster is 4.57053
Entropy class is 4.60444
normalized_mutual_information is 0.39539
tp_and_fp = 184237.0
tp = 14977.0
fp is 169260.0
fn is 157773.0
RI is 0.9813591894995453
Precision is 0.08129203145947882
Recall is 0.08669753979739508
F_1 is 0.08390781737150091

normalized_mutual_information = 0.3953916416991141
RI = 0.9813591894995453
F_1 = 0.08390781737150091

The NN is 0.18839
The FT is 0.10167
The ST is 0.16468
The DCG is 0.48327
The E is 0.08363
The MAP 0.07947

2018-10-22 12:04:37.881371: Epoch [ 51/1000] [ 20/183], total loss: 0.00439, regularization loss: 0.29020, contrastive loss: 0.00439, Loss positive: 0.00000, Loss negative: 0.00439
2018-10-22 12:04:47.938398: Epoch [ 51/1000] [ 40/183], total loss: 0.00801, regularization loss: 0.29020, contrastive loss: 0.00801, Loss positive: 0.00767, Loss negative: 0.00034
2018-10-22 12:04:57.982692: Epoch [ 51/1000] [ 60/183], total loss: 0.00029, regularization loss: 0.29020, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 12:05:08.098387: Epoch [ 51/1000] [ 80/183], total loss: 0.00167, regularization loss: 0.29020, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 12:05:18.215774: Epoch [ 51/1000] [100/183], total loss: 0.00622, regularization loss: 0.29020, contrastive loss: 0.00622, Loss positive: 0.00000, Loss negative: 0.00622
2018-10-22 12:05:28.320050: Epoch [ 51/1000] [120/183], total loss: 0.01194, regularization loss: 0.29020, contrastive loss: 0.01194, Loss positive: 0.01096, Loss negative: 0.00098
2018-10-22 12:05:38.474403: Epoch [ 51/1000] [140/183], total loss: 0.01939, regularization loss: 0.29020, contrastive loss: 0.01939, Loss positive: 0.01452, Loss negative: 0.00487
2018-10-22 12:05:48.649884: Epoch [ 51/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:05:58.848998: Epoch [ 51/1000] [180/183], total loss: 0.00017, regularization loss: 0.29020, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 12:06:19.530678: Epoch [ 52/1000] [ 20/183], total loss: 0.00138, regularization loss: 0.29020, contrastive loss: 0.00138, Loss positive: 0.00000, Loss negative: 0.00138
2018-10-22 12:06:29.667862: Epoch [ 52/1000] [ 40/183], total loss: 0.00039, regularization loss: 0.29020, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 12:06:39.778657: Epoch [ 52/1000] [ 60/183], total loss: 0.00026, regularization loss: 0.29020, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 12:06:49.897026: Epoch [ 52/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:07:00.015231: Epoch [ 52/1000] [100/183], total loss: 0.00258, regularization loss: 0.29020, contrastive loss: 0.00258, Loss positive: 0.00000, Loss negative: 0.00258
2018-10-22 12:07:10.286788: Epoch [ 52/1000] [120/183], total loss: 0.00144, regularization loss: 0.29020, contrastive loss: 0.00144, Loss positive: 0.00000, Loss negative: 0.00144
2018-10-22 12:07:20.494835: Epoch [ 52/1000] [140/183], total loss: 0.00003, regularization loss: 0.29020, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 12:07:30.734296: Epoch [ 52/1000] [160/183], total loss: 0.00213, regularization loss: 0.29020, contrastive loss: 0.00213, Loss positive: 0.00000, Loss negative: 0.00213
2018-10-22 12:07:40.991414: Epoch [ 52/1000] [180/183], total loss: 0.04614, regularization loss: 0.29020, contrastive loss: 0.04614, Loss positive: 0.04588, Loss negative: 0.00027
2018-10-22 12:08:02.021724: Epoch [ 53/1000] [ 20/183], total loss: 0.00265, regularization loss: 0.29020, contrastive loss: 0.00265, Loss positive: 0.00000, Loss negative: 0.00265
2018-10-22 12:08:12.156816: Epoch [ 53/1000] [ 40/183], total loss: 0.00089, regularization loss: 0.29020, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 12:08:22.292853: Epoch [ 53/1000] [ 60/183], total loss: 0.02537, regularization loss: 0.29020, contrastive loss: 0.02537, Loss positive: 0.01764, Loss negative: 0.00773
2018-10-22 12:08:32.420297: Epoch [ 53/1000] [ 80/183], total loss: 0.00206, regularization loss: 0.29020, contrastive loss: 0.00206, Loss positive: 0.00000, Loss negative: 0.00206
2018-10-22 12:08:42.564580: Epoch [ 53/1000] [100/183], total loss: 0.00073, regularization loss: 0.29020, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 12:08:52.998847: Epoch [ 53/1000] [120/183], total loss: 0.02817, regularization loss: 0.29020, contrastive loss: 0.02817, Loss positive: 0.02700, Loss negative: 0.00117
2018-10-22 12:09:03.200835: Epoch [ 53/1000] [140/183], total loss: 0.00130, regularization loss: 0.29020, contrastive loss: 0.00130, Loss positive: 0.00000, Loss negative: 0.00130
2018-10-22 12:09:13.525554: Epoch [ 53/1000] [160/183], total loss: 0.00106, regularization loss: 0.29020, contrastive loss: 0.00106, Loss positive: 0.00000, Loss negative: 0.00106
2018-10-22 12:09:23.848639: Epoch [ 53/1000] [180/183], total loss: 0.00068, regularization loss: 0.29020, contrastive loss: 0.00068, Loss positive: 0.00000, Loss negative: 0.00068
2018-10-22 12:09:44.875821: Epoch [ 54/1000] [ 20/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 12:09:55.022270: Epoch [ 54/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:10:05.139105: Epoch [ 54/1000] [ 60/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 12:10:15.264548: Epoch [ 54/1000] [ 80/183], total loss: 0.01911, regularization loss: 0.29020, contrastive loss: 0.01911, Loss positive: 0.01843, Loss negative: 0.00068
2018-10-22 12:10:25.454387: Epoch [ 54/1000] [100/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 12:10:35.662250: Epoch [ 54/1000] [120/183], total loss: 0.00268, regularization loss: 0.29020, contrastive loss: 0.00268, Loss positive: 0.00000, Loss negative: 0.00268
2018-10-22 12:10:45.898212: Epoch [ 54/1000] [140/183], total loss: 0.00051, regularization loss: 0.29020, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 12:10:56.140857: Epoch [ 54/1000] [160/183], total loss: 0.00077, regularization loss: 0.29020, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 12:11:06.383733: Epoch [ 54/1000] [180/183], total loss: 0.02507, regularization loss: 0.29020, contrastive loss: 0.02507, Loss positive: 0.02448, Loss negative: 0.00060
2018-10-22 12:11:28.894777: Epoch [ 55/1000] [ 20/183], total loss: 0.03103, regularization loss: 0.29020, contrastive loss: 0.03103, Loss positive: 0.03013, Loss negative: 0.00090
2018-10-22 12:11:39.023957: Epoch [ 55/1000] [ 40/183], total loss: 0.00439, regularization loss: 0.29020, contrastive loss: 0.00439, Loss positive: 0.00000, Loss negative: 0.00439
2018-10-22 12:11:49.184104: Epoch [ 55/1000] [ 60/183], total loss: 0.00044, regularization loss: 0.29020, contrastive loss: 0.00044, Loss positive: 0.00000, Loss negative: 0.00044
2018-10-22 12:11:59.368677: Epoch [ 55/1000] [ 80/183], total loss: 0.00167, regularization loss: 0.29020, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 12:12:09.654468: Epoch [ 55/1000] [100/183], total loss: 0.00210, regularization loss: 0.29020, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 12:12:19.920252: Epoch [ 55/1000] [120/183], total loss: 0.00075, regularization loss: 0.29020, contrastive loss: 0.00075, Loss positive: 0.00000, Loss negative: 0.00075
2018-10-22 12:12:30.169104: Epoch [ 55/1000] [140/183], total loss: 0.00075, regularization loss: 0.29019, contrastive loss: 0.00075, Loss positive: 0.00000, Loss negative: 0.00075
2018-10-22 12:12:40.377540: Epoch [ 55/1000] [160/183], total loss: 0.00244, regularization loss: 0.29019, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 12:12:50.646838: Epoch [ 55/1000] [180/183], total loss: 0.00128, regularization loss: 0.29020, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 12:13:13.335307: Epoch [ 56/1000] [ 20/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 12:13:23.517146: Epoch [ 56/1000] [ 40/183], total loss: 0.00057, regularization loss: 0.29020, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 12:13:33.703553: Epoch [ 56/1000] [ 60/183], total loss: 0.00086, regularization loss: 0.29020, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 12:13:43.895094: Epoch [ 56/1000] [ 80/183], total loss: 0.00234, regularization loss: 0.29020, contrastive loss: 0.00234, Loss positive: 0.00000, Loss negative: 0.00234
2018-10-22 12:13:54.213040: Epoch [ 56/1000] [100/183], total loss: 0.00239, regularization loss: 0.29020, contrastive loss: 0.00239, Loss positive: 0.00000, Loss negative: 0.00239
2018-10-22 12:14:04.491789: Epoch [ 56/1000] [120/183], total loss: 0.00165, regularization loss: 0.29020, contrastive loss: 0.00165, Loss positive: 0.00000, Loss negative: 0.00165
2018-10-22 12:14:14.733624: Epoch [ 56/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:14:24.945217: Epoch [ 56/1000] [160/183], total loss: 0.07556, regularization loss: 0.29020, contrastive loss: 0.07556, Loss positive: 0.07532, Loss negative: 0.00024
2018-10-22 12:14:35.224170: Epoch [ 56/1000] [180/183], total loss: 0.00287, regularization loss: 0.29019, contrastive loss: 0.00287, Loss positive: 0.00000, Loss negative: 0.00287
2018-10-22 12:14:58.126416: Epoch [ 57/1000] [ 20/183], total loss: 0.04001, regularization loss: 0.29019, contrastive loss: 0.04001, Loss positive: 0.03784, Loss negative: 0.00217
2018-10-22 12:15:08.289980: Epoch [ 57/1000] [ 40/183], total loss: 0.02277, regularization loss: 0.29019, contrastive loss: 0.02277, Loss positive: 0.02125, Loss negative: 0.00152
2018-10-22 12:15:18.437121: Epoch [ 57/1000] [ 60/183], total loss: 0.00250, regularization loss: 0.29020, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 12:15:28.665760: Epoch [ 57/1000] [ 80/183], total loss: 0.00151, regularization loss: 0.29019, contrastive loss: 0.00151, Loss positive: 0.00000, Loss negative: 0.00151
2018-10-22 12:15:38.879504: Epoch [ 57/1000] [100/183], total loss: 0.02067, regularization loss: 0.29020, contrastive loss: 0.02067, Loss positive: 0.02037, Loss negative: 0.00031
2018-10-22 12:15:49.249106: Epoch [ 57/1000] [120/183], total loss: 0.01417, regularization loss: 0.29019, contrastive loss: 0.01417, Loss positive: 0.01415, Loss negative: 0.00002
2018-10-22 12:15:59.473704: Epoch [ 57/1000] [140/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 12:16:09.697725: Epoch [ 57/1000] [160/183], total loss: 0.00277, regularization loss: 0.29019, contrastive loss: 0.00277, Loss positive: 0.00000, Loss negative: 0.00277
2018-10-22 12:16:19.957661: Epoch [ 57/1000] [180/183], total loss: 0.00020, regularization loss: 0.29020, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 12:16:42.757220: Epoch [ 58/1000] [ 20/183], total loss: 0.04672, regularization loss: 0.29020, contrastive loss: 0.04672, Loss positive: 0.04672, Loss negative: 0.00000
2018-10-22 12:16:52.896916: Epoch [ 58/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:17:03.064599: Epoch [ 58/1000] [ 60/183], total loss: 0.00152, regularization loss: 0.29019, contrastive loss: 0.00152, Loss positive: 0.00000, Loss negative: 0.00152
2018-10-22 12:17:13.209124: Epoch [ 58/1000] [ 80/183], total loss: 0.00488, regularization loss: 0.29019, contrastive loss: 0.00488, Loss positive: 0.00000, Loss negative: 0.00488
2018-10-22 12:17:23.441718: Epoch [ 58/1000] [100/183], total loss: 0.02274, regularization loss: 0.29020, contrastive loss: 0.02274, Loss positive: 0.01858, Loss negative: 0.00416
2018-10-22 12:17:33.698099: Epoch [ 58/1000] [120/183], total loss: 0.00411, regularization loss: 0.29020, contrastive loss: 0.00411, Loss positive: 0.00000, Loss negative: 0.00411
2018-10-22 12:17:43.906409: Epoch [ 58/1000] [140/183], total loss: 0.00174, regularization loss: 0.29020, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 12:17:54.176797: Epoch [ 58/1000] [160/183], total loss: 0.02907, regularization loss: 0.29020, contrastive loss: 0.02907, Loss positive: 0.02827, Loss negative: 0.00080
2018-10-22 12:18:04.404226: Epoch [ 58/1000] [180/183], total loss: 0.00250, regularization loss: 0.29020, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 12:18:25.786968: Epoch [ 59/1000] [ 20/183], total loss: 0.02897, regularization loss: 0.29020, contrastive loss: 0.02897, Loss positive: 0.02526, Loss negative: 0.00371
2018-10-22 12:18:35.940221: Epoch [ 59/1000] [ 40/183], total loss: 0.00133, regularization loss: 0.29019, contrastive loss: 0.00133, Loss positive: 0.00000, Loss negative: 0.00133
2018-10-22 12:18:46.071032: Epoch [ 59/1000] [ 60/183], total loss: 0.00842, regularization loss: 0.29019, contrastive loss: 0.00842, Loss positive: 0.00000, Loss negative: 0.00842
2018-10-22 12:18:56.220009: Epoch [ 59/1000] [ 80/183], total loss: 0.05427, regularization loss: 0.29019, contrastive loss: 0.05427, Loss positive: 0.05174, Loss negative: 0.00253
2018-10-22 12:19:06.445214: Epoch [ 59/1000] [100/183], total loss: 0.00017, regularization loss: 0.29019, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 12:19:16.708446: Epoch [ 59/1000] [120/183], total loss: 0.00019, regularization loss: 0.29020, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 12:19:26.936923: Epoch [ 59/1000] [140/183], total loss: 0.02585, regularization loss: 0.29020, contrastive loss: 0.02585, Loss positive: 0.02475, Loss negative: 0.00110
2018-10-22 12:19:37.264262: Epoch [ 59/1000] [160/183], total loss: 0.01546, regularization loss: 0.29020, contrastive loss: 0.01546, Loss positive: 0.01527, Loss negative: 0.00020
2018-10-22 12:19:47.669407: Epoch [ 59/1000] [180/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 12:20:09.147780: Epoch [ 60/1000] [ 20/183], total loss: 0.00210, regularization loss: 0.29020, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 12:20:19.313707: Epoch [ 60/1000] [ 40/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 12:20:29.469868: Epoch [ 60/1000] [ 60/183], total loss: 0.00060, regularization loss: 0.29020, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 12:20:39.665873: Epoch [ 60/1000] [ 80/183], total loss: 0.00563, regularization loss: 0.29020, contrastive loss: 0.00563, Loss positive: 0.00000, Loss negative: 0.00563
2018-10-22 12:20:49.880531: Epoch [ 60/1000] [100/183], total loss: 0.00506, regularization loss: 0.29020, contrastive loss: 0.00506, Loss positive: 0.00000, Loss negative: 0.00506
2018-10-22 12:21:00.104130: Epoch [ 60/1000] [120/183], total loss: 0.00636, regularization loss: 0.29020, contrastive loss: 0.00636, Loss positive: 0.00000, Loss negative: 0.00636
2018-10-22 12:21:10.492570: Epoch [ 60/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:21:20.856630: Epoch [ 60/1000] [160/183], total loss: 0.03366, regularization loss: 0.29020, contrastive loss: 0.03366, Loss positive: 0.03227, Loss negative: 0.00140
2018-10-22 12:21:31.303940: Epoch [ 60/1000] [180/183], total loss: 0.03448, regularization loss: 0.29020, contrastive loss: 0.03448, Loss positive: 0.03280, Loss negative: 0.00169
Recall@1: 0.19126
Recall@2: 0.28916
Recall@4: 0.41745
Recall@8: 0.55520
Recall@16: 0.69142
Recall@32: 0.81263
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [35. 53. 61. 68. 63. 71. 63. 37. 62. 44. 65. 73. 62. 54. 53. 70. 34. 46.
 82. 57. 52. 71. 63. 78. 76. 69. 44. 74. 49. 65. 55. 56. 53. 66. 59. 63.
 90. 76. 52. 48. 61. 61. 59. 59. 59. 57. 36. 59. 39. 19. 55. 64. 71. 52.
 55. 74. 60. 85. 52. 65. 62. 44. 66. 32. 46. 49. 79. 67. 71. 62. 60. 50.
 76. 35. 40. 54. 41. 34. 74. 71. 45. 80. 69. 61. 55. 67. 83. 71. 19. 76.
 47. 78. 71. 63. 67. 58. 67. 82. 69. 29.]
Purity is 0.191
count_cross = [[ 0.  1.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  5.]
 [ 0.  0.  0. ...  1.  2.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  2.  1.  0.]
 [24.  0.  0. ...  0.  0.  0.]]
Mutual information is 1.83688
5924.0
5924
Entropy cluster is 4.57357
Entropy class is 4.60444
normalized_mutual_information is 0.40028
tp_and_fp = 182760.0
tp = 14462.0
fp is 168298.0
fn is 158288.0
RI is 0.9813846684031841
Precision is 0.07913110089735172
Recall is 0.0837163531114327
F_1 is 0.08135917414418722

normalized_mutual_information = 0.40027935449357444
RI = 0.9813846684031841
F_1 = 0.08135917414418722

The NN is 0.19126
The FT is 0.10394
The ST is 0.16759
The DCG is 0.48479
The E is 0.08576
The MAP 0.07958

2018-10-22 12:23:15.794941: Epoch [ 61/1000] [ 20/183], total loss: 0.01395, regularization loss: 0.29020, contrastive loss: 0.01395, Loss positive: 0.01229, Loss negative: 0.00166
2018-10-22 12:23:25.928970: Epoch [ 61/1000] [ 40/183], total loss: 0.00361, regularization loss: 0.29019, contrastive loss: 0.00361, Loss positive: 0.00000, Loss negative: 0.00361
2018-10-22 12:23:36.044250: Epoch [ 61/1000] [ 60/183], total loss: 0.00030, regularization loss: 0.29020, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 12:23:46.219881: Epoch [ 61/1000] [ 80/183], total loss: 0.00148, regularization loss: 0.29020, contrastive loss: 0.00148, Loss positive: 0.00000, Loss negative: 0.00148
2018-10-22 12:23:56.426038: Epoch [ 61/1000] [100/183], total loss: 0.02517, regularization loss: 0.29020, contrastive loss: 0.02517, Loss positive: 0.02238, Loss negative: 0.00279
2018-10-22 12:24:06.637230: Epoch [ 61/1000] [120/183], total loss: 0.00437, regularization loss: 0.29019, contrastive loss: 0.00437, Loss positive: 0.00000, Loss negative: 0.00437
2018-10-22 12:24:16.859341: Epoch [ 61/1000] [140/183], total loss: 0.00277, regularization loss: 0.29020, contrastive loss: 0.00277, Loss positive: 0.00000, Loss negative: 0.00277
2018-10-22 12:24:27.043907: Epoch [ 61/1000] [160/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 12:24:37.392145: Epoch [ 61/1000] [180/183], total loss: 0.00044, regularization loss: 0.29019, contrastive loss: 0.00044, Loss positive: 0.00000, Loss negative: 0.00044
2018-10-22 12:25:00.043715: Epoch [ 62/1000] [ 20/183], total loss: 0.05993, regularization loss: 0.29020, contrastive loss: 0.05993, Loss positive: 0.05851, Loss negative: 0.00142
2018-10-22 12:25:10.167617: Epoch [ 62/1000] [ 40/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 12:25:20.268901: Epoch [ 62/1000] [ 60/183], total loss: 0.04892, regularization loss: 0.29019, contrastive loss: 0.04892, Loss positive: 0.04632, Loss negative: 0.00260
2018-10-22 12:25:30.384344: Epoch [ 62/1000] [ 80/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 12:25:40.553746: Epoch [ 62/1000] [100/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 12:25:50.787393: Epoch [ 62/1000] [120/183], total loss: 0.00059, regularization loss: 0.29019, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 12:26:00.996168: Epoch [ 62/1000] [140/183], total loss: 0.02077, regularization loss: 0.29019, contrastive loss: 0.02077, Loss positive: 0.01711, Loss negative: 0.00366
2018-10-22 12:26:11.238776: Epoch [ 62/1000] [160/183], total loss: 0.04557, regularization loss: 0.29019, contrastive loss: 0.04557, Loss positive: 0.04446, Loss negative: 0.00111
2018-10-22 12:26:21.470985: Epoch [ 62/1000] [180/183], total loss: 0.03305, regularization loss: 0.29019, contrastive loss: 0.03305, Loss positive: 0.03217, Loss negative: 0.00088
2018-10-22 12:26:42.836446: Epoch [ 63/1000] [ 20/183], total loss: 0.00269, regularization loss: 0.29019, contrastive loss: 0.00269, Loss positive: 0.00000, Loss negative: 0.00269
2018-10-22 12:26:52.974174: Epoch [ 63/1000] [ 40/183], total loss: 0.00022, regularization loss: 0.29019, contrastive loss: 0.00022, Loss positive: 0.00000, Loss negative: 0.00022
2018-10-22 12:27:03.127647: Epoch [ 63/1000] [ 60/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 12:27:13.290917: Epoch [ 63/1000] [ 80/183], total loss: 0.00028, regularization loss: 0.29019, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 12:27:23.530170: Epoch [ 63/1000] [100/183], total loss: 0.00021, regularization loss: 0.29019, contrastive loss: 0.00021, Loss positive: 0.00000, Loss negative: 0.00021
2018-10-22 12:27:33.746615: Epoch [ 63/1000] [120/183], total loss: 0.00145, regularization loss: 0.29019, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 12:27:44.210399: Epoch [ 63/1000] [140/183], total loss: 0.00494, regularization loss: 0.29019, contrastive loss: 0.00494, Loss positive: 0.00000, Loss negative: 0.00494
2018-10-22 12:27:54.438380: Epoch [ 63/1000] [160/183], total loss: 0.00429, regularization loss: 0.29019, contrastive loss: 0.00429, Loss positive: 0.00000, Loss negative: 0.00429
2018-10-22 12:28:04.671559: Epoch [ 63/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:28:26.144178: Epoch [ 64/1000] [ 20/183], total loss: 0.00228, regularization loss: 0.29019, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 12:28:36.274834: Epoch [ 64/1000] [ 40/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 12:28:46.417234: Epoch [ 64/1000] [ 60/183], total loss: 0.00108, regularization loss: 0.29019, contrastive loss: 0.00108, Loss positive: 0.00000, Loss negative: 0.00108
2018-10-22 12:28:56.580549: Epoch [ 64/1000] [ 80/183], total loss: 0.00283, regularization loss: 0.29019, contrastive loss: 0.00283, Loss positive: 0.00000, Loss negative: 0.00283
2018-10-22 12:29:06.932434: Epoch [ 64/1000] [100/183], total loss: 0.00078, regularization loss: 0.29019, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 12:29:17.250779: Epoch [ 64/1000] [120/183], total loss: 0.04064, regularization loss: 0.29019, contrastive loss: 0.04064, Loss positive: 0.04064, Loss negative: 0.00000
2018-10-22 12:29:27.465568: Epoch [ 64/1000] [140/183], total loss: 0.01221, regularization loss: 0.29019, contrastive loss: 0.01221, Loss positive: 0.01177, Loss negative: 0.00044
2018-10-22 12:29:37.703693: Epoch [ 64/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:29:47.913629: Epoch [ 64/1000] [180/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 12:30:09.257095: Epoch [ 65/1000] [ 20/183], total loss: 0.00051, regularization loss: 0.29019, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 12:30:19.398071: Epoch [ 65/1000] [ 40/183], total loss: 0.05093, regularization loss: 0.29019, contrastive loss: 0.05093, Loss positive: 0.05042, Loss negative: 0.00051
2018-10-22 12:30:29.594942: Epoch [ 65/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:30:39.908455: Epoch [ 65/1000] [ 80/183], total loss: 0.00074, regularization loss: 0.29019, contrastive loss: 0.00074, Loss positive: 0.00000, Loss negative: 0.00074
2018-10-22 12:30:50.180601: Epoch [ 65/1000] [100/183], total loss: 0.02569, regularization loss: 0.29019, contrastive loss: 0.02569, Loss positive: 0.02454, Loss negative: 0.00115
2018-10-22 12:31:00.418343: Epoch [ 65/1000] [120/183], total loss: 0.00121, regularization loss: 0.29019, contrastive loss: 0.00121, Loss positive: 0.00000, Loss negative: 0.00121
2018-10-22 12:31:10.692276: Epoch [ 65/1000] [140/183], total loss: 0.00458, regularization loss: 0.29019, contrastive loss: 0.00458, Loss positive: 0.00000, Loss negative: 0.00458
2018-10-22 12:31:21.022204: Epoch [ 65/1000] [160/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 12:31:31.365233: Epoch [ 65/1000] [180/183], total loss: 0.00136, regularization loss: 0.29019, contrastive loss: 0.00136, Loss positive: 0.00000, Loss negative: 0.00136
2018-10-22 12:31:56.086207: Epoch [ 66/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:32:06.238596: Epoch [ 66/1000] [ 40/183], total loss: 0.00053, regularization loss: 0.29019, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 12:32:16.452153: Epoch [ 66/1000] [ 60/183], total loss: 0.00299, regularization loss: 0.29019, contrastive loss: 0.00299, Loss positive: 0.00000, Loss negative: 0.00299
2018-10-22 12:32:26.710655: Epoch [ 66/1000] [ 80/183], total loss: 0.00111, regularization loss: 0.29019, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 12:32:36.924541: Epoch [ 66/1000] [100/183], total loss: 0.00589, regularization loss: 0.29019, contrastive loss: 0.00589, Loss positive: 0.00000, Loss negative: 0.00589
2018-10-22 12:32:47.314639: Epoch [ 66/1000] [120/183], total loss: 0.00242, regularization loss: 0.29019, contrastive loss: 0.00242, Loss positive: 0.00000, Loss negative: 0.00242
2018-10-22 12:32:57.586671: Epoch [ 66/1000] [140/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 12:33:07.842279: Epoch [ 66/1000] [160/183], total loss: 0.00232, regularization loss: 0.29019, contrastive loss: 0.00232, Loss positive: 0.00000, Loss negative: 0.00232
2018-10-22 12:33:18.263674: Epoch [ 66/1000] [180/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 12:33:42.654636: Epoch [ 67/1000] [ 20/183], total loss: 0.02006, regularization loss: 0.29019, contrastive loss: 0.02006, Loss positive: 0.01583, Loss negative: 0.00423
2018-10-22 12:33:52.889534: Epoch [ 67/1000] [ 40/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 12:34:03.069680: Epoch [ 67/1000] [ 60/183], total loss: 0.00195, regularization loss: 0.29019, contrastive loss: 0.00195, Loss positive: 0.00000, Loss negative: 0.00195
2018-10-22 12:34:13.257646: Epoch [ 67/1000] [ 80/183], total loss: 0.02198, regularization loss: 0.29019, contrastive loss: 0.02198, Loss positive: 0.01673, Loss negative: 0.00525
2018-10-22 12:34:23.481289: Epoch [ 67/1000] [100/183], total loss: 0.00099, regularization loss: 0.29019, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 12:34:33.735105: Epoch [ 67/1000] [120/183], total loss: 0.00339, regularization loss: 0.29019, contrastive loss: 0.00339, Loss positive: 0.00000, Loss negative: 0.00339
2018-10-22 12:34:43.958815: Epoch [ 67/1000] [140/183], total loss: 0.02797, regularization loss: 0.29019, contrastive loss: 0.02797, Loss positive: 0.02787, Loss negative: 0.00010
2018-10-22 12:34:54.193363: Epoch [ 67/1000] [160/183], total loss: 0.04635, regularization loss: 0.29019, contrastive loss: 0.04635, Loss positive: 0.04360, Loss negative: 0.00275
2018-10-22 12:35:04.425450: Epoch [ 67/1000] [180/183], total loss: 0.01150, regularization loss: 0.29019, contrastive loss: 0.01150, Loss positive: 0.01078, Loss negative: 0.00072
2018-10-22 12:35:25.850697: Epoch [ 68/1000] [ 20/183], total loss: 0.04917, regularization loss: 0.29019, contrastive loss: 0.04917, Loss positive: 0.04379, Loss negative: 0.00538
2018-10-22 12:35:36.031183: Epoch [ 68/1000] [ 40/183], total loss: 0.00355, regularization loss: 0.29019, contrastive loss: 0.00355, Loss positive: 0.00000, Loss negative: 0.00355
2018-10-22 12:35:46.221128: Epoch [ 68/1000] [ 60/183], total loss: 0.00309, regularization loss: 0.29019, contrastive loss: 0.00309, Loss positive: 0.00000, Loss negative: 0.00309
2018-10-22 12:35:56.374235: Epoch [ 68/1000] [ 80/183], total loss: 0.00017, regularization loss: 0.29019, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 12:36:06.624644: Epoch [ 68/1000] [100/183], total loss: 0.00356, regularization loss: 0.29019, contrastive loss: 0.00356, Loss positive: 0.00000, Loss negative: 0.00356
2018-10-22 12:36:16.840656: Epoch [ 68/1000] [120/183], total loss: 0.01305, regularization loss: 0.29019, contrastive loss: 0.01305, Loss positive: 0.01304, Loss negative: 0.00002
2018-10-22 12:36:27.073151: Epoch [ 68/1000] [140/183], total loss: 0.03533, regularization loss: 0.29019, contrastive loss: 0.03533, Loss positive: 0.03253, Loss negative: 0.00280
2018-10-22 12:36:37.310298: Epoch [ 68/1000] [160/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 12:36:47.535765: Epoch [ 68/1000] [180/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 12:37:09.042276: Epoch [ 69/1000] [ 20/183], total loss: 0.00024, regularization loss: 0.29019, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 12:37:19.184478: Epoch [ 69/1000] [ 40/183], total loss: 0.06212, regularization loss: 0.29019, contrastive loss: 0.06212, Loss positive: 0.05912, Loss negative: 0.00300
2018-10-22 12:37:29.309684: Epoch [ 69/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:37:39.517659: Epoch [ 69/1000] [ 80/183], total loss: 0.00067, regularization loss: 0.29019, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 12:37:49.711783: Epoch [ 69/1000] [100/183], total loss: 0.00019, regularization loss: 0.29019, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 12:37:59.940738: Epoch [ 69/1000] [120/183], total loss: 0.00289, regularization loss: 0.29019, contrastive loss: 0.00289, Loss positive: 0.00000, Loss negative: 0.00289
2018-10-22 12:38:10.188393: Epoch [ 69/1000] [140/183], total loss: 0.01100, regularization loss: 0.29019, contrastive loss: 0.01100, Loss positive: 0.00000, Loss negative: 0.01100
2018-10-22 12:38:20.449769: Epoch [ 69/1000] [160/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 12:38:30.672337: Epoch [ 69/1000] [180/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 12:38:52.462849: Epoch [ 70/1000] [ 20/183], total loss: 0.01107, regularization loss: 0.29019, contrastive loss: 0.01107, Loss positive: 0.00000, Loss negative: 0.01107
2018-10-22 12:39:02.576127: Epoch [ 70/1000] [ 40/183], total loss: 0.01419, regularization loss: 0.29019, contrastive loss: 0.01419, Loss positive: 0.01343, Loss negative: 0.00076
2018-10-22 12:39:12.776559: Epoch [ 70/1000] [ 60/183], total loss: 0.00091, regularization loss: 0.29019, contrastive loss: 0.00091, Loss positive: 0.00000, Loss negative: 0.00091
2018-10-22 12:39:23.028233: Epoch [ 70/1000] [ 80/183], total loss: 0.02145, regularization loss: 0.29019, contrastive loss: 0.02145, Loss positive: 0.01795, Loss negative: 0.00351
2018-10-22 12:39:33.249975: Epoch [ 70/1000] [100/183], total loss: 0.02115, regularization loss: 0.29019, contrastive loss: 0.02115, Loss positive: 0.02094, Loss negative: 0.00021
2018-10-22 12:39:43.498296: Epoch [ 70/1000] [120/183], total loss: 0.04614, regularization loss: 0.29019, contrastive loss: 0.04614, Loss positive: 0.04422, Loss negative: 0.00193
2018-10-22 12:39:53.734077: Epoch [ 70/1000] [140/183], total loss: 0.00168, regularization loss: 0.29019, contrastive loss: 0.00168, Loss positive: 0.00000, Loss negative: 0.00168
2018-10-22 12:40:04.010698: Epoch [ 70/1000] [160/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 12:40:14.234989: Epoch [ 70/1000] [180/183], total loss: 0.00360, regularization loss: 0.29019, contrastive loss: 0.00360, Loss positive: 0.00000, Loss negative: 0.00360
Recall@1: 0.18991
Recall@2: 0.28663
Recall@4: 0.41053
Recall@8: 0.54878
Recall@16: 0.69683
Recall@32: 0.82444
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 42.  35.  78.  81.  75.  58.  66.  38.  45.  60.  73.  67.  45. 100.
  68.  68.  48.  56.  47.  75.  70.  38.  70.  44.  70.  77.  75.  39.
  40.  25.  93.  60.  50. 107.  33.  56.  60.  78.  87.  97.  99.  50.
  34.  43.  47.  38.  63. 106.  62.  31.  37.  50.  45.  46.  67.  40.
  69.  55. 102.  54.  63.  30.  91.  53.  65.  35.  48.  52.  28.  60.
  67.  58.  67.  73.  67.  63.  31.  57.  75.  33.  74.  50.  69.  55.
  48.  62.  30.  72.  25.  45.  38.  97.  24.  74.  29.  71.  75.  82.
  92.  64.]
Purity is 0.188
count_cross = [[ 0.  1.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  3.]
 [ 0. 14.  5. ...  4.  4.  0.]
 ...
 [ 0.  0.  0. ...  0.  1.  0.]
 [ 0.  0.  0. ...  0.  1.  0.]
 [ 0.  0.  0. ...  2.  4.  0.]]
Mutual information is 1.79571
5924.0
5924
Entropy cluster is 4.54728
Entropy class is 4.60444
normalized_mutual_information is 0.39243
tp_and_fp = 192661.0
tp = 14003.0
fp is 178658.0
fn is 158747.0
RI is 0.9807679877354704
Precision is 0.07268206850374492
Recall is 0.08105933429811867
F_1 is 0.07664246560722035

normalized_mutual_information = 0.39243022362452595
RI = 0.9807679877354704
F_1 = 0.07664246560722035

The NN is 0.18991
The FT is 0.10118
The ST is 0.16153
The DCG is 0.47950
The E is 0.08394
The MAP 0.07572

2018-10-22 12:41:54.541929: Epoch [ 71/1000] [ 20/183], total loss: 0.02085, regularization loss: 0.29019, contrastive loss: 0.02085, Loss positive: 0.01858, Loss negative: 0.00227
2018-10-22 12:42:04.629502: Epoch [ 71/1000] [ 40/183], total loss: 0.00564, regularization loss: 0.29019, contrastive loss: 0.00564, Loss positive: 0.00000, Loss negative: 0.00564
2018-10-22 12:42:14.781232: Epoch [ 71/1000] [ 60/183], total loss: 0.00164, regularization loss: 0.29019, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 12:42:24.951818: Epoch [ 71/1000] [ 80/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 12:42:35.116803: Epoch [ 71/1000] [100/183], total loss: 0.03306, regularization loss: 0.29019, contrastive loss: 0.03306, Loss positive: 0.03306, Loss negative: 0.00000
2018-10-22 12:42:45.333747: Epoch [ 71/1000] [120/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 12:42:55.559832: Epoch [ 71/1000] [140/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 12:43:05.920857: Epoch [ 71/1000] [160/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 12:43:16.231688: Epoch [ 71/1000] [180/183], total loss: 0.01369, regularization loss: 0.29019, contrastive loss: 0.01369, Loss positive: 0.01369, Loss negative: 0.00000
2018-10-22 12:43:40.859601: Epoch [ 72/1000] [ 20/183], total loss: 0.00057, regularization loss: 0.29019, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 12:43:51.065403: Epoch [ 72/1000] [ 40/183], total loss: 0.02400, regularization loss: 0.29019, contrastive loss: 0.02400, Loss positive: 0.02400, Loss negative: 0.00000
2018-10-22 12:44:01.239111: Epoch [ 72/1000] [ 60/183], total loss: 0.01711, regularization loss: 0.29019, contrastive loss: 0.01711, Loss positive: 0.01427, Loss negative: 0.00284
2018-10-22 12:44:11.365967: Epoch [ 72/1000] [ 80/183], total loss: 0.02588, regularization loss: 0.29019, contrastive loss: 0.02588, Loss positive: 0.02330, Loss negative: 0.00258
2018-10-22 12:44:21.568631: Epoch [ 72/1000] [100/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 12:44:31.932229: Epoch [ 72/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:44:42.197047: Epoch [ 72/1000] [140/183], total loss: 0.04751, regularization loss: 0.29019, contrastive loss: 0.04751, Loss positive: 0.04748, Loss negative: 0.00003
2018-10-22 12:44:52.446863: Epoch [ 72/1000] [160/183], total loss: 0.00632, regularization loss: 0.29019, contrastive loss: 0.00632, Loss positive: 0.00000, Loss negative: 0.00632
2018-10-22 12:45:02.682955: Epoch [ 72/1000] [180/183], total loss: 0.00063, regularization loss: 0.29019, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 12:45:24.140062: Epoch [ 73/1000] [ 20/183], total loss: 0.00145, regularization loss: 0.29019, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 12:45:34.304686: Epoch [ 73/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:45:44.424990: Epoch [ 73/1000] [ 60/183], total loss: 0.00210, regularization loss: 0.29019, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 12:45:54.615646: Epoch [ 73/1000] [ 80/183], total loss: 0.01743, regularization loss: 0.29019, contrastive loss: 0.01743, Loss positive: 0.01689, Loss negative: 0.00054
2018-10-22 12:46:04.853365: Epoch [ 73/1000] [100/183], total loss: 0.00437, regularization loss: 0.29019, contrastive loss: 0.00437, Loss positive: 0.00000, Loss negative: 0.00437
2018-10-22 12:46:15.065572: Epoch [ 73/1000] [120/183], total loss: 0.03713, regularization loss: 0.29019, contrastive loss: 0.03713, Loss positive: 0.03561, Loss negative: 0.00152
2018-10-22 12:46:25.383903: Epoch [ 73/1000] [140/183], total loss: 0.00063, regularization loss: 0.29019, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 12:46:35.754738: Epoch [ 73/1000] [160/183], total loss: 0.00401, regularization loss: 0.29019, contrastive loss: 0.00401, Loss positive: 0.00000, Loss negative: 0.00401
2018-10-22 12:46:46.110136: Epoch [ 73/1000] [180/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 12:47:07.557916: Epoch [ 74/1000] [ 20/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 12:47:17.661054: Epoch [ 74/1000] [ 40/183], total loss: 0.00060, regularization loss: 0.29019, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 12:47:27.783948: Epoch [ 74/1000] [ 60/183], total loss: 0.04073, regularization loss: 0.29019, contrastive loss: 0.04073, Loss positive: 0.04073, Loss negative: 0.00000
2018-10-22 12:47:38.114129: Epoch [ 74/1000] [ 80/183], total loss: 0.03955, regularization loss: 0.29019, contrastive loss: 0.03955, Loss positive: 0.03955, Loss negative: 0.00000
2018-10-22 12:47:48.510734: Epoch [ 74/1000] [100/183], total loss: 0.00078, regularization loss: 0.29019, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 12:47:58.912257: Epoch [ 74/1000] [120/183], total loss: 0.00137, regularization loss: 0.29019, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 12:48:09.150982: Epoch [ 74/1000] [140/183], total loss: 0.00105, regularization loss: 0.29019, contrastive loss: 0.00105, Loss positive: 0.00000, Loss negative: 0.00105
2018-10-22 12:48:19.402389: Epoch [ 74/1000] [160/183], total loss: 0.00223, regularization loss: 0.29019, contrastive loss: 0.00223, Loss positive: 0.00000, Loss negative: 0.00223
2018-10-22 12:48:29.646156: Epoch [ 74/1000] [180/183], total loss: 0.02097, regularization loss: 0.29019, contrastive loss: 0.02097, Loss positive: 0.01637, Loss negative: 0.00460
2018-10-22 12:48:51.050954: Epoch [ 75/1000] [ 20/183], total loss: 0.02417, regularization loss: 0.29019, contrastive loss: 0.02417, Loss positive: 0.02416, Loss negative: 0.00001
2018-10-22 12:49:01.198681: Epoch [ 75/1000] [ 40/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 12:49:11.394212: Epoch [ 75/1000] [ 60/183], total loss: 0.04906, regularization loss: 0.29019, contrastive loss: 0.04906, Loss positive: 0.04825, Loss negative: 0.00080
2018-10-22 12:49:21.651789: Epoch [ 75/1000] [ 80/183], total loss: 0.00036, regularization loss: 0.29019, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 12:49:31.900685: Epoch [ 75/1000] [100/183], total loss: 0.00329, regularization loss: 0.29019, contrastive loss: 0.00329, Loss positive: 0.00000, Loss negative: 0.00329
2018-10-22 12:49:42.119155: Epoch [ 75/1000] [120/183], total loss: 0.00473, regularization loss: 0.29019, contrastive loss: 0.00473, Loss positive: 0.00000, Loss negative: 0.00473
2018-10-22 12:49:52.374748: Epoch [ 75/1000] [140/183], total loss: 0.00352, regularization loss: 0.29019, contrastive loss: 0.00352, Loss positive: 0.00000, Loss negative: 0.00352
2018-10-22 12:50:02.604009: Epoch [ 75/1000] [160/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 12:50:12.860894: Epoch [ 75/1000] [180/183], total loss: 0.00042, regularization loss: 0.29019, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 12:50:35.410370: Epoch [ 76/1000] [ 20/183], total loss: 0.00334, regularization loss: 0.29019, contrastive loss: 0.00334, Loss positive: 0.00000, Loss negative: 0.00334
2018-10-22 12:50:45.569261: Epoch [ 76/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:50:55.748134: Epoch [ 76/1000] [ 60/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 12:51:06.111359: Epoch [ 76/1000] [ 80/183], total loss: 0.02565, regularization loss: 0.29019, contrastive loss: 0.02565, Loss positive: 0.02544, Loss negative: 0.00021
2018-10-22 12:51:16.377402: Epoch [ 76/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:51:26.731749: Epoch [ 76/1000] [120/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 12:51:37.244282: Epoch [ 76/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:51:47.506089: Epoch [ 76/1000] [160/183], total loss: 0.00222, regularization loss: 0.29019, contrastive loss: 0.00222, Loss positive: 0.00000, Loss negative: 0.00222
2018-10-22 12:51:57.874482: Epoch [ 76/1000] [180/183], total loss: 0.01392, regularization loss: 0.29019, contrastive loss: 0.01392, Loss positive: 0.00854, Loss negative: 0.00538
2018-10-22 12:52:22.651757: Epoch [ 77/1000] [ 20/183], total loss: 0.00863, regularization loss: 0.29019, contrastive loss: 0.00863, Loss positive: 0.00000, Loss negative: 0.00863
2018-10-22 12:52:32.868518: Epoch [ 77/1000] [ 40/183], total loss: 0.00052, regularization loss: 0.29019, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 12:52:43.090256: Epoch [ 77/1000] [ 60/183], total loss: 0.00459, regularization loss: 0.29019, contrastive loss: 0.00459, Loss positive: 0.00000, Loss negative: 0.00459
2018-10-22 12:52:53.296098: Epoch [ 77/1000] [ 80/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 12:53:03.573401: Epoch [ 77/1000] [100/183], total loss: 0.01465, regularization loss: 0.29019, contrastive loss: 0.01465, Loss positive: 0.01225, Loss negative: 0.00240
2018-10-22 12:53:13.927139: Epoch [ 77/1000] [120/183], total loss: 0.00072, regularization loss: 0.29019, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 12:53:24.174603: Epoch [ 77/1000] [140/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 12:53:34.627572: Epoch [ 77/1000] [160/183], total loss: 0.00625, regularization loss: 0.29019, contrastive loss: 0.00625, Loss positive: 0.00000, Loss negative: 0.00625
2018-10-22 12:53:45.055757: Epoch [ 77/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:54:08.658708: Epoch [ 78/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:54:18.840789: Epoch [ 78/1000] [ 40/183], total loss: 0.00746, regularization loss: 0.29019, contrastive loss: 0.00746, Loss positive: 0.00000, Loss negative: 0.00746
2018-10-22 12:54:29.028861: Epoch [ 78/1000] [ 60/183], total loss: 0.00131, regularization loss: 0.29019, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 12:54:39.214962: Epoch [ 78/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:54:49.460874: Epoch [ 78/1000] [100/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 12:54:59.699684: Epoch [ 78/1000] [120/183], total loss: 0.00303, regularization loss: 0.29019, contrastive loss: 0.00303, Loss positive: 0.00000, Loss negative: 0.00303
2018-10-22 12:55:09.935940: Epoch [ 78/1000] [140/183], total loss: 0.02835, regularization loss: 0.29019, contrastive loss: 0.02835, Loss positive: 0.02745, Loss negative: 0.00090
2018-10-22 12:55:20.201583: Epoch [ 78/1000] [160/183], total loss: 0.00228, regularization loss: 0.29019, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 12:55:30.421095: Epoch [ 78/1000] [180/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 12:55:51.957087: Epoch [ 79/1000] [ 20/183], total loss: 0.00065, regularization loss: 0.29019, contrastive loss: 0.00065, Loss positive: 0.00000, Loss negative: 0.00065
2018-10-22 12:56:02.074228: Epoch [ 79/1000] [ 40/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 12:56:12.238701: Epoch [ 79/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:56:22.446370: Epoch [ 79/1000] [ 80/183], total loss: 0.00216, regularization loss: 0.29019, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 12:56:32.677854: Epoch [ 79/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:56:42.915028: Epoch [ 79/1000] [120/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 12:56:53.140197: Epoch [ 79/1000] [140/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 12:57:03.374236: Epoch [ 79/1000] [160/183], total loss: 0.00517, regularization loss: 0.29019, contrastive loss: 0.00517, Loss positive: 0.00000, Loss negative: 0.00517
2018-10-22 12:57:13.630051: Epoch [ 79/1000] [180/183], total loss: 0.00473, regularization loss: 0.29019, contrastive loss: 0.00473, Loss positive: 0.00000, Loss negative: 0.00473
2018-10-22 12:57:35.008409: Epoch [ 80/1000] [ 20/183], total loss: 0.00070, regularization loss: 0.29019, contrastive loss: 0.00070, Loss positive: 0.00000, Loss negative: 0.00070
2018-10-22 12:57:45.174996: Epoch [ 80/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:57:55.351519: Epoch [ 80/1000] [ 60/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 12:58:05.540848: Epoch [ 80/1000] [ 80/183], total loss: 0.00057, regularization loss: 0.29019, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 12:58:15.792140: Epoch [ 80/1000] [100/183], total loss: 0.00585, regularization loss: 0.29019, contrastive loss: 0.00585, Loss positive: 0.00000, Loss negative: 0.00585
2018-10-22 12:58:26.038302: Epoch [ 80/1000] [120/183], total loss: 0.00108, regularization loss: 0.29019, contrastive loss: 0.00108, Loss positive: 0.00000, Loss negative: 0.00108
2018-10-22 12:58:36.278020: Epoch [ 80/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 12:58:46.502696: Epoch [ 80/1000] [160/183], total loss: 0.00687, regularization loss: 0.29019, contrastive loss: 0.00687, Loss positive: 0.00000, Loss negative: 0.00687
2018-10-22 12:58:56.759886: Epoch [ 80/1000] [180/183], total loss: 0.00203, regularization loss: 0.29019, contrastive loss: 0.00203, Loss positive: 0.00000, Loss negative: 0.00203
Recall@1: 0.19007
Recall@2: 0.28916
Recall@4: 0.42151
Recall@8: 0.56431
Recall@16: 0.71151
Recall@32: 0.82157
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 56.  50.  43.  60.  66.  64.  74.  39.  50.  70.  85.  48.  50.  57.
  69.  77.  38.  38.  77.  50.  87.  76.  68.  77.  59.  66.  76.  64.
  39.  37.  59.  39.  51.  63.  67.  44.  54.  80.  59.  59.  50.  61.
  50.  53.  58.  84.  44.  45.  42.  24.  56.  70.  49.  71.  51.  84.
  44.  61.  72.  60.  68.  74.  49.  33.  79.  93.  49.  66.  43. 100.
  57.  63.  52.  62.  54.  54.  51.  57.  60.  53.  50.  66.  67.  51.
  53.  56.  63.  79.  43.  41.  64.  51.  36.  53.  70.  52.  71.  60.
  72.  95.]
Purity is 0.199
count_cross = [[ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  1. ...  1.  1.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 1.  0.  0. ...  0.  0.  0.]
 [ 0. 15.  9. ...  4.  1.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]]
Mutual information is 1.87304
5924.0
5924
Entropy cluster is 4.57571
Entropy class is 4.60444
normalized_mutual_information is 0.40806
tp_and_fp = 182862.0
tp = 15586.0
fp is 167276.0
fn is 157164.0
RI is 0.9815069899405641
Precision is 0.08523367348054817
Recall is 0.09022286541244573
F_1 is 0.08765733439816428

normalized_mutual_information = 0.4080633884733772
RI = 0.9815069899405641
F_1 = 0.08765733439816428

The NN is 0.19007
The FT is 0.10599
The ST is 0.17062
The DCG is 0.48755
The E is 0.08789
The MAP 0.08199

2018-10-22 13:00:27.757401: Epoch [ 81/1000] [ 20/183], total loss: 0.00137, regularization loss: 0.29019, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 13:00:37.838603: Epoch [ 81/1000] [ 40/183], total loss: 0.00184, regularization loss: 0.29019, contrastive loss: 0.00184, Loss positive: 0.00000, Loss negative: 0.00184
2018-10-22 13:00:47.934423: Epoch [ 81/1000] [ 60/183], total loss: 0.00090, regularization loss: 0.29019, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 13:00:58.047866: Epoch [ 81/1000] [ 80/183], total loss: 0.00170, regularization loss: 0.29019, contrastive loss: 0.00170, Loss positive: 0.00000, Loss negative: 0.00170
2018-10-22 13:01:08.241792: Epoch [ 81/1000] [100/183], total loss: 0.00110, regularization loss: 0.29019, contrastive loss: 0.00110, Loss positive: 0.00000, Loss negative: 0.00110
2018-10-22 13:01:18.454166: Epoch [ 81/1000] [120/183], total loss: 0.00279, regularization loss: 0.29019, contrastive loss: 0.00279, Loss positive: 0.00000, Loss negative: 0.00279
2018-10-22 13:01:28.839732: Epoch [ 81/1000] [140/183], total loss: 0.01692, regularization loss: 0.29019, contrastive loss: 0.01692, Loss positive: 0.01658, Loss negative: 0.00033
2018-10-22 13:01:39.140144: Epoch [ 81/1000] [160/183], total loss: 0.04362, regularization loss: 0.29019, contrastive loss: 0.04362, Loss positive: 0.03614, Loss negative: 0.00749
2018-10-22 13:01:49.451648: Epoch [ 81/1000] [180/183], total loss: 0.00399, regularization loss: 0.29019, contrastive loss: 0.00399, Loss positive: 0.00000, Loss negative: 0.00399
2018-10-22 13:02:14.656539: Epoch [ 82/1000] [ 20/183], total loss: 0.05992, regularization loss: 0.29019, contrastive loss: 0.05992, Loss positive: 0.05754, Loss negative: 0.00238
2018-10-22 13:02:24.859786: Epoch [ 82/1000] [ 40/183], total loss: 0.00413, regularization loss: 0.29019, contrastive loss: 0.00413, Loss positive: 0.00000, Loss negative: 0.00413
2018-10-22 13:02:35.041742: Epoch [ 82/1000] [ 60/183], total loss: 0.00133, regularization loss: 0.29019, contrastive loss: 0.00133, Loss positive: 0.00000, Loss negative: 0.00133
2018-10-22 13:02:45.273021: Epoch [ 82/1000] [ 80/183], total loss: 0.00152, regularization loss: 0.29019, contrastive loss: 0.00152, Loss positive: 0.00000, Loss negative: 0.00152
2018-10-22 13:02:55.674936: Epoch [ 82/1000] [100/183], total loss: 0.07064, regularization loss: 0.29019, contrastive loss: 0.07064, Loss positive: 0.06836, Loss negative: 0.00228
2018-10-22 13:03:06.017921: Epoch [ 82/1000] [120/183], total loss: 0.02413, regularization loss: 0.29019, contrastive loss: 0.02413, Loss positive: 0.02069, Loss negative: 0.00344
2018-10-22 13:03:16.269786: Epoch [ 82/1000] [140/183], total loss: 0.00673, regularization loss: 0.29019, contrastive loss: 0.00673, Loss positive: 0.00000, Loss negative: 0.00673
2018-10-22 13:03:26.694250: Epoch [ 82/1000] [160/183], total loss: 0.00251, regularization loss: 0.29019, contrastive loss: 0.00251, Loss positive: 0.00000, Loss negative: 0.00251
2018-10-22 13:03:37.052478: Epoch [ 82/1000] [180/183], total loss: 0.00310, regularization loss: 0.29019, contrastive loss: 0.00310, Loss positive: 0.00000, Loss negative: 0.00310
2018-10-22 13:04:01.454866: Epoch [ 83/1000] [ 20/183], total loss: 0.00077, regularization loss: 0.29019, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 13:04:11.652133: Epoch [ 83/1000] [ 40/183], total loss: 0.01046, regularization loss: 0.29019, contrastive loss: 0.01046, Loss positive: 0.00837, Loss negative: 0.00209
2018-10-22 13:04:21.837095: Epoch [ 83/1000] [ 60/183], total loss: 0.03308, regularization loss: 0.29019, contrastive loss: 0.03308, Loss positive: 0.03308, Loss negative: 0.00000
2018-10-22 13:04:32.023627: Epoch [ 83/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:04:42.253719: Epoch [ 83/1000] [100/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 13:04:52.503065: Epoch [ 83/1000] [120/183], total loss: 0.01174, regularization loss: 0.29019, contrastive loss: 0.01174, Loss positive: 0.01065, Loss negative: 0.00109
2018-10-22 13:05:02.761163: Epoch [ 83/1000] [140/183], total loss: 0.04776, regularization loss: 0.29019, contrastive loss: 0.04776, Loss positive: 0.04724, Loss negative: 0.00052
2018-10-22 13:05:12.972014: Epoch [ 83/1000] [160/183], total loss: 0.00302, regularization loss: 0.29019, contrastive loss: 0.00302, Loss positive: 0.00000, Loss negative: 0.00302
2018-10-22 13:05:23.243423: Epoch [ 83/1000] [180/183], total loss: 0.00939, regularization loss: 0.29019, contrastive loss: 0.00939, Loss positive: 0.00000, Loss negative: 0.00939
2018-10-22 13:05:44.657302: Epoch [ 84/1000] [ 20/183], total loss: 0.01424, regularization loss: 0.29019, contrastive loss: 0.01424, Loss positive: 0.01039, Loss negative: 0.00385
2018-10-22 13:05:54.840449: Epoch [ 84/1000] [ 40/183], total loss: 0.00545, regularization loss: 0.29019, contrastive loss: 0.00545, Loss positive: 0.00000, Loss negative: 0.00545
2018-10-22 13:06:05.006789: Epoch [ 84/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:06:15.344214: Epoch [ 84/1000] [ 80/183], total loss: 0.00277, regularization loss: 0.29019, contrastive loss: 0.00277, Loss positive: 0.00000, Loss negative: 0.00277
2018-10-22 13:06:25.606912: Epoch [ 84/1000] [100/183], total loss: 0.00737, regularization loss: 0.29019, contrastive loss: 0.00737, Loss positive: 0.00673, Loss negative: 0.00064
2018-10-22 13:06:35.833974: Epoch [ 84/1000] [120/183], total loss: 0.00219, regularization loss: 0.29019, contrastive loss: 0.00219, Loss positive: 0.00000, Loss negative: 0.00219
2018-10-22 13:06:46.191045: Epoch [ 84/1000] [140/183], total loss: 0.04443, regularization loss: 0.29019, contrastive loss: 0.04443, Loss positive: 0.04443, Loss negative: 0.00000
2018-10-22 13:06:56.417343: Epoch [ 84/1000] [160/183], total loss: 0.00071, regularization loss: 0.29019, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 13:07:06.640585: Epoch [ 84/1000] [180/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 13:07:28.016030: Epoch [ 85/1000] [ 20/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 13:07:38.186374: Epoch [ 85/1000] [ 40/183], total loss: 0.02750, regularization loss: 0.29019, contrastive loss: 0.02750, Loss positive: 0.02414, Loss negative: 0.00336
2018-10-22 13:07:48.534998: Epoch [ 85/1000] [ 60/183], total loss: 0.00137, regularization loss: 0.29019, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 13:07:58.788731: Epoch [ 85/1000] [ 80/183], total loss: 0.01920, regularization loss: 0.29019, contrastive loss: 0.01920, Loss positive: 0.01916, Loss negative: 0.00004
2018-10-22 13:08:09.022658: Epoch [ 85/1000] [100/183], total loss: 0.00815, regularization loss: 0.29019, contrastive loss: 0.00815, Loss positive: 0.00000, Loss negative: 0.00815
2018-10-22 13:08:19.267330: Epoch [ 85/1000] [120/183], total loss: 0.00062, regularization loss: 0.29019, contrastive loss: 0.00062, Loss positive: 0.00000, Loss negative: 0.00062
2018-10-22 13:08:29.484336: Epoch [ 85/1000] [140/183], total loss: 0.00291, regularization loss: 0.29019, contrastive loss: 0.00291, Loss positive: 0.00000, Loss negative: 0.00291
2018-10-22 13:08:39.851878: Epoch [ 85/1000] [160/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 13:08:50.085733: Epoch [ 85/1000] [180/183], total loss: 0.02990, regularization loss: 0.29019, contrastive loss: 0.02990, Loss positive: 0.02977, Loss negative: 0.00013
2018-10-22 13:09:11.613923: Epoch [ 86/1000] [ 20/183], total loss: 0.00199, regularization loss: 0.29019, contrastive loss: 0.00199, Loss positive: 0.00000, Loss negative: 0.00199
2018-10-22 13:09:21.772370: Epoch [ 86/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:09:31.925476: Epoch [ 86/1000] [ 60/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 13:09:42.274192: Epoch [ 86/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:09:52.565118: Epoch [ 86/1000] [100/183], total loss: 0.00467, regularization loss: 0.29019, contrastive loss: 0.00467, Loss positive: 0.00000, Loss negative: 0.00467
2018-10-22 13:10:02.773127: Epoch [ 86/1000] [120/183], total loss: 0.00412, regularization loss: 0.29019, contrastive loss: 0.00412, Loss positive: 0.00000, Loss negative: 0.00412
2018-10-22 13:10:13.022398: Epoch [ 86/1000] [140/183], total loss: 0.00058, regularization loss: 0.29019, contrastive loss: 0.00058, Loss positive: 0.00000, Loss negative: 0.00058
2018-10-22 13:10:23.265339: Epoch [ 86/1000] [160/183], total loss: 0.00896, regularization loss: 0.29019, contrastive loss: 0.00896, Loss positive: 0.00000, Loss negative: 0.00896
2018-10-22 13:10:33.518181: Epoch [ 86/1000] [180/183], total loss: 0.00117, regularization loss: 0.29019, contrastive loss: 0.00117, Loss positive: 0.00000, Loss negative: 0.00117
2018-10-22 13:10:56.026513: Epoch [ 87/1000] [ 20/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 13:11:06.211415: Epoch [ 87/1000] [ 40/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 13:11:16.449446: Epoch [ 87/1000] [ 60/183], total loss: 0.00271, regularization loss: 0.29019, contrastive loss: 0.00271, Loss positive: 0.00000, Loss negative: 0.00271
2018-10-22 13:11:26.919084: Epoch [ 87/1000] [ 80/183], total loss: 0.00474, regularization loss: 0.29019, contrastive loss: 0.00474, Loss positive: 0.00000, Loss negative: 0.00474
2018-10-22 13:11:37.226406: Epoch [ 87/1000] [100/183], total loss: 0.00104, regularization loss: 0.29019, contrastive loss: 0.00104, Loss positive: 0.00000, Loss negative: 0.00104
2018-10-22 13:11:47.554983: Epoch [ 87/1000] [120/183], total loss: 0.00211, regularization loss: 0.29019, contrastive loss: 0.00211, Loss positive: 0.00000, Loss negative: 0.00211
2018-10-22 13:11:57.849430: Epoch [ 87/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:12:08.374661: Epoch [ 87/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:12:18.678901: Epoch [ 87/1000] [180/183], total loss: 0.00653, regularization loss: 0.29019, contrastive loss: 0.00653, Loss positive: 0.00000, Loss negative: 0.00653
2018-10-22 13:12:43.200175: Epoch [ 88/1000] [ 20/183], total loss: 0.01787, regularization loss: 0.29019, contrastive loss: 0.01787, Loss positive: 0.01787, Loss negative: 0.00000
2018-10-22 13:12:53.375362: Epoch [ 88/1000] [ 40/183], total loss: 0.00165, regularization loss: 0.29019, contrastive loss: 0.00165, Loss positive: 0.00000, Loss negative: 0.00165
2018-10-22 13:13:03.604803: Epoch [ 88/1000] [ 60/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 13:13:14.074369: Epoch [ 88/1000] [ 80/183], total loss: 0.00039, regularization loss: 0.29019, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 13:13:24.351169: Epoch [ 88/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:13:34.701226: Epoch [ 88/1000] [120/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 13:13:45.092851: Epoch [ 88/1000] [140/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 13:13:55.382498: Epoch [ 88/1000] [160/183], total loss: 0.00441, regularization loss: 0.29019, contrastive loss: 0.00441, Loss positive: 0.00000, Loss negative: 0.00441
2018-10-22 13:14:05.928929: Epoch [ 88/1000] [180/183], total loss: 0.00381, regularization loss: 0.29019, contrastive loss: 0.00381, Loss positive: 0.00000, Loss negative: 0.00381
2018-10-22 13:14:28.654582: Epoch [ 89/1000] [ 20/183], total loss: 0.00936, regularization loss: 0.29019, contrastive loss: 0.00936, Loss positive: 0.00936, Loss negative: 0.00000
2018-10-22 13:14:38.830853: Epoch [ 89/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:14:49.029286: Epoch [ 89/1000] [ 60/183], total loss: 0.01770, regularization loss: 0.29019, contrastive loss: 0.01770, Loss positive: 0.01747, Loss negative: 0.00023
2018-10-22 13:14:59.231843: Epoch [ 89/1000] [ 80/183], total loss: 0.00010, regularization loss: 0.29019, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 13:15:09.496129: Epoch [ 89/1000] [100/183], total loss: 0.00142, regularization loss: 0.29019, contrastive loss: 0.00142, Loss positive: 0.00000, Loss negative: 0.00142
2018-10-22 13:15:19.831122: Epoch [ 89/1000] [120/183], total loss: 0.00248, regularization loss: 0.29019, contrastive loss: 0.00248, Loss positive: 0.00000, Loss negative: 0.00248
2018-10-22 13:15:30.064208: Epoch [ 89/1000] [140/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 13:15:40.323007: Epoch [ 89/1000] [160/183], total loss: 0.01600, regularization loss: 0.29019, contrastive loss: 0.01600, Loss positive: 0.01333, Loss negative: 0.00267
2018-10-22 13:15:50.847871: Epoch [ 89/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:16:12.279885: Epoch [ 90/1000] [ 20/183], total loss: 0.00733, regularization loss: 0.29019, contrastive loss: 0.00733, Loss positive: 0.00000, Loss negative: 0.00733
2018-10-22 13:16:22.451587: Epoch [ 90/1000] [ 40/183], total loss: 0.00393, regularization loss: 0.29019, contrastive loss: 0.00393, Loss positive: 0.00000, Loss negative: 0.00393
2018-10-22 13:16:32.641088: Epoch [ 90/1000] [ 60/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 13:16:42.993874: Epoch [ 90/1000] [ 80/183], total loss: 0.00629, regularization loss: 0.29019, contrastive loss: 0.00629, Loss positive: 0.00000, Loss negative: 0.00629
2018-10-22 13:16:53.223666: Epoch [ 90/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:17:03.468142: Epoch [ 90/1000] [120/183], total loss: 0.00056, regularization loss: 0.29019, contrastive loss: 0.00056, Loss positive: 0.00000, Loss negative: 0.00056
2018-10-22 13:17:13.686270: Epoch [ 90/1000] [140/183], total loss: 0.02252, regularization loss: 0.29019, contrastive loss: 0.02252, Loss positive: 0.02252, Loss negative: 0.00000
2018-10-22 13:17:23.950720: Epoch [ 90/1000] [160/183], total loss: 0.00249, regularization loss: 0.29019, contrastive loss: 0.00249, Loss positive: 0.00000, Loss negative: 0.00249
2018-10-22 13:17:34.183901: Epoch [ 90/1000] [180/183], total loss: 0.09935, regularization loss: 0.29019, contrastive loss: 0.09935, Loss positive: 0.09650, Loss negative: 0.00285
Recall@1: 0.19902
Recall@2: 0.30149
Recall@4: 0.42252
Recall@8: 0.57259
Recall@16: 0.71725
Recall@32: 0.83575
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [42. 83. 56. 50. 65. 92. 54. 96. 82. 60. 54. 74. 31. 41. 55. 74. 46. 85.
 75. 59. 56. 57. 63. 36. 57. 73. 71. 45. 31. 44. 53. 62. 48. 79. 57. 44.
 45. 55. 72. 60. 88. 50. 47. 46. 64. 81. 70. 63. 62. 63. 60. 86. 62. 38.
 85. 73. 50. 55. 82. 70. 57. 46. 33. 47. 60. 54. 69. 55. 36. 58. 55. 67.
 39. 30. 41. 53. 47. 54. 71. 72. 67. 60. 38. 65. 70. 60. 65. 59. 92. 46.
 69. 48. 54. 77. 72. 66. 44. 69. 25. 57.]
Purity is 0.207
count_cross = [[ 0.  1.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0. 10.]
 [ 0.  1.  2. ...  2.  0.  0.]
 ...
 [ 0.  1.  0. ...  0.  2.  0.]
 [ 8.  0.  0. ...  0.  0.  0.]
 [ 1.  0.  0. ...  0.  0.  0.]]
Mutual information is 1.90108
5924.0
5924
Entropy cluster is 4.57223
Entropy class is 4.60444
normalized_mutual_information is 0.41433
tp_and_fp = 183880.0
tp = 16732.0
fp is 167148.0
fn is 156018.0
RI is 0.9815796076659238
Precision is 0.09099412660430715
Recall is 0.09685672937771346
F_1 is 0.09383394554580378

normalized_mutual_information = 0.4143292995695909
RI = 0.9815796076659238
F_1 = 0.09383394554580378

The NN is 0.19902
The FT is 0.11294
The ST is 0.18080
The DCG is 0.49447
The E is 0.09328
The MAP 0.08868

2018-10-22 13:19:04.225115: Epoch [ 91/1000] [ 20/183], total loss: 0.00242, regularization loss: 0.29019, contrastive loss: 0.00242, Loss positive: 0.00000, Loss negative: 0.00242
2018-10-22 13:19:14.313429: Epoch [ 91/1000] [ 40/183], total loss: 0.00142, regularization loss: 0.29019, contrastive loss: 0.00142, Loss positive: 0.00000, Loss negative: 0.00142
2018-10-22 13:19:24.392501: Epoch [ 91/1000] [ 60/183], total loss: 0.00078, regularization loss: 0.29018, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 13:19:34.535267: Epoch [ 91/1000] [ 80/183], total loss: 0.04229, regularization loss: 0.29019, contrastive loss: 0.04229, Loss positive: 0.04106, Loss negative: 0.00123
2018-10-22 13:19:44.700499: Epoch [ 91/1000] [100/183], total loss: 0.00110, regularization loss: 0.29019, contrastive loss: 0.00110, Loss positive: 0.00000, Loss negative: 0.00110
2018-10-22 13:19:54.901231: Epoch [ 91/1000] [120/183], total loss: 0.00027, regularization loss: 0.29019, contrastive loss: 0.00027, Loss positive: 0.00000, Loss negative: 0.00027
2018-10-22 13:20:05.132338: Epoch [ 91/1000] [140/183], total loss: 0.00215, regularization loss: 0.29019, contrastive loss: 0.00215, Loss positive: 0.00000, Loss negative: 0.00215
2018-10-22 13:20:15.570460: Epoch [ 91/1000] [160/183], total loss: 0.00039, regularization loss: 0.29019, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 13:20:25.833457: Epoch [ 91/1000] [180/183], total loss: 0.00226, regularization loss: 0.29019, contrastive loss: 0.00226, Loss positive: 0.00000, Loss negative: 0.00226
2018-10-22 13:20:48.386333: Epoch [ 92/1000] [ 20/183], total loss: 0.02808, regularization loss: 0.29019, contrastive loss: 0.02808, Loss positive: 0.02450, Loss negative: 0.00358
2018-10-22 13:20:58.557034: Epoch [ 92/1000] [ 40/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 13:21:08.930186: Epoch [ 92/1000] [ 60/183], total loss: 0.03237, regularization loss: 0.29019, contrastive loss: 0.03237, Loss positive: 0.02923, Loss negative: 0.00314
2018-10-22 13:21:19.156823: Epoch [ 92/1000] [ 80/183], total loss: 0.01652, regularization loss: 0.29019, contrastive loss: 0.01652, Loss positive: 0.01391, Loss negative: 0.00261
2018-10-22 13:21:29.421205: Epoch [ 92/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:21:39.870710: Epoch [ 92/1000] [120/183], total loss: 0.06365, regularization loss: 0.29019, contrastive loss: 0.06365, Loss positive: 0.06040, Loss negative: 0.00325
2018-10-22 13:21:50.322238: Epoch [ 92/1000] [140/183], total loss: 0.00017, regularization loss: 0.29019, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 13:22:00.753186: Epoch [ 92/1000] [160/183], total loss: 0.03021, regularization loss: 0.29019, contrastive loss: 0.03021, Loss positive: 0.03012, Loss negative: 0.00009
2018-10-22 13:22:11.180279: Epoch [ 92/1000] [180/183], total loss: 0.00137, regularization loss: 0.29019, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 13:22:36.273331: Epoch [ 93/1000] [ 20/183], total loss: 0.00026, regularization loss: 0.29019, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 13:22:46.531951: Epoch [ 93/1000] [ 40/183], total loss: 0.00326, regularization loss: 0.29019, contrastive loss: 0.00326, Loss positive: 0.00000, Loss negative: 0.00326
2018-10-22 13:22:56.769347: Epoch [ 93/1000] [ 60/183], total loss: 0.01822, regularization loss: 0.29019, contrastive loss: 0.01822, Loss positive: 0.01641, Loss negative: 0.00181
2018-10-22 13:23:07.082776: Epoch [ 93/1000] [ 80/183], total loss: 0.00509, regularization loss: 0.29019, contrastive loss: 0.00509, Loss positive: 0.00000, Loss negative: 0.00509
2018-10-22 13:23:17.358523: Epoch [ 93/1000] [100/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 13:23:27.653247: Epoch [ 93/1000] [120/183], total loss: 0.00203, regularization loss: 0.29018, contrastive loss: 0.00203, Loss positive: 0.00000, Loss negative: 0.00203
2018-10-22 13:23:37.969981: Epoch [ 93/1000] [140/183], total loss: 0.00184, regularization loss: 0.29019, contrastive loss: 0.00184, Loss positive: 0.00000, Loss negative: 0.00184
2018-10-22 13:23:48.482894: Epoch [ 93/1000] [160/183], total loss: 0.06925, regularization loss: 0.29019, contrastive loss: 0.06925, Loss positive: 0.06850, Loss negative: 0.00075
2018-10-22 13:23:58.750151: Epoch [ 93/1000] [180/183], total loss: 0.00097, regularization loss: 0.29019, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 13:24:21.395292: Epoch [ 94/1000] [ 20/183], total loss: 0.01754, regularization loss: 0.29019, contrastive loss: 0.01754, Loss positive: 0.01754, Loss negative: 0.00000
2018-10-22 13:24:31.566640: Epoch [ 94/1000] [ 40/183], total loss: 0.00308, regularization loss: 0.29019, contrastive loss: 0.00308, Loss positive: 0.00000, Loss negative: 0.00308
2018-10-22 13:24:41.754514: Epoch [ 94/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:24:51.949454: Epoch [ 94/1000] [ 80/183], total loss: 0.04370, regularization loss: 0.29019, contrastive loss: 0.04370, Loss positive: 0.03977, Loss negative: 0.00393
2018-10-22 13:25:02.235561: Epoch [ 94/1000] [100/183], total loss: 0.00513, regularization loss: 0.29018, contrastive loss: 0.00513, Loss positive: 0.00000, Loss negative: 0.00513
2018-10-22 13:25:12.941754: Epoch [ 94/1000] [120/183], total loss: 0.00089, regularization loss: 0.29018, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 13:25:23.194262: Epoch [ 94/1000] [140/183], total loss: 0.00898, regularization loss: 0.29019, contrastive loss: 0.00898, Loss positive: 0.00000, Loss negative: 0.00898
2018-10-22 13:25:33.413764: Epoch [ 94/1000] [160/183], total loss: 0.00506, regularization loss: 0.29019, contrastive loss: 0.00506, Loss positive: 0.00000, Loss negative: 0.00506
2018-10-22 13:25:43.649112: Epoch [ 94/1000] [180/183], total loss: 0.00288, regularization loss: 0.29019, contrastive loss: 0.00288, Loss positive: 0.00000, Loss negative: 0.00288
2018-10-22 13:26:05.107858: Epoch [ 95/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:26:15.265686: Epoch [ 95/1000] [ 40/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 13:26:25.455694: Epoch [ 95/1000] [ 60/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 13:26:35.701923: Epoch [ 95/1000] [ 80/183], total loss: 0.00149, regularization loss: 0.29018, contrastive loss: 0.00149, Loss positive: 0.00000, Loss negative: 0.00149
2018-10-22 13:26:45.935461: Epoch [ 95/1000] [100/183], total loss: 0.00332, regularization loss: 0.29018, contrastive loss: 0.00332, Loss positive: 0.00000, Loss negative: 0.00332
2018-10-22 13:26:56.305663: Epoch [ 95/1000] [120/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 13:27:06.508078: Epoch [ 95/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:27:16.742453: Epoch [ 95/1000] [160/183], total loss: 0.04100, regularization loss: 0.29019, contrastive loss: 0.04100, Loss positive: 0.03544, Loss negative: 0.00556
2018-10-22 13:27:26.993453: Epoch [ 95/1000] [180/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 13:27:48.463187: Epoch [ 96/1000] [ 20/183], total loss: 0.00168, regularization loss: 0.29019, contrastive loss: 0.00168, Loss positive: 0.00000, Loss negative: 0.00168
2018-10-22 13:27:58.599741: Epoch [ 96/1000] [ 40/183], total loss: 0.01438, regularization loss: 0.29019, contrastive loss: 0.01438, Loss positive: 0.01438, Loss negative: 0.00000
2018-10-22 13:28:08.784705: Epoch [ 96/1000] [ 60/183], total loss: 0.01933, regularization loss: 0.29019, contrastive loss: 0.01933, Loss positive: 0.01563, Loss negative: 0.00371
2018-10-22 13:28:18.989947: Epoch [ 96/1000] [ 80/183], total loss: 0.04817, regularization loss: 0.29018, contrastive loss: 0.04817, Loss positive: 0.04600, Loss negative: 0.00217
2018-10-22 13:28:29.236063: Epoch [ 96/1000] [100/183], total loss: 0.02200, regularization loss: 0.29019, contrastive loss: 0.02200, Loss positive: 0.01690, Loss negative: 0.00510
2018-10-22 13:28:39.500213: Epoch [ 96/1000] [120/183], total loss: 0.01936, regularization loss: 0.29019, contrastive loss: 0.01936, Loss positive: 0.01702, Loss negative: 0.00234
2018-10-22 13:28:49.720326: Epoch [ 96/1000] [140/183], total loss: 0.00353, regularization loss: 0.29019, contrastive loss: 0.00353, Loss positive: 0.00000, Loss negative: 0.00353
2018-10-22 13:28:59.976013: Epoch [ 96/1000] [160/183], total loss: 0.00083, regularization loss: 0.29019, contrastive loss: 0.00083, Loss positive: 0.00000, Loss negative: 0.00083
2018-10-22 13:29:10.201046: Epoch [ 96/1000] [180/183], total loss: 0.00625, regularization loss: 0.29019, contrastive loss: 0.00625, Loss positive: 0.00000, Loss negative: 0.00625
2018-10-22 13:29:31.818277: Epoch [ 97/1000] [ 20/183], total loss: 0.00250, regularization loss: 0.29018, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 13:29:41.991575: Epoch [ 97/1000] [ 40/183], total loss: 0.00736, regularization loss: 0.29018, contrastive loss: 0.00736, Loss positive: 0.00000, Loss negative: 0.00736
2018-10-22 13:29:52.185149: Epoch [ 97/1000] [ 60/183], total loss: 0.00949, regularization loss: 0.29018, contrastive loss: 0.00949, Loss positive: 0.00893, Loss negative: 0.00056
2018-10-22 13:30:02.445244: Epoch [ 97/1000] [ 80/183], total loss: 0.00308, regularization loss: 0.29018, contrastive loss: 0.00308, Loss positive: 0.00000, Loss negative: 0.00308
2018-10-22 13:30:12.682303: Epoch [ 97/1000] [100/183], total loss: 0.02703, regularization loss: 0.29019, contrastive loss: 0.02703, Loss positive: 0.02677, Loss negative: 0.00026
2018-10-22 13:30:22.913224: Epoch [ 97/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:30:33.156562: Epoch [ 97/1000] [140/183], total loss: 0.00164, regularization loss: 0.29019, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 13:30:43.517866: Epoch [ 97/1000] [160/183], total loss: 0.02600, regularization loss: 0.29019, contrastive loss: 0.02600, Loss positive: 0.02600, Loss negative: 0.00000
2018-10-22 13:30:53.764365: Epoch [ 97/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:31:16.443840: Epoch [ 98/1000] [ 20/183], total loss: 0.01026, regularization loss: 0.29019, contrastive loss: 0.01026, Loss positive: 0.00000, Loss negative: 0.01026
2018-10-22 13:31:26.660026: Epoch [ 98/1000] [ 40/183], total loss: 0.00231, regularization loss: 0.29019, contrastive loss: 0.00231, Loss positive: 0.00000, Loss negative: 0.00231
2018-10-22 13:31:36.863727: Epoch [ 98/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:31:47.115566: Epoch [ 98/1000] [ 80/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 13:31:57.607250: Epoch [ 98/1000] [100/183], total loss: 0.01327, regularization loss: 0.29019, contrastive loss: 0.01327, Loss positive: 0.00000, Loss negative: 0.01327
2018-10-22 13:32:08.116020: Epoch [ 98/1000] [120/183], total loss: 0.00146, regularization loss: 0.29019, contrastive loss: 0.00146, Loss positive: 0.00000, Loss negative: 0.00146
2018-10-22 13:32:18.457402: Epoch [ 98/1000] [140/183], total loss: 0.00217, regularization loss: 0.29018, contrastive loss: 0.00217, Loss positive: 0.00000, Loss negative: 0.00217
2018-10-22 13:32:28.888193: Epoch [ 98/1000] [160/183], total loss: 0.08489, regularization loss: 0.29019, contrastive loss: 0.08489, Loss positive: 0.07901, Loss negative: 0.00588
2018-10-22 13:32:39.258144: Epoch [ 98/1000] [180/183], total loss: 0.08318, regularization loss: 0.29019, contrastive loss: 0.08318, Loss positive: 0.08162, Loss negative: 0.00155
2018-10-22 13:33:03.998201: Epoch [ 99/1000] [ 20/183], total loss: 0.00456, regularization loss: 0.29019, contrastive loss: 0.00456, Loss positive: 0.00000, Loss negative: 0.00456
2018-10-22 13:33:14.219607: Epoch [ 99/1000] [ 40/183], total loss: 0.01818, regularization loss: 0.29019, contrastive loss: 0.01818, Loss positive: 0.01811, Loss negative: 0.00007
2018-10-22 13:33:24.431319: Epoch [ 99/1000] [ 60/183], total loss: 0.00369, regularization loss: 0.29019, contrastive loss: 0.00369, Loss positive: 0.00000, Loss negative: 0.00369
2018-10-22 13:33:34.911665: Epoch [ 99/1000] [ 80/183], total loss: 0.00511, regularization loss: 0.29019, contrastive loss: 0.00511, Loss positive: 0.00000, Loss negative: 0.00511
2018-10-22 13:33:45.166888: Epoch [ 99/1000] [100/183], total loss: 0.00138, regularization loss: 0.29019, contrastive loss: 0.00138, Loss positive: 0.00000, Loss negative: 0.00138
2018-10-22 13:33:55.409323: Epoch [ 99/1000] [120/183], total loss: 0.00253, regularization loss: 0.29019, contrastive loss: 0.00253, Loss positive: 0.00000, Loss negative: 0.00253
2018-10-22 13:34:05.641443: Epoch [ 99/1000] [140/183], total loss: 0.00039, regularization loss: 0.29019, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 13:34:15.888498: Epoch [ 99/1000] [160/183], total loss: 0.03496, regularization loss: 0.29019, contrastive loss: 0.03496, Loss positive: 0.03183, Loss negative: 0.00313
2018-10-22 13:34:26.182637: Epoch [ 99/1000] [180/183], total loss: 0.00472, regularization loss: 0.29019, contrastive loss: 0.00472, Loss positive: 0.00000, Loss negative: 0.00472
2018-10-22 13:34:49.039759: Epoch [100/1000] [ 20/183], total loss: 0.00151, regularization loss: 0.29019, contrastive loss: 0.00151, Loss positive: 0.00000, Loss negative: 0.00151
2018-10-22 13:34:59.233247: Epoch [100/1000] [ 40/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 13:35:09.448357: Epoch [100/1000] [ 60/183], total loss: 0.00825, regularization loss: 0.29019, contrastive loss: 0.00825, Loss positive: 0.00000, Loss negative: 0.00825
2018-10-22 13:35:19.686162: Epoch [100/1000] [ 80/183], total loss: 0.00036, regularization loss: 0.29019, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 13:35:29.902400: Epoch [100/1000] [100/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 13:35:40.149685: Epoch [100/1000] [120/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 13:35:50.413329: Epoch [100/1000] [140/183], total loss: 0.02049, regularization loss: 0.29019, contrastive loss: 0.02049, Loss positive: 0.01792, Loss negative: 0.00257
2018-10-22 13:36:00.644674: Epoch [100/1000] [160/183], total loss: 0.00169, regularization loss: 0.29019, contrastive loss: 0.00169, Loss positive: 0.00000, Loss negative: 0.00169
2018-10-22 13:36:10.902853: Epoch [100/1000] [180/183], total loss: 0.00319, regularization loss: 0.29019, contrastive loss: 0.00319, Loss positive: 0.00000, Loss negative: 0.00319
Recall@1: 0.21388
Recall@2: 0.32056
Recall@4: 0.45155
Recall@8: 0.58913
Recall@16: 0.72974
Recall@32: 0.83356
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 54.  51.  51.  81.  68.  56.  61.  76.  40.  81.  52.  47.  62.  49.
  43.  71.  49.  42.  88.  65.  57.  95.  50.  27.  59.  45.  52.  50.
  40.  58.  46.  34.  88. 100.  67.  42.  98.  78.  88.  64.  42.  61.
  54.  65.  78.  78.  85.  80.  38.  68.  31.  40.  64.  69.  76.  67.
  75.  72.  56.  49.  95.  77.  84.  54.  36.  98.  37.  58.  64.  42.
  50. 104.  38.  20.  62.  63.  46.  89.  52.  47.  31.  82.  54.  43.
  40.  39.  38.  48.  41.  38.  51.  48.  62.  60.  57.  49.  48.  81.
  47.  78.]
Purity is 0.211
count_cross = [[0. 0. 0. ... 1. 3. 0.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 4. 4. ... 4. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 1.92299
5924.0
5924
Entropy cluster is 4.55751
Entropy class is 4.60444
normalized_mutual_information is 0.41978
tp_and_fp = 189394.0
tp = 16972.0
fp is 172422.0
fn is 155778.0
RI is 0.9812926707511193
Precision is 0.08961213132411798
Recall is 0.09824602026049203
F_1 is 0.0937306706724397

normalized_mutual_information = 0.41977677761227344
RI = 0.9812926707511193
F_1 = 0.0937306706724397

The NN is 0.21388
The FT is 0.11801
The ST is 0.18743
The DCG is 0.50016
The E is 0.09811
The MAP 0.09219

2018-10-22 13:37:41.086302: Epoch [101/1000] [ 20/183], total loss: 0.00131, regularization loss: 0.29019, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 13:37:51.172560: Epoch [101/1000] [ 40/183], total loss: 0.00010, regularization loss: 0.29019, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 13:38:01.265020: Epoch [101/1000] [ 60/183], total loss: 0.00113, regularization loss: 0.29019, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 13:38:11.394150: Epoch [101/1000] [ 80/183], total loss: 0.01363, regularization loss: 0.29019, contrastive loss: 0.01363, Loss positive: 0.01325, Loss negative: 0.00038
2018-10-22 13:38:21.549526: Epoch [101/1000] [100/183], total loss: 0.00216, regularization loss: 0.29019, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 13:38:31.790861: Epoch [101/1000] [120/183], total loss: 0.07877, regularization loss: 0.29019, contrastive loss: 0.07877, Loss positive: 0.07877, Loss negative: 0.00000
2018-10-22 13:38:42.067351: Epoch [101/1000] [140/183], total loss: 0.00248, regularization loss: 0.29019, contrastive loss: 0.00248, Loss positive: 0.00000, Loss negative: 0.00248
2018-10-22 13:38:52.350531: Epoch [101/1000] [160/183], total loss: 0.00139, regularization loss: 0.29019, contrastive loss: 0.00139, Loss positive: 0.00000, Loss negative: 0.00139
2018-10-22 13:39:02.809183: Epoch [101/1000] [180/183], total loss: 0.00057, regularization loss: 0.29019, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 13:39:24.174834: Epoch [102/1000] [ 20/183], total loss: 0.01134, regularization loss: 0.29019, contrastive loss: 0.01134, Loss positive: 0.00000, Loss negative: 0.01134
2018-10-22 13:39:34.343345: Epoch [102/1000] [ 40/183], total loss: 0.02297, regularization loss: 0.29019, contrastive loss: 0.02297, Loss positive: 0.02266, Loss negative: 0.00032
2018-10-22 13:39:44.552704: Epoch [102/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:39:55.166881: Epoch [102/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:40:05.382537: Epoch [102/1000] [100/183], total loss: 0.00692, regularization loss: 0.29019, contrastive loss: 0.00692, Loss positive: 0.00670, Loss negative: 0.00022
2018-10-22 13:40:15.659929: Epoch [102/1000] [120/183], total loss: 0.00176, regularization loss: 0.29019, contrastive loss: 0.00176, Loss positive: 0.00000, Loss negative: 0.00176
2018-10-22 13:40:26.193297: Epoch [102/1000] [140/183], total loss: 0.01065, regularization loss: 0.29019, contrastive loss: 0.01065, Loss positive: 0.00000, Loss negative: 0.01065
2018-10-22 13:40:36.428135: Epoch [102/1000] [160/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 13:40:46.764400: Epoch [102/1000] [180/183], total loss: 0.04069, regularization loss: 0.29019, contrastive loss: 0.04069, Loss positive: 0.04069, Loss negative: 0.00000
2018-10-22 13:41:09.374710: Epoch [103/1000] [ 20/183], total loss: 0.03267, regularization loss: 0.29019, contrastive loss: 0.03267, Loss positive: 0.03199, Loss negative: 0.00069
2018-10-22 13:41:19.583398: Epoch [103/1000] [ 40/183], total loss: 0.00101, regularization loss: 0.29019, contrastive loss: 0.00101, Loss positive: 0.00000, Loss negative: 0.00101
2018-10-22 13:41:29.951185: Epoch [103/1000] [ 60/183], total loss: 0.00440, regularization loss: 0.29019, contrastive loss: 0.00440, Loss positive: 0.00000, Loss negative: 0.00440
2018-10-22 13:41:40.316843: Epoch [103/1000] [ 80/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 13:41:50.847692: Epoch [103/1000] [100/183], total loss: 0.00339, regularization loss: 0.29019, contrastive loss: 0.00339, Loss positive: 0.00000, Loss negative: 0.00339
2018-10-22 13:42:01.127432: Epoch [103/1000] [120/183], total loss: 0.00043, regularization loss: 0.29019, contrastive loss: 0.00043, Loss positive: 0.00000, Loss negative: 0.00043
2018-10-22 13:42:11.580602: Epoch [103/1000] [140/183], total loss: 0.00237, regularization loss: 0.29019, contrastive loss: 0.00237, Loss positive: 0.00000, Loss negative: 0.00237
2018-10-22 13:42:22.084808: Epoch [103/1000] [160/183], total loss: 0.03229, regularization loss: 0.29019, contrastive loss: 0.03229, Loss positive: 0.03027, Loss negative: 0.00202
2018-10-22 13:42:32.467948: Epoch [103/1000] [180/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 13:42:56.937411: Epoch [104/1000] [ 20/183], total loss: 0.03502, regularization loss: 0.29019, contrastive loss: 0.03502, Loss positive: 0.03502, Loss negative: 0.00000
2018-10-22 13:43:07.180500: Epoch [104/1000] [ 40/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 13:43:17.373765: Epoch [104/1000] [ 60/183], total loss: 0.00083, regularization loss: 0.29019, contrastive loss: 0.00083, Loss positive: 0.00000, Loss negative: 0.00083
2018-10-22 13:43:27.740654: Epoch [104/1000] [ 80/183], total loss: 0.00095, regularization loss: 0.29019, contrastive loss: 0.00095, Loss positive: 0.00000, Loss negative: 0.00095
2018-10-22 13:43:38.007210: Epoch [104/1000] [100/183], total loss: 0.00415, regularization loss: 0.29019, contrastive loss: 0.00415, Loss positive: 0.00000, Loss negative: 0.00415
2018-10-22 13:43:48.591674: Epoch [104/1000] [120/183], total loss: 0.00226, regularization loss: 0.29019, contrastive loss: 0.00226, Loss positive: 0.00000, Loss negative: 0.00226
2018-10-22 13:43:58.854616: Epoch [104/1000] [140/183], total loss: 0.00113, regularization loss: 0.29019, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 13:44:09.100245: Epoch [104/1000] [160/183], total loss: 0.00544, regularization loss: 0.29019, contrastive loss: 0.00544, Loss positive: 0.00000, Loss negative: 0.00544
2018-10-22 13:44:19.331417: Epoch [104/1000] [180/183], total loss: 0.00352, regularization loss: 0.29019, contrastive loss: 0.00352, Loss positive: 0.00000, Loss negative: 0.00352
2018-10-22 13:44:41.180066: Epoch [105/1000] [ 20/183], total loss: 0.00356, regularization loss: 0.29019, contrastive loss: 0.00356, Loss positive: 0.00000, Loss negative: 0.00356
2018-10-22 13:44:51.352936: Epoch [105/1000] [ 40/183], total loss: 0.03740, regularization loss: 0.29019, contrastive loss: 0.03740, Loss positive: 0.03624, Loss negative: 0.00116
2018-10-22 13:45:01.520977: Epoch [105/1000] [ 60/183], total loss: 0.00214, regularization loss: 0.29019, contrastive loss: 0.00214, Loss positive: 0.00000, Loss negative: 0.00214
2018-10-22 13:45:11.959303: Epoch [105/1000] [ 80/183], total loss: 0.00152, regularization loss: 0.29019, contrastive loss: 0.00152, Loss positive: 0.00000, Loss negative: 0.00152
2018-10-22 13:45:22.231156: Epoch [105/1000] [100/183], total loss: 0.01004, regularization loss: 0.29019, contrastive loss: 0.01004, Loss positive: 0.00950, Loss negative: 0.00055
2018-10-22 13:45:32.443291: Epoch [105/1000] [120/183], total loss: 0.00084, regularization loss: 0.29019, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 13:45:42.809582: Epoch [105/1000] [140/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 13:45:53.042245: Epoch [105/1000] [160/183], total loss: 0.00183, regularization loss: 0.29019, contrastive loss: 0.00183, Loss positive: 0.00000, Loss negative: 0.00183
2018-10-22 13:46:03.279562: Epoch [105/1000] [180/183], total loss: 0.00189, regularization loss: 0.29019, contrastive loss: 0.00189, Loss positive: 0.00000, Loss negative: 0.00189
2018-10-22 13:46:24.449381: Epoch [106/1000] [ 20/183], total loss: 0.06221, regularization loss: 0.29019, contrastive loss: 0.06221, Loss positive: 0.05786, Loss negative: 0.00435
2018-10-22 13:46:34.623985: Epoch [106/1000] [ 40/183], total loss: 0.00052, regularization loss: 0.29019, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 13:46:44.858038: Epoch [106/1000] [ 60/183], total loss: 0.00058, regularization loss: 0.29019, contrastive loss: 0.00058, Loss positive: 0.00000, Loss negative: 0.00058
2018-10-22 13:46:55.096706: Epoch [106/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:47:05.329731: Epoch [106/1000] [100/183], total loss: 0.00099, regularization loss: 0.29019, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 13:47:15.572455: Epoch [106/1000] [120/183], total loss: 0.00887, regularization loss: 0.29019, contrastive loss: 0.00887, Loss positive: 0.00827, Loss negative: 0.00060
2018-10-22 13:47:25.792023: Epoch [106/1000] [140/183], total loss: 0.00081, regularization loss: 0.29019, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 13:47:36.273189: Epoch [106/1000] [160/183], total loss: 0.02416, regularization loss: 0.29019, contrastive loss: 0.02416, Loss positive: 0.02416, Loss negative: 0.00000
2018-10-22 13:47:46.492904: Epoch [106/1000] [180/183], total loss: 0.00333, regularization loss: 0.29019, contrastive loss: 0.00333, Loss positive: 0.00000, Loss negative: 0.00333
2018-10-22 13:48:07.626708: Epoch [107/1000] [ 20/183], total loss: 0.00048, regularization loss: 0.29019, contrastive loss: 0.00048, Loss positive: 0.00000, Loss negative: 0.00048
2018-10-22 13:48:17.780838: Epoch [107/1000] [ 40/183], total loss: 0.05330, regularization loss: 0.29019, contrastive loss: 0.05330, Loss positive: 0.05279, Loss negative: 0.00051
2018-10-22 13:48:27.952156: Epoch [107/1000] [ 60/183], total loss: 0.01565, regularization loss: 0.29019, contrastive loss: 0.01565, Loss positive: 0.01495, Loss negative: 0.00071
2018-10-22 13:48:38.197234: Epoch [107/1000] [ 80/183], total loss: 0.00063, regularization loss: 0.29019, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 13:48:48.436179: Epoch [107/1000] [100/183], total loss: 0.01364, regularization loss: 0.29019, contrastive loss: 0.01364, Loss positive: 0.01345, Loss negative: 0.00019
2018-10-22 13:48:58.698471: Epoch [107/1000] [120/183], total loss: 0.01802, regularization loss: 0.29019, contrastive loss: 0.01802, Loss positive: 0.01742, Loss negative: 0.00060
2018-10-22 13:49:08.933707: Epoch [107/1000] [140/183], total loss: 0.02052, regularization loss: 0.29019, contrastive loss: 0.02052, Loss positive: 0.01057, Loss negative: 0.00995
2018-10-22 13:49:19.169751: Epoch [107/1000] [160/183], total loss: 0.00154, regularization loss: 0.29019, contrastive loss: 0.00154, Loss positive: 0.00000, Loss negative: 0.00154
2018-10-22 13:49:29.407284: Epoch [107/1000] [180/183], total loss: 0.00398, regularization loss: 0.29019, contrastive loss: 0.00398, Loss positive: 0.00000, Loss negative: 0.00398
2018-10-22 13:49:50.460958: Epoch [108/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:50:00.611034: Epoch [108/1000] [ 40/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 13:50:10.804423: Epoch [108/1000] [ 60/183], total loss: 0.00032, regularization loss: 0.29019, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 13:50:21.004456: Epoch [108/1000] [ 80/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 13:50:31.449087: Epoch [108/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:50:41.696253: Epoch [108/1000] [120/183], total loss: 0.01641, regularization loss: 0.29019, contrastive loss: 0.01641, Loss positive: 0.01641, Loss negative: 0.00000
2018-10-22 13:50:51.946320: Epoch [108/1000] [140/183], total loss: 0.00208, regularization loss: 0.29019, contrastive loss: 0.00208, Loss positive: 0.00000, Loss negative: 0.00208
2018-10-22 13:51:02.193457: Epoch [108/1000] [160/183], total loss: 0.01347, regularization loss: 0.29019, contrastive loss: 0.01347, Loss positive: 0.01336, Loss negative: 0.00011
2018-10-22 13:51:12.434250: Epoch [108/1000] [180/183], total loss: 0.07977, regularization loss: 0.29019, contrastive loss: 0.07977, Loss positive: 0.07868, Loss negative: 0.00109
2018-10-22 13:51:34.444723: Epoch [109/1000] [ 20/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 13:51:44.627571: Epoch [109/1000] [ 40/183], total loss: 0.00162, regularization loss: 0.29019, contrastive loss: 0.00162, Loss positive: 0.00000, Loss negative: 0.00162
2018-10-22 13:51:54.771267: Epoch [109/1000] [ 60/183], total loss: 0.00064, regularization loss: 0.29019, contrastive loss: 0.00064, Loss positive: 0.00000, Loss negative: 0.00064
2018-10-22 13:52:04.961157: Epoch [109/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:52:15.210192: Epoch [109/1000] [100/183], total loss: 0.02751, regularization loss: 0.29019, contrastive loss: 0.02751, Loss positive: 0.02464, Loss negative: 0.00287
2018-10-22 13:52:25.443681: Epoch [109/1000] [120/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 13:52:35.679016: Epoch [109/1000] [140/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 13:52:45.944596: Epoch [109/1000] [160/183], total loss: 0.03816, regularization loss: 0.29019, contrastive loss: 0.03816, Loss positive: 0.03497, Loss negative: 0.00319
2018-10-22 13:52:56.257916: Epoch [109/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:53:19.021187: Epoch [110/1000] [ 20/183], total loss: 0.00380, regularization loss: 0.29019, contrastive loss: 0.00380, Loss positive: 0.00000, Loss negative: 0.00380
2018-10-22 13:53:29.233297: Epoch [110/1000] [ 40/183], total loss: 0.00078, regularization loss: 0.29019, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 13:53:39.421396: Epoch [110/1000] [ 60/183], total loss: 0.04457, regularization loss: 0.29019, contrastive loss: 0.04457, Loss positive: 0.04449, Loss negative: 0.00008
2018-10-22 13:53:49.659907: Epoch [110/1000] [ 80/183], total loss: 0.00176, regularization loss: 0.29019, contrastive loss: 0.00176, Loss positive: 0.00000, Loss negative: 0.00176
2018-10-22 13:53:59.895094: Epoch [110/1000] [100/183], total loss: 0.01235, regularization loss: 0.29019, contrastive loss: 0.01235, Loss positive: 0.01062, Loss negative: 0.00173
2018-10-22 13:54:10.312464: Epoch [110/1000] [120/183], total loss: 0.02249, regularization loss: 0.29019, contrastive loss: 0.02249, Loss positive: 0.02172, Loss negative: 0.00078
2018-10-22 13:54:20.639806: Epoch [110/1000] [140/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 13:54:30.900477: Epoch [110/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:54:41.163400: Epoch [110/1000] [180/183], total loss: 0.07562, regularization loss: 0.29019, contrastive loss: 0.07562, Loss positive: 0.07562, Loss negative: 0.00000
Recall@1: 0.22130
Recall@2: 0.32951
Recall@4: 0.46269
Recall@8: 0.60331
Recall@16: 0.73582
Recall@32: 0.83609
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 32.  64.  75.  78.  74.  62.  41.  64.  99.  70.  62.  37.  47.  50.
  78.  59.  46.  83.  59.  75.  50.  64.  60.  90.  82.  46.  61.  84.
  67.  51.  58.  78.  20.  21.  71.  38.  44.  59.  59.  95. 103.  50.
  46.  61.  73.  51.  87.  94.  43.  50.  37.  50.  63.  56.  55.  76.
  42.  53.  39.  27.  62.  80.  69.  56.  45.  44.  26.  53.  76.  71.
  39.  57.  57.  57.  72.  58.  62.  33.  44.  75.  36.  83.  81.  40.
  74.  54.  59.  75.  67.  57.  62.  59.  76.  50.  43.  53.  38.  53.
  56.  63.]
Purity is 0.215
count_cross = [[1. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 2. 0.]
 [0. 2. 5. ... 3. 1. 0.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [1. 0. 0. ... 0. 0. 0.]
 [0. 2. 2. ... 2. 0. 0.]]
Mutual information is 1.92826
5924.0
5924
Entropy cluster is 4.56251
Entropy class is 4.60444
normalized_mutual_information is 0.42070
tp_and_fp = 187071.0
tp = 17205.0
fp is 169866.0
fn is 155545.0
RI is 0.9814516431498856
Precision is 0.09197042834004202
Recall is 0.09959479015918958
F_1 is 0.09563088313355808

normalized_mutual_information = 0.42069798263024094
RI = 0.9814516431498856
F_1 = 0.09563088313355808

The NN is 0.22130
The FT is 0.11821
The ST is 0.18711
The DCG is 0.50111
The E is 0.09885
The MAP 0.09246

2018-10-22 13:56:11.042331: Epoch [111/1000] [ 20/183], total loss: 0.01381, regularization loss: 0.29019, contrastive loss: 0.01381, Loss positive: 0.01312, Loss negative: 0.00069
2018-10-22 13:56:21.128574: Epoch [111/1000] [ 40/183], total loss: 0.00372, regularization loss: 0.29019, contrastive loss: 0.00372, Loss positive: 0.00000, Loss negative: 0.00372
2018-10-22 13:56:31.234181: Epoch [111/1000] [ 60/183], total loss: 0.03416, regularization loss: 0.29019, contrastive loss: 0.03416, Loss positive: 0.03173, Loss negative: 0.00243
2018-10-22 13:56:41.379626: Epoch [111/1000] [ 80/183], total loss: 0.00435, regularization loss: 0.29019, contrastive loss: 0.00435, Loss positive: 0.00000, Loss negative: 0.00435
2018-10-22 13:56:51.535300: Epoch [111/1000] [100/183], total loss: 0.00230, regularization loss: 0.29019, contrastive loss: 0.00230, Loss positive: 0.00000, Loss negative: 0.00230
2018-10-22 13:57:01.745273: Epoch [111/1000] [120/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 13:57:12.102866: Epoch [111/1000] [140/183], total loss: 0.02830, regularization loss: 0.29019, contrastive loss: 0.02830, Loss positive: 0.02830, Loss negative: 0.00000
2018-10-22 13:57:22.325129: Epoch [111/1000] [160/183], total loss: 0.00628, regularization loss: 0.29019, contrastive loss: 0.00628, Loss positive: 0.00000, Loss negative: 0.00628
2018-10-22 13:57:32.555654: Epoch [111/1000] [180/183], total loss: 0.01581, regularization loss: 0.29019, contrastive loss: 0.01581, Loss positive: 0.01299, Loss negative: 0.00282
2018-10-22 13:57:54.004437: Epoch [112/1000] [ 20/183], total loss: 0.04382, regularization loss: 0.29019, contrastive loss: 0.04382, Loss positive: 0.04102, Loss negative: 0.00280
2018-10-22 13:58:04.166959: Epoch [112/1000] [ 40/183], total loss: 0.03116, regularization loss: 0.29019, contrastive loss: 0.03116, Loss positive: 0.02992, Loss negative: 0.00123
2018-10-22 13:58:14.355976: Epoch [112/1000] [ 60/183], total loss: 0.04276, regularization loss: 0.29019, contrastive loss: 0.04276, Loss positive: 0.03899, Loss negative: 0.00377
2018-10-22 13:58:24.550699: Epoch [112/1000] [ 80/183], total loss: 0.00076, regularization loss: 0.29019, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 13:58:34.799903: Epoch [112/1000] [100/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 13:58:45.058100: Epoch [112/1000] [120/183], total loss: 0.00111, regularization loss: 0.29019, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 13:58:55.318768: Epoch [112/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 13:59:05.540462: Epoch [112/1000] [160/183], total loss: 0.00125, regularization loss: 0.29019, contrastive loss: 0.00125, Loss positive: 0.00000, Loss negative: 0.00125
2018-10-22 13:59:15.824740: Epoch [112/1000] [180/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 13:59:37.366281: Epoch [113/1000] [ 20/183], total loss: 0.03750, regularization loss: 0.29019, contrastive loss: 0.03750, Loss positive: 0.03596, Loss negative: 0.00154
2018-10-22 13:59:47.523748: Epoch [113/1000] [ 40/183], total loss: 0.00720, regularization loss: 0.29019, contrastive loss: 0.00720, Loss positive: 0.00000, Loss negative: 0.00720
2018-10-22 13:59:57.840594: Epoch [113/1000] [ 60/183], total loss: 0.00036, regularization loss: 0.29019, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 14:00:08.226665: Epoch [113/1000] [ 80/183], total loss: 0.00047, regularization loss: 0.29019, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 14:00:18.537503: Epoch [113/1000] [100/183], total loss: 0.00523, regularization loss: 0.29019, contrastive loss: 0.00523, Loss positive: 0.00000, Loss negative: 0.00523
2018-10-22 14:00:28.775208: Epoch [113/1000] [120/183], total loss: 0.04270, regularization loss: 0.29019, contrastive loss: 0.04270, Loss positive: 0.04127, Loss negative: 0.00143
2018-10-22 14:00:39.030418: Epoch [113/1000] [140/183], total loss: 0.00579, regularization loss: 0.29019, contrastive loss: 0.00579, Loss positive: 0.00000, Loss negative: 0.00579
2018-10-22 14:00:49.266684: Epoch [113/1000] [160/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 14:00:59.495283: Epoch [113/1000] [180/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 14:01:21.491335: Epoch [114/1000] [ 20/183], total loss: 0.00839, regularization loss: 0.29019, contrastive loss: 0.00839, Loss positive: 0.00000, Loss negative: 0.00839
2018-10-22 14:01:31.654278: Epoch [114/1000] [ 40/183], total loss: 0.00329, regularization loss: 0.29019, contrastive loss: 0.00329, Loss positive: 0.00000, Loss negative: 0.00329
2018-10-22 14:01:41.925695: Epoch [114/1000] [ 60/183], total loss: 0.00283, regularization loss: 0.29019, contrastive loss: 0.00283, Loss positive: 0.00000, Loss negative: 0.00283
2018-10-22 14:01:52.147044: Epoch [114/1000] [ 80/183], total loss: 0.00172, regularization loss: 0.29019, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 14:02:02.418552: Epoch [114/1000] [100/183], total loss: 0.00095, regularization loss: 0.29019, contrastive loss: 0.00095, Loss positive: 0.00000, Loss negative: 0.00095
2018-10-22 14:02:12.739698: Epoch [114/1000] [120/183], total loss: 0.00125, regularization loss: 0.29019, contrastive loss: 0.00125, Loss positive: 0.00000, Loss negative: 0.00125
2018-10-22 14:02:22.990291: Epoch [114/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:02:33.503886: Epoch [114/1000] [160/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 14:02:43.989686: Epoch [114/1000] [180/183], total loss: 0.00401, regularization loss: 0.29019, contrastive loss: 0.00401, Loss positive: 0.00000, Loss negative: 0.00401
2018-10-22 14:03:06.570749: Epoch [115/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:03:16.759079: Epoch [115/1000] [ 40/183], total loss: 0.00380, regularization loss: 0.29019, contrastive loss: 0.00380, Loss positive: 0.00000, Loss negative: 0.00380
2018-10-22 14:03:26.982390: Epoch [115/1000] [ 60/183], total loss: 0.04839, regularization loss: 0.29019, contrastive loss: 0.04839, Loss positive: 0.04839, Loss negative: 0.00000
2018-10-22 14:03:37.217479: Epoch [115/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:03:47.461457: Epoch [115/1000] [100/183], total loss: 0.00117, regularization loss: 0.29019, contrastive loss: 0.00117, Loss positive: 0.00000, Loss negative: 0.00117
2018-10-22 14:03:57.729702: Epoch [115/1000] [120/183], total loss: 0.00143, regularization loss: 0.29019, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 14:04:08.075603: Epoch [115/1000] [140/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 14:04:18.397756: Epoch [115/1000] [160/183], total loss: 0.00379, regularization loss: 0.29019, contrastive loss: 0.00379, Loss positive: 0.00000, Loss negative: 0.00379
2018-10-22 14:04:28.700870: Epoch [115/1000] [180/183], total loss: 0.00285, regularization loss: 0.29019, contrastive loss: 0.00285, Loss positive: 0.00000, Loss negative: 0.00285
2018-10-22 14:04:51.332053: Epoch [116/1000] [ 20/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 14:05:01.482362: Epoch [116/1000] [ 40/183], total loss: 0.02197, regularization loss: 0.29019, contrastive loss: 0.02197, Loss positive: 0.01940, Loss negative: 0.00257
2018-10-22 14:05:11.860386: Epoch [116/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:05:22.077276: Epoch [116/1000] [ 80/183], total loss: 0.00797, regularization loss: 0.29019, contrastive loss: 0.00797, Loss positive: 0.00000, Loss negative: 0.00797
2018-10-22 14:05:32.315477: Epoch [116/1000] [100/183], total loss: 0.00233, regularization loss: 0.29019, contrastive loss: 0.00233, Loss positive: 0.00000, Loss negative: 0.00233
2018-10-22 14:05:42.853390: Epoch [116/1000] [120/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 14:05:53.087395: Epoch [116/1000] [140/183], total loss: 0.00800, regularization loss: 0.29019, contrastive loss: 0.00800, Loss positive: 0.00800, Loss negative: 0.00000
2018-10-22 14:06:03.343758: Epoch [116/1000] [160/183], total loss: 0.00453, regularization loss: 0.29019, contrastive loss: 0.00453, Loss positive: 0.00000, Loss negative: 0.00453
2018-10-22 14:06:13.583632: Epoch [116/1000] [180/183], total loss: 0.00070, regularization loss: 0.29019, contrastive loss: 0.00070, Loss positive: 0.00000, Loss negative: 0.00070
2018-10-22 14:06:35.615379: Epoch [117/1000] [ 20/183], total loss: 0.00172, regularization loss: 0.29019, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 14:06:45.777674: Epoch [117/1000] [ 40/183], total loss: 0.01985, regularization loss: 0.29019, contrastive loss: 0.01985, Loss positive: 0.01904, Loss negative: 0.00081
2018-10-22 14:06:55.986990: Epoch [117/1000] [ 60/183], total loss: 0.03445, regularization loss: 0.29019, contrastive loss: 0.03445, Loss positive: 0.03438, Loss negative: 0.00006
2018-10-22 14:07:06.280661: Epoch [117/1000] [ 80/183], total loss: 0.00130, regularization loss: 0.29019, contrastive loss: 0.00130, Loss positive: 0.00000, Loss negative: 0.00130
2018-10-22 14:07:16.697925: Epoch [117/1000] [100/183], total loss: 0.00361, regularization loss: 0.29019, contrastive loss: 0.00361, Loss positive: 0.00000, Loss negative: 0.00361
2018-10-22 14:07:27.016093: Epoch [117/1000] [120/183], total loss: 0.00426, regularization loss: 0.29019, contrastive loss: 0.00426, Loss positive: 0.00000, Loss negative: 0.00426
2018-10-22 14:07:37.261297: Epoch [117/1000] [140/183], total loss: 0.02339, regularization loss: 0.29019, contrastive loss: 0.02339, Loss positive: 0.02203, Loss negative: 0.00136
2018-10-22 14:07:47.501250: Epoch [117/1000] [160/183], total loss: 0.02557, regularization loss: 0.29019, contrastive loss: 0.02557, Loss positive: 0.02292, Loss negative: 0.00265
2018-10-22 14:07:57.706701: Epoch [117/1000] [180/183], total loss: 0.00386, regularization loss: 0.29019, contrastive loss: 0.00386, Loss positive: 0.00000, Loss negative: 0.00386
2018-10-22 14:08:18.436384: Epoch [118/1000] [ 20/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 14:08:28.617093: Epoch [118/1000] [ 40/183], total loss: 0.03635, regularization loss: 0.29019, contrastive loss: 0.03635, Loss positive: 0.03621, Loss negative: 0.00014
2018-10-22 14:08:38.791430: Epoch [118/1000] [ 60/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 14:08:49.001920: Epoch [118/1000] [ 80/183], total loss: 0.00088, regularization loss: 0.29019, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 14:08:59.256954: Epoch [118/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:09:09.518354: Epoch [118/1000] [120/183], total loss: 0.03354, regularization loss: 0.29019, contrastive loss: 0.03354, Loss positive: 0.03354, Loss negative: 0.00000
2018-10-22 14:09:19.765074: Epoch [118/1000] [140/183], total loss: 0.00067, regularization loss: 0.29019, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 14:09:29.971543: Epoch [118/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:09:40.219583: Epoch [118/1000] [180/183], total loss: 0.03030, regularization loss: 0.29019, contrastive loss: 0.03030, Loss positive: 0.02484, Loss negative: 0.00545
2018-10-22 14:10:00.809445: Epoch [119/1000] [ 20/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 14:10:10.978381: Epoch [119/1000] [ 40/183], total loss: 0.01994, regularization loss: 0.29019, contrastive loss: 0.01994, Loss positive: 0.01958, Loss negative: 0.00035
2018-10-22 14:10:21.198602: Epoch [119/1000] [ 60/183], total loss: 0.00264, regularization loss: 0.29019, contrastive loss: 0.00264, Loss positive: 0.00000, Loss negative: 0.00264
2018-10-22 14:10:31.417224: Epoch [119/1000] [ 80/183], total loss: 0.00294, regularization loss: 0.29019, contrastive loss: 0.00294, Loss positive: 0.00000, Loss negative: 0.00294
2018-10-22 14:10:41.676860: Epoch [119/1000] [100/183], total loss: 0.00000, regularization loss: 0.29018, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:10:51.897104: Epoch [119/1000] [120/183], total loss: 0.00000, regularization loss: 0.29018, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:11:02.186102: Epoch [119/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:11:12.625034: Epoch [119/1000] [160/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 14:11:22.857416: Epoch [119/1000] [180/183], total loss: 0.04920, regularization loss: 0.29019, contrastive loss: 0.04920, Loss positive: 0.04920, Loss negative: 0.00000
2018-10-22 14:11:44.307929: Epoch [120/1000] [ 20/183], total loss: 0.04259, regularization loss: 0.29019, contrastive loss: 0.04259, Loss positive: 0.04240, Loss negative: 0.00019
2018-10-22 14:11:54.471651: Epoch [120/1000] [ 40/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 14:12:04.643910: Epoch [120/1000] [ 60/183], total loss: 0.00116, regularization loss: 0.29019, contrastive loss: 0.00116, Loss positive: 0.00000, Loss negative: 0.00116
2018-10-22 14:12:14.863094: Epoch [120/1000] [ 80/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 14:12:25.120472: Epoch [120/1000] [100/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 14:12:35.349089: Epoch [120/1000] [120/183], total loss: 0.02997, regularization loss: 0.29019, contrastive loss: 0.02997, Loss positive: 0.02452, Loss negative: 0.00545
2018-10-22 14:12:45.582050: Epoch [120/1000] [140/183], total loss: 0.02330, regularization loss: 0.29019, contrastive loss: 0.02330, Loss positive: 0.02312, Loss negative: 0.00018
2018-10-22 14:12:55.832734: Epoch [120/1000] [160/183], total loss: 0.00095, regularization loss: 0.29019, contrastive loss: 0.00095, Loss positive: 0.00000, Loss negative: 0.00095
2018-10-22 14:13:06.054156: Epoch [120/1000] [180/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
Recall@1: 0.24207
Recall@2: 0.34976
Recall@4: 0.47333
Recall@8: 0.61546
Recall@16: 0.74882
Recall@32: 0.85550
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 76.  47.  41.  72.  82.  55.  77. 101.  51.  53.  52.  73.  78.  53.
  50.  70.  67.  68.  40.  72.  84.  75.  16.  51.  52.  80.  42.  68.
  81.  52.  77.  71.  74.  55.  66.  61.  85.  78.  57.  27.  73.  55.
  36.  85.  57.  26.  71.  66.  43.  41.  66.  53.  71.  82.  61.  45.
  30.  78.  52.  66.  74.  79.  43.  52.  63.  70.  62.  70.  41.  35.
  70.  44.  35.  71.  31.  72.  64.  80.  39.  56.  69.  65.  43.  50.
  53.  61.  75.  87.  54.  28.  23.  40.  77.  48.  50.  27.  92.  41.
  47.  56.]
Purity is 0.220
count_cross = [[0. 0. 0. ... 1. 3. 0.]
 [0. 0. 0. ... 0. 3. 0.]
 [0. 0. 0. ... 0. 1. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 2.]
 [0. 1. 2. ... 0. 0. 0.]]
Mutual information is 1.96702
5924.0
5924
Entropy cluster is 4.55960
Entropy class is 4.60444
normalized_mutual_information is 0.42929
tp_and_fp = 187522.0
tp = 17951.0
fp is 169571.0
fn is 154799.0
RI is 0.9815109799254739
Precision is 0.095727434647668
Recall is 0.10391316931982633
F_1 is 0.09965248478927032

normalized_mutual_information = 0.4292919507008277
RI = 0.9815109799254739
F_1 = 0.09965248478927032

The NN is 0.24207
The FT is 0.12765
The ST is 0.20199
The DCG is 0.51136
The E is 0.10622
The MAP 0.10226

2018-10-22 14:14:36.440727: Epoch [121/1000] [ 20/183], total loss: 0.04255, regularization loss: 0.29019, contrastive loss: 0.04255, Loss positive: 0.03909, Loss negative: 0.00346
2018-10-22 14:14:46.506881: Epoch [121/1000] [ 40/183], total loss: 0.00250, regularization loss: 0.29019, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 14:14:56.611265: Epoch [121/1000] [ 60/183], total loss: 0.01962, regularization loss: 0.29019, contrastive loss: 0.01962, Loss positive: 0.01743, Loss negative: 0.00219
2018-10-22 14:15:06.724950: Epoch [121/1000] [ 80/183], total loss: 0.02748, regularization loss: 0.29019, contrastive loss: 0.02748, Loss positive: 0.02405, Loss negative: 0.00343
2018-10-22 14:15:16.908089: Epoch [121/1000] [100/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 14:15:27.091805: Epoch [121/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:15:37.322136: Epoch [121/1000] [140/183], total loss: 0.00454, regularization loss: 0.29019, contrastive loss: 0.00454, Loss positive: 0.00000, Loss negative: 0.00454
2018-10-22 14:15:47.767847: Epoch [121/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:15:57.986501: Epoch [121/1000] [180/183], total loss: 0.07317, regularization loss: 0.29019, contrastive loss: 0.07317, Loss positive: 0.07080, Loss negative: 0.00238
2018-10-22 14:16:18.724636: Epoch [122/1000] [ 20/183], total loss: 0.00296, regularization loss: 0.29019, contrastive loss: 0.00296, Loss positive: 0.00000, Loss negative: 0.00296
2018-10-22 14:16:28.870097: Epoch [122/1000] [ 40/183], total loss: 0.00275, regularization loss: 0.29019, contrastive loss: 0.00275, Loss positive: 0.00000, Loss negative: 0.00275
2018-10-22 14:16:39.075796: Epoch [122/1000] [ 60/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 14:16:49.307433: Epoch [122/1000] [ 80/183], total loss: 0.00353, regularization loss: 0.29019, contrastive loss: 0.00353, Loss positive: 0.00000, Loss negative: 0.00353
2018-10-22 14:16:59.516000: Epoch [122/1000] [100/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 14:17:09.905654: Epoch [122/1000] [120/183], total loss: 0.00103, regularization loss: 0.29019, contrastive loss: 0.00103, Loss positive: 0.00000, Loss negative: 0.00103
2018-10-22 14:17:20.125417: Epoch [122/1000] [140/183], total loss: 0.00123, regularization loss: 0.29019, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 14:17:30.376259: Epoch [122/1000] [160/183], total loss: 0.00309, regularization loss: 0.29019, contrastive loss: 0.00309, Loss positive: 0.00000, Loss negative: 0.00309
2018-10-22 14:17:40.631264: Epoch [122/1000] [180/183], total loss: 0.01346, regularization loss: 0.29019, contrastive loss: 0.01346, Loss positive: 0.01342, Loss negative: 0.00004
2018-10-22 14:18:01.298262: Epoch [123/1000] [ 20/183], total loss: 0.00146, regularization loss: 0.29019, contrastive loss: 0.00146, Loss positive: 0.00000, Loss negative: 0.00146
2018-10-22 14:18:11.498727: Epoch [123/1000] [ 40/183], total loss: 0.00071, regularization loss: 0.29019, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 14:18:21.676760: Epoch [123/1000] [ 60/183], total loss: 0.01120, regularization loss: 0.29019, contrastive loss: 0.01120, Loss positive: 0.00000, Loss negative: 0.01120
2018-10-22 14:18:31.940219: Epoch [123/1000] [ 80/183], total loss: 0.00348, regularization loss: 0.29019, contrastive loss: 0.00348, Loss positive: 0.00000, Loss negative: 0.00348
2018-10-22 14:18:42.165909: Epoch [123/1000] [100/183], total loss: 0.00458, regularization loss: 0.29019, contrastive loss: 0.00458, Loss positive: 0.00000, Loss negative: 0.00458
2018-10-22 14:18:52.393746: Epoch [123/1000] [120/183], total loss: 0.03314, regularization loss: 0.29019, contrastive loss: 0.03314, Loss positive: 0.03313, Loss negative: 0.00000
2018-10-22 14:19:02.626188: Epoch [123/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:19:12.877466: Epoch [123/1000] [160/183], total loss: 0.02145, regularization loss: 0.29019, contrastive loss: 0.02145, Loss positive: 0.01949, Loss negative: 0.00197
2018-10-22 14:19:23.136315: Epoch [123/1000] [180/183], total loss: 0.01354, regularization loss: 0.29019, contrastive loss: 0.01354, Loss positive: 0.01097, Loss negative: 0.00257
2018-10-22 14:19:43.752635: Epoch [124/1000] [ 20/183], total loss: 0.04483, regularization loss: 0.29019, contrastive loss: 0.04483, Loss positive: 0.04326, Loss negative: 0.00157
2018-10-22 14:19:53.921754: Epoch [124/1000] [ 40/183], total loss: 0.00019, regularization loss: 0.29019, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 14:20:04.083760: Epoch [124/1000] [ 60/183], total loss: 0.00509, regularization loss: 0.29019, contrastive loss: 0.00509, Loss positive: 0.00000, Loss negative: 0.00509
2018-10-22 14:20:14.302808: Epoch [124/1000] [ 80/183], total loss: 0.00102, regularization loss: 0.29019, contrastive loss: 0.00102, Loss positive: 0.00000, Loss negative: 0.00102
2018-10-22 14:20:24.545560: Epoch [124/1000] [100/183], total loss: 0.00014, regularization loss: 0.29019, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 14:20:34.804414: Epoch [124/1000] [120/183], total loss: 0.02391, regularization loss: 0.29019, contrastive loss: 0.02391, Loss positive: 0.02262, Loss negative: 0.00129
2018-10-22 14:20:45.024560: Epoch [124/1000] [140/183], total loss: 0.00076, regularization loss: 0.29019, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 14:20:55.290171: Epoch [124/1000] [160/183], total loss: 0.01359, regularization loss: 0.29019, contrastive loss: 0.01359, Loss positive: 0.01333, Loss negative: 0.00025
2018-10-22 14:21:05.511582: Epoch [124/1000] [180/183], total loss: 0.00213, regularization loss: 0.29019, contrastive loss: 0.00213, Loss positive: 0.00000, Loss negative: 0.00213
2018-10-22 14:21:26.976419: Epoch [125/1000] [ 20/183], total loss: 0.00371, regularization loss: 0.29019, contrastive loss: 0.00371, Loss positive: 0.00000, Loss negative: 0.00371
2018-10-22 14:21:37.139588: Epoch [125/1000] [ 40/183], total loss: 0.00890, regularization loss: 0.29019, contrastive loss: 0.00890, Loss positive: 0.00690, Loss negative: 0.00201
2018-10-22 14:21:47.510415: Epoch [125/1000] [ 60/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 14:21:57.758210: Epoch [125/1000] [ 80/183], total loss: 0.02084, regularization loss: 0.29019, contrastive loss: 0.02084, Loss positive: 0.02072, Loss negative: 0.00012
2018-10-22 14:22:07.979797: Epoch [125/1000] [100/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 14:22:18.217900: Epoch [125/1000] [120/183], total loss: 0.00020, regularization loss: 0.29019, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 14:22:28.467667: Epoch [125/1000] [140/183], total loss: 0.00126, regularization loss: 0.29019, contrastive loss: 0.00126, Loss positive: 0.00000, Loss negative: 0.00126
2018-10-22 14:22:38.714344: Epoch [125/1000] [160/183], total loss: 0.02525, regularization loss: 0.29019, contrastive loss: 0.02525, Loss positive: 0.02520, Loss negative: 0.00005
2018-10-22 14:22:48.966341: Epoch [125/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:23:10.429122: Epoch [126/1000] [ 20/183], total loss: 0.00069, regularization loss: 0.29019, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 14:23:20.601168: Epoch [126/1000] [ 40/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 14:23:30.788155: Epoch [126/1000] [ 60/183], total loss: 0.00505, regularization loss: 0.29019, contrastive loss: 0.00505, Loss positive: 0.00000, Loss negative: 0.00505
2018-10-22 14:23:41.166507: Epoch [126/1000] [ 80/183], total loss: 0.00067, regularization loss: 0.29019, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 14:23:51.446174: Epoch [126/1000] [100/183], total loss: 0.02474, regularization loss: 0.29019, contrastive loss: 0.02474, Loss positive: 0.02402, Loss negative: 0.00072
2018-10-22 14:24:01.666177: Epoch [126/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:24:11.896943: Epoch [126/1000] [140/183], total loss: 0.04952, regularization loss: 0.29019, contrastive loss: 0.04952, Loss positive: 0.04519, Loss negative: 0.00434
2018-10-22 14:24:22.110216: Epoch [126/1000] [160/183], total loss: 0.00201, regularization loss: 0.29019, contrastive loss: 0.00201, Loss positive: 0.00000, Loss negative: 0.00201
2018-10-22 14:24:32.351148: Epoch [126/1000] [180/183], total loss: 0.00900, regularization loss: 0.29019, contrastive loss: 0.00900, Loss positive: 0.00000, Loss negative: 0.00900
2018-10-22 14:24:53.103595: Epoch [127/1000] [ 20/183], total loss: 0.00246, regularization loss: 0.29019, contrastive loss: 0.00246, Loss positive: 0.00000, Loss negative: 0.00246
2018-10-22 14:25:03.284841: Epoch [127/1000] [ 40/183], total loss: 0.00042, regularization loss: 0.29019, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 14:25:13.455524: Epoch [127/1000] [ 60/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 14:25:23.655281: Epoch [127/1000] [ 80/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 14:25:33.912589: Epoch [127/1000] [100/183], total loss: 0.00132, regularization loss: 0.29019, contrastive loss: 0.00132, Loss positive: 0.00000, Loss negative: 0.00132
2018-10-22 14:25:44.138915: Epoch [127/1000] [120/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 14:25:54.431784: Epoch [127/1000] [140/183], total loss: 0.01789, regularization loss: 0.29019, contrastive loss: 0.01789, Loss positive: 0.01785, Loss negative: 0.00004
2018-10-22 14:26:04.730623: Epoch [127/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:26:14.962447: Epoch [127/1000] [180/183], total loss: 0.02235, regularization loss: 0.29019, contrastive loss: 0.02235, Loss positive: 0.02064, Loss negative: 0.00171
2018-10-22 14:26:35.938222: Epoch [128/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29018, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:26:46.117496: Epoch [128/1000] [ 40/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 14:26:56.335882: Epoch [128/1000] [ 60/183], total loss: 0.00658, regularization loss: 0.29019, contrastive loss: 0.00658, Loss positive: 0.00000, Loss negative: 0.00658
2018-10-22 14:27:06.582665: Epoch [128/1000] [ 80/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 14:27:16.803777: Epoch [128/1000] [100/183], total loss: 0.00257, regularization loss: 0.29019, contrastive loss: 0.00257, Loss positive: 0.00000, Loss negative: 0.00257
2018-10-22 14:27:27.175056: Epoch [128/1000] [120/183], total loss: 0.00060, regularization loss: 0.29019, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 14:27:37.596308: Epoch [128/1000] [140/183], total loss: 0.03356, regularization loss: 0.29019, contrastive loss: 0.03356, Loss positive: 0.03356, Loss negative: 0.00000
2018-10-22 14:27:47.860499: Epoch [128/1000] [160/183], total loss: 0.00772, regularization loss: 0.29019, contrastive loss: 0.00772, Loss positive: 0.00772, Loss negative: 0.00000
2018-10-22 14:27:58.105821: Epoch [128/1000] [180/183], total loss: 0.02081, regularization loss: 0.29019, contrastive loss: 0.02081, Loss positive: 0.02081, Loss negative: 0.00000
2018-10-22 14:28:18.729463: Epoch [129/1000] [ 20/183], total loss: 0.00017, regularization loss: 0.29018, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 14:28:28.914259: Epoch [129/1000] [ 40/183], total loss: 0.00205, regularization loss: 0.29019, contrastive loss: 0.00205, Loss positive: 0.00000, Loss negative: 0.00205
2018-10-22 14:28:39.255252: Epoch [129/1000] [ 60/183], total loss: 0.02380, regularization loss: 0.29019, contrastive loss: 0.02380, Loss positive: 0.02194, Loss negative: 0.00186
2018-10-22 14:28:49.498848: Epoch [129/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:28:59.717281: Epoch [129/1000] [100/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 14:29:09.974482: Epoch [129/1000] [120/183], total loss: 0.03187, regularization loss: 0.29019, contrastive loss: 0.03187, Loss positive: 0.02628, Loss negative: 0.00559
2018-10-22 14:29:20.212288: Epoch [129/1000] [140/183], total loss: 0.00149, regularization loss: 0.29019, contrastive loss: 0.00149, Loss positive: 0.00000, Loss negative: 0.00149
2018-10-22 14:29:30.490554: Epoch [129/1000] [160/183], total loss: 0.00166, regularization loss: 0.29019, contrastive loss: 0.00166, Loss positive: 0.00000, Loss negative: 0.00166
2018-10-22 14:29:40.727978: Epoch [129/1000] [180/183], total loss: 0.00061, regularization loss: 0.29019, contrastive loss: 0.00061, Loss positive: 0.00000, Loss negative: 0.00061
2018-10-22 14:30:01.323239: Epoch [130/1000] [ 20/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 14:30:11.483862: Epoch [130/1000] [ 40/183], total loss: 0.00213, regularization loss: 0.29019, contrastive loss: 0.00213, Loss positive: 0.00000, Loss negative: 0.00213
2018-10-22 14:30:21.657313: Epoch [130/1000] [ 60/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 14:30:31.898602: Epoch [130/1000] [ 80/183], total loss: 0.00111, regularization loss: 0.29019, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 14:30:42.144490: Epoch [130/1000] [100/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 14:30:52.384236: Epoch [130/1000] [120/183], total loss: 0.00007, regularization loss: 0.29018, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 14:31:02.617847: Epoch [130/1000] [140/183], total loss: 0.00215, regularization loss: 0.29018, contrastive loss: 0.00215, Loss positive: 0.00000, Loss negative: 0.00215
2018-10-22 14:31:12.883695: Epoch [130/1000] [160/183], total loss: 0.00038, regularization loss: 0.29019, contrastive loss: 0.00038, Loss positive: 0.00000, Loss negative: 0.00038
2018-10-22 14:31:23.095864: Epoch [130/1000] [180/183], total loss: 0.00315, regularization loss: 0.29019, contrastive loss: 0.00315, Loss positive: 0.00000, Loss negative: 0.00315
Recall@1: 0.24190
Recall@2: 0.34740
Recall@4: 0.47974
Recall@8: 0.62390
Recall@16: 0.74730
Recall@32: 0.85381
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 59.  76.  57.  72.  69.  73.  68.  53.  86.  67.  50.  37.  72.  58.
  48.  50.  65.  72.  28.  38.  64.  82.  66.  43.  53.  86.  57.  51.
  61.  46.  74.  41.  44.  50.  60.  75.  93.  33.  37.  40.  71.  49.
  78.  78.  43.  73.  59.  74.  81. 100.  68.  28.  38.  65.  65.  22.
  65.  36.  60.  25.  57.  69.  67.  49.  84.  85.  86.  78.  26.  95.
  75.  64.  68.  54.  59.  48.  68.  53.  27.  51.  55.  89.  62.  58.
  42.  51.  91.  97.  69.  52.  36.  71.  38.  35.  42.  27.  40.  63.
  77.  34.]
Purity is 0.225
count_cross = [[ 0.  0.  0. ...  0.  0. 10.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  1.  2. ...  1.  0.  0.]
 ...
 [ 0.  9. 11. ...  2.  0.  0.]
 [ 0.  1.  1. ...  9.  4.  0.]
 [ 0.  0.  0. ...  0.  1.  0.]]
Mutual information is 1.98849
5924.0
5924
Entropy cluster is 4.55585
Entropy class is 4.60444
normalized_mutual_information is 0.43415
tp_and_fp = 189155.0
tp = 18852.0
fp is 170303.0
fn is 153898.0
RI is 0.981520612889042
Precision is 0.09966429647643467
Recall is 0.1091287988422576
F_1 is 0.10418203672234425

normalized_mutual_information = 0.4341532367235891
RI = 0.981520612889042
F_1 = 0.10418203672234425

The NN is 0.24190
The FT is 0.12787
The ST is 0.20039
The DCG is 0.51164
The E is 0.10738
The MAP 0.10176

2018-10-22 14:32:58.110401: Epoch [131/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:33:08.215206: Epoch [131/1000] [ 40/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 14:33:18.362288: Epoch [131/1000] [ 60/183], total loss: 0.00010, regularization loss: 0.29019, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 14:33:28.518186: Epoch [131/1000] [ 80/183], total loss: 0.00485, regularization loss: 0.29019, contrastive loss: 0.00485, Loss positive: 0.00000, Loss negative: 0.00485
2018-10-22 14:33:38.704998: Epoch [131/1000] [100/183], total loss: 0.02969, regularization loss: 0.29019, contrastive loss: 0.02969, Loss positive: 0.02351, Loss negative: 0.00618
2018-10-22 14:33:48.885788: Epoch [131/1000] [120/183], total loss: 0.02530, regularization loss: 0.29019, contrastive loss: 0.02530, Loss positive: 0.02336, Loss negative: 0.00194
2018-10-22 14:33:59.206959: Epoch [131/1000] [140/183], total loss: 0.03330, regularization loss: 0.29019, contrastive loss: 0.03330, Loss positive: 0.03327, Loss negative: 0.00003
2018-10-22 14:34:09.521567: Epoch [131/1000] [160/183], total loss: 0.06762, regularization loss: 0.29019, contrastive loss: 0.06762, Loss positive: 0.06762, Loss negative: 0.00000
2018-10-22 14:34:19.844358: Epoch [131/1000] [180/183], total loss: 0.00202, regularization loss: 0.29019, contrastive loss: 0.00202, Loss positive: 0.00000, Loss negative: 0.00202
2018-10-22 14:34:42.587240: Epoch [132/1000] [ 20/183], total loss: 0.03366, regularization loss: 0.29019, contrastive loss: 0.03366, Loss positive: 0.03364, Loss negative: 0.00002
2018-10-22 14:34:52.707644: Epoch [132/1000] [ 40/183], total loss: 0.02488, regularization loss: 0.29018, contrastive loss: 0.02488, Loss positive: 0.02376, Loss negative: 0.00112
2018-10-22 14:35:02.909626: Epoch [132/1000] [ 60/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 14:35:13.104966: Epoch [132/1000] [ 80/183], total loss: 0.00101, regularization loss: 0.29018, contrastive loss: 0.00101, Loss positive: 0.00000, Loss negative: 0.00101
2018-10-22 14:35:23.333318: Epoch [132/1000] [100/183], total loss: 0.00212, regularization loss: 0.29018, contrastive loss: 0.00212, Loss positive: 0.00000, Loss negative: 0.00212
2018-10-22 14:35:33.610798: Epoch [132/1000] [120/183], total loss: 0.00030, regularization loss: 0.29018, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 14:35:43.822363: Epoch [132/1000] [140/183], total loss: 0.04478, regularization loss: 0.29018, contrastive loss: 0.04478, Loss positive: 0.04076, Loss negative: 0.00403
2018-10-22 14:35:54.066116: Epoch [132/1000] [160/183], total loss: 0.01042, regularization loss: 0.29018, contrastive loss: 0.01042, Loss positive: 0.00000, Loss negative: 0.01042
2018-10-22 14:36:04.301184: Epoch [132/1000] [180/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 14:36:25.734423: Epoch [133/1000] [ 20/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 14:36:35.958995: Epoch [133/1000] [ 40/183], total loss: 0.00027, regularization loss: 0.29019, contrastive loss: 0.00027, Loss positive: 0.00000, Loss negative: 0.00027
2018-10-22 14:36:46.115220: Epoch [133/1000] [ 60/183], total loss: 0.00243, regularization loss: 0.29018, contrastive loss: 0.00243, Loss positive: 0.00000, Loss negative: 0.00243
2018-10-22 14:36:56.338318: Epoch [133/1000] [ 80/183], total loss: 0.04786, regularization loss: 0.29018, contrastive loss: 0.04786, Loss positive: 0.04618, Loss negative: 0.00168
2018-10-22 14:37:06.582263: Epoch [133/1000] [100/183], total loss: 0.00643, regularization loss: 0.29018, contrastive loss: 0.00643, Loss positive: 0.00399, Loss negative: 0.00243
2018-10-22 14:37:16.795653: Epoch [133/1000] [120/183], total loss: 0.00069, regularization loss: 0.29018, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 14:37:27.201377: Epoch [133/1000] [140/183], total loss: 0.00477, regularization loss: 0.29018, contrastive loss: 0.00477, Loss positive: 0.00000, Loss negative: 0.00477
2018-10-22 14:37:37.481060: Epoch [133/1000] [160/183], total loss: 0.00082, regularization loss: 0.29019, contrastive loss: 0.00082, Loss positive: 0.00000, Loss negative: 0.00082
2018-10-22 14:37:47.719248: Epoch [133/1000] [180/183], total loss: 0.02078, regularization loss: 0.29019, contrastive loss: 0.02078, Loss positive: 0.01308, Loss negative: 0.00770
2018-10-22 14:38:09.213166: Epoch [134/1000] [ 20/183], total loss: 0.00734, regularization loss: 0.29019, contrastive loss: 0.00734, Loss positive: 0.00000, Loss negative: 0.00734
2018-10-22 14:38:19.372292: Epoch [134/1000] [ 40/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 14:38:29.592729: Epoch [134/1000] [ 60/183], total loss: 0.02621, regularization loss: 0.29019, contrastive loss: 0.02621, Loss positive: 0.02589, Loss negative: 0.00031
2018-10-22 14:38:39.951762: Epoch [134/1000] [ 80/183], total loss: 0.02216, regularization loss: 0.29019, contrastive loss: 0.02216, Loss positive: 0.02210, Loss negative: 0.00006
2018-10-22 14:38:50.371927: Epoch [134/1000] [100/183], total loss: 0.00349, regularization loss: 0.29019, contrastive loss: 0.00349, Loss positive: 0.00000, Loss negative: 0.00349
2018-10-22 14:39:00.643744: Epoch [134/1000] [120/183], total loss: 0.00290, regularization loss: 0.29018, contrastive loss: 0.00290, Loss positive: 0.00000, Loss negative: 0.00290
2018-10-22 14:39:10.890711: Epoch [134/1000] [140/183], total loss: 0.02752, regularization loss: 0.29018, contrastive loss: 0.02752, Loss positive: 0.02296, Loss negative: 0.00455
2018-10-22 14:39:21.151156: Epoch [134/1000] [160/183], total loss: 0.00055, regularization loss: 0.29018, contrastive loss: 0.00055, Loss positive: 0.00000, Loss negative: 0.00055
2018-10-22 14:39:31.378068: Epoch [134/1000] [180/183], total loss: 0.01259, regularization loss: 0.29018, contrastive loss: 0.01259, Loss positive: 0.00646, Loss negative: 0.00613
2018-10-22 14:39:52.943791: Epoch [135/1000] [ 20/183], total loss: 0.00201, regularization loss: 0.29019, contrastive loss: 0.00201, Loss positive: 0.00000, Loss negative: 0.00201
2018-10-22 14:40:03.116256: Epoch [135/1000] [ 40/183], total loss: 0.00156, regularization loss: 0.29019, contrastive loss: 0.00156, Loss positive: 0.00000, Loss negative: 0.00156
2018-10-22 14:40:13.298623: Epoch [135/1000] [ 60/183], total loss: 0.01079, regularization loss: 0.29019, contrastive loss: 0.01079, Loss positive: 0.00000, Loss negative: 0.01079
2018-10-22 14:40:23.718702: Epoch [135/1000] [ 80/183], total loss: 0.00756, regularization loss: 0.29019, contrastive loss: 0.00756, Loss positive: 0.00000, Loss negative: 0.00756
2018-10-22 14:40:33.936912: Epoch [135/1000] [100/183], total loss: 0.00047, regularization loss: 0.29019, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 14:40:44.204613: Epoch [135/1000] [120/183], total loss: 0.03450, regularization loss: 0.29019, contrastive loss: 0.03450, Loss positive: 0.02888, Loss negative: 0.00562
2018-10-22 14:40:54.441788: Epoch [135/1000] [140/183], total loss: 0.00024, regularization loss: 0.29019, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 14:41:04.820672: Epoch [135/1000] [160/183], total loss: 0.00017, regularization loss: 0.29019, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 14:41:15.121188: Epoch [135/1000] [180/183], total loss: 0.02448, regularization loss: 0.29019, contrastive loss: 0.02448, Loss positive: 0.02278, Loss negative: 0.00170
2018-10-22 14:41:37.689513: Epoch [136/1000] [ 20/183], total loss: 0.04100, regularization loss: 0.29019, contrastive loss: 0.04100, Loss positive: 0.03744, Loss negative: 0.00355
2018-10-22 14:41:47.877057: Epoch [136/1000] [ 40/183], total loss: 0.00146, regularization loss: 0.29019, contrastive loss: 0.00146, Loss positive: 0.00000, Loss negative: 0.00146
2018-10-22 14:41:58.076753: Epoch [136/1000] [ 60/183], total loss: 0.01765, regularization loss: 0.29019, contrastive loss: 0.01765, Loss positive: 0.01266, Loss negative: 0.00498
2018-10-22 14:42:08.489700: Epoch [136/1000] [ 80/183], total loss: 0.00219, regularization loss: 0.29019, contrastive loss: 0.00219, Loss positive: 0.00000, Loss negative: 0.00219
2018-10-22 14:42:18.737756: Epoch [136/1000] [100/183], total loss: 0.00010, regularization loss: 0.29018, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 14:42:29.058713: Epoch [136/1000] [120/183], total loss: 0.02240, regularization loss: 0.29019, contrastive loss: 0.02240, Loss positive: 0.01958, Loss negative: 0.00282
2018-10-22 14:42:39.268972: Epoch [136/1000] [140/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 14:42:49.506914: Epoch [136/1000] [160/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 14:43:00.006040: Epoch [136/1000] [180/183], total loss: 0.05355, regularization loss: 0.29019, contrastive loss: 0.05355, Loss positive: 0.05353, Loss negative: 0.00002
2018-10-22 14:43:22.824770: Epoch [137/1000] [ 20/183], total loss: 0.00173, regularization loss: 0.29019, contrastive loss: 0.00173, Loss positive: 0.00000, Loss negative: 0.00173
2018-10-22 14:43:33.017553: Epoch [137/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:43:43.209723: Epoch [137/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:43:53.401160: Epoch [137/1000] [ 80/183], total loss: 0.00209, regularization loss: 0.29019, contrastive loss: 0.00209, Loss positive: 0.00000, Loss negative: 0.00209
2018-10-22 14:44:03.663103: Epoch [137/1000] [100/183], total loss: 0.02867, regularization loss: 0.29019, contrastive loss: 0.02867, Loss positive: 0.02212, Loss negative: 0.00655
2018-10-22 14:44:13.905044: Epoch [137/1000] [120/183], total loss: 0.00593, regularization loss: 0.29019, contrastive loss: 0.00593, Loss positive: 0.00000, Loss negative: 0.00593
2018-10-22 14:44:24.137431: Epoch [137/1000] [140/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 14:44:34.442915: Epoch [137/1000] [160/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 14:44:44.721763: Epoch [137/1000] [180/183], total loss: 0.02908, regularization loss: 0.29019, contrastive loss: 0.02908, Loss positive: 0.02907, Loss negative: 0.00000
2018-10-22 14:45:06.232650: Epoch [138/1000] [ 20/183], total loss: 0.01571, regularization loss: 0.29019, contrastive loss: 0.01571, Loss positive: 0.01525, Loss negative: 0.00046
2018-10-22 14:45:16.427903: Epoch [138/1000] [ 40/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 14:45:26.628085: Epoch [138/1000] [ 60/183], total loss: 0.00382, regularization loss: 0.29019, contrastive loss: 0.00382, Loss positive: 0.00000, Loss negative: 0.00382
2018-10-22 14:45:36.828770: Epoch [138/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:45:47.109968: Epoch [138/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:45:57.348747: Epoch [138/1000] [120/183], total loss: 0.00397, regularization loss: 0.29019, contrastive loss: 0.00397, Loss positive: 0.00000, Loss negative: 0.00397
2018-10-22 14:46:07.568633: Epoch [138/1000] [140/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 14:46:17.799311: Epoch [138/1000] [160/183], total loss: 0.02192, regularization loss: 0.29019, contrastive loss: 0.02192, Loss positive: 0.02192, Loss negative: 0.00000
2018-10-22 14:46:28.031014: Epoch [138/1000] [180/183], total loss: 0.03035, regularization loss: 0.29019, contrastive loss: 0.03035, Loss positive: 0.03035, Loss negative: 0.00000
2018-10-22 14:46:49.507230: Epoch [139/1000] [ 20/183], total loss: 0.00068, regularization loss: 0.29019, contrastive loss: 0.00068, Loss positive: 0.00000, Loss negative: 0.00068
2018-10-22 14:46:59.663298: Epoch [139/1000] [ 40/183], total loss: 0.05428, regularization loss: 0.29019, contrastive loss: 0.05428, Loss positive: 0.05261, Loss negative: 0.00168
2018-10-22 14:47:09.850156: Epoch [139/1000] [ 60/183], total loss: 0.01268, regularization loss: 0.29019, contrastive loss: 0.01268, Loss positive: 0.01261, Loss negative: 0.00007
2018-10-22 14:47:20.054941: Epoch [139/1000] [ 80/183], total loss: 0.00623, regularization loss: 0.29019, contrastive loss: 0.00623, Loss positive: 0.00000, Loss negative: 0.00623
2018-10-22 14:47:30.283648: Epoch [139/1000] [100/183], total loss: 0.00019, regularization loss: 0.29019, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 14:47:40.528556: Epoch [139/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:47:50.779113: Epoch [139/1000] [140/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 14:48:01.045637: Epoch [139/1000] [160/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 14:48:11.280937: Epoch [139/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:48:33.115115: Epoch [140/1000] [ 20/183], total loss: 0.00071, regularization loss: 0.29019, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 14:48:43.300372: Epoch [140/1000] [ 40/183], total loss: 0.00023, regularization loss: 0.29019, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 14:48:53.436751: Epoch [140/1000] [ 60/183], total loss: 0.02008, regularization loss: 0.29019, contrastive loss: 0.02008, Loss positive: 0.01484, Loss negative: 0.00524
2018-10-22 14:49:03.689565: Epoch [140/1000] [ 80/183], total loss: 0.00294, regularization loss: 0.29019, contrastive loss: 0.00294, Loss positive: 0.00000, Loss negative: 0.00294
2018-10-22 14:49:13.918871: Epoch [140/1000] [100/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 14:49:24.166486: Epoch [140/1000] [120/183], total loss: 0.02413, regularization loss: 0.29019, contrastive loss: 0.02413, Loss positive: 0.02053, Loss negative: 0.00360
2018-10-22 14:49:34.410300: Epoch [140/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:49:44.675953: Epoch [140/1000] [160/183], total loss: 0.02098, regularization loss: 0.29019, contrastive loss: 0.02098, Loss positive: 0.02097, Loss negative: 0.00000
2018-10-22 14:49:54.917275: Epoch [140/1000] [180/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
Recall@1: 0.25743
Recall@2: 0.36597
Recall@4: 0.49156
Recall@8: 0.63656
Recall@16: 0.76772
Recall@32: 0.86597
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 74.  61.  56.  56.  46.  64.  51.  45.  54.  76.  50.  56.  45.  44.
  65.  52. 101.  46.  36.  83.  52.  62.  71.  84.  86.  42.  66.  42.
  66.  54.  84.  46.  50.  47.  65.  47.  51.  58.  67.  90.  94.  52.
  82.  68.  84.  76.  44.  86.  38.  47.  90.  43.  96.  23.  69.  78.
  71.  46.  58.  35.  72.  54.  50.  52.  36.  48.  51.  44.  39.  57.
  49.  85.  43.  67.  61.  42.  38.  41.  32.  88.  65.  48.  49.  58.
  50.  65.  52.  60.  44.  27.  75.  67.  94.  93.  52.  59.  66.  58.
  58.  64.]
Purity is 0.232
count_cross = [[0. 0. 0. ... 6. 9. 0.]
 [0. 0. 0. ... 5. 1. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 2. 0.]]
Mutual information is 2.02733
5924.0
5924
Entropy cluster is 4.56492
Entropy class is 4.60444
normalized_mutual_information is 0.44220
tp_and_fp = 186832.0
tp = 19882.0
fp is 166950.0
fn is 152868.0
RI is 0.9817704429441848
Precision is 0.10641645970711655
Recall is 0.11509117221418234
F_1 is 0.11058395581536339

normalized_mutual_information = 0.4421953909517812
RI = 0.9817704429441848
F_1 = 0.11058395581536339

The NN is 0.25743
The FT is 0.13638
The ST is 0.21144
The DCG is 0.51881
The E is 0.11540
The MAP 0.10902

2018-10-22 14:51:25.640568: Epoch [141/1000] [ 20/183], total loss: 0.01797, regularization loss: 0.29019, contrastive loss: 0.01797, Loss positive: 0.01792, Loss negative: 0.00005
2018-10-22 14:51:35.723636: Epoch [141/1000] [ 40/183], total loss: 0.00223, regularization loss: 0.29019, contrastive loss: 0.00223, Loss positive: 0.00000, Loss negative: 0.00223
2018-10-22 14:51:45.874771: Epoch [141/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:51:56.029939: Epoch [141/1000] [ 80/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 14:52:06.233536: Epoch [141/1000] [100/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 14:52:16.436008: Epoch [141/1000] [120/183], total loss: 0.00176, regularization loss: 0.29019, contrastive loss: 0.00176, Loss positive: 0.00000, Loss negative: 0.00176
2018-10-22 14:52:26.699724: Epoch [141/1000] [140/183], total loss: 0.00214, regularization loss: 0.29019, contrastive loss: 0.00214, Loss positive: 0.00000, Loss negative: 0.00214
2018-10-22 14:52:36.977764: Epoch [141/1000] [160/183], total loss: 0.00623, regularization loss: 0.29019, contrastive loss: 0.00623, Loss positive: 0.00000, Loss negative: 0.00623
2018-10-22 14:52:47.265692: Epoch [141/1000] [180/183], total loss: 0.03027, regularization loss: 0.29019, contrastive loss: 0.03027, Loss positive: 0.02957, Loss negative: 0.00070
2018-10-22 14:53:09.898388: Epoch [142/1000] [ 20/183], total loss: 0.02001, regularization loss: 0.29019, contrastive loss: 0.02001, Loss positive: 0.01963, Loss negative: 0.00037
2018-10-22 14:53:20.092822: Epoch [142/1000] [ 40/183], total loss: 0.02007, regularization loss: 0.29019, contrastive loss: 0.02007, Loss positive: 0.01387, Loss negative: 0.00620
2018-10-22 14:53:30.272722: Epoch [142/1000] [ 60/183], total loss: 0.00052, regularization loss: 0.29019, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 14:53:40.478119: Epoch [142/1000] [ 80/183], total loss: 0.00024, regularization loss: 0.29019, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 14:53:50.734100: Epoch [142/1000] [100/183], total loss: 0.04378, regularization loss: 0.29019, contrastive loss: 0.04378, Loss positive: 0.04371, Loss negative: 0.00007
2018-10-22 14:54:00.983458: Epoch [142/1000] [120/183], total loss: 0.00184, regularization loss: 0.29019, contrastive loss: 0.00184, Loss positive: 0.00000, Loss negative: 0.00184
2018-10-22 14:54:11.213681: Epoch [142/1000] [140/183], total loss: 0.01856, regularization loss: 0.29019, contrastive loss: 0.01856, Loss positive: 0.01829, Loss negative: 0.00027
2018-10-22 14:54:21.454108: Epoch [142/1000] [160/183], total loss: 0.00124, regularization loss: 0.29019, contrastive loss: 0.00124, Loss positive: 0.00000, Loss negative: 0.00124
2018-10-22 14:54:31.798582: Epoch [142/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:54:54.279924: Epoch [143/1000] [ 20/183], total loss: 0.00832, regularization loss: 0.29019, contrastive loss: 0.00832, Loss positive: 0.00000, Loss negative: 0.00832
2018-10-22 14:55:04.436196: Epoch [143/1000] [ 40/183], total loss: 0.00184, regularization loss: 0.29019, contrastive loss: 0.00184, Loss positive: 0.00000, Loss negative: 0.00184
2018-10-22 14:55:14.643961: Epoch [143/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:55:24.823731: Epoch [143/1000] [ 80/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 14:55:35.064732: Epoch [143/1000] [100/183], total loss: 0.00343, regularization loss: 0.29019, contrastive loss: 0.00343, Loss positive: 0.00000, Loss negative: 0.00343
2018-10-22 14:55:45.291960: Epoch [143/1000] [120/183], total loss: 0.00088, regularization loss: 0.29019, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 14:55:55.791738: Epoch [143/1000] [140/183], total loss: 0.00131, regularization loss: 0.29019, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 14:56:06.008964: Epoch [143/1000] [160/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 14:56:16.266867: Epoch [143/1000] [180/183], total loss: 0.00023, regularization loss: 0.29019, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 14:56:37.709401: Epoch [144/1000] [ 20/183], total loss: 0.00341, regularization loss: 0.29019, contrastive loss: 0.00341, Loss positive: 0.00000, Loss negative: 0.00341
2018-10-22 14:56:47.869699: Epoch [144/1000] [ 40/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 14:56:58.051797: Epoch [144/1000] [ 60/183], total loss: 0.00838, regularization loss: 0.29019, contrastive loss: 0.00838, Loss positive: 0.00755, Loss negative: 0.00083
2018-10-22 14:57:08.259968: Epoch [144/1000] [ 80/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 14:57:18.504771: Epoch [144/1000] [100/183], total loss: 0.00587, regularization loss: 0.29019, contrastive loss: 0.00587, Loss positive: 0.00000, Loss negative: 0.00587
2018-10-22 14:57:28.752549: Epoch [144/1000] [120/183], total loss: 0.00652, regularization loss: 0.29019, contrastive loss: 0.00652, Loss positive: 0.00000, Loss negative: 0.00652
2018-10-22 14:57:38.977379: Epoch [144/1000] [140/183], total loss: 0.00207, regularization loss: 0.29019, contrastive loss: 0.00207, Loss positive: 0.00000, Loss negative: 0.00207
2018-10-22 14:57:49.215992: Epoch [144/1000] [160/183], total loss: 0.00693, regularization loss: 0.29019, contrastive loss: 0.00693, Loss positive: 0.00000, Loss negative: 0.00693
2018-10-22 14:57:59.458787: Epoch [144/1000] [180/183], total loss: 0.00123, regularization loss: 0.29019, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 14:58:20.966833: Epoch [145/1000] [ 20/183], total loss: 0.00501, regularization loss: 0.29019, contrastive loss: 0.00501, Loss positive: 0.00000, Loss negative: 0.00501
2018-10-22 14:58:31.177946: Epoch [145/1000] [ 40/183], total loss: 0.00053, regularization loss: 0.29019, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 14:58:41.377448: Epoch [145/1000] [ 60/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 14:58:51.594408: Epoch [145/1000] [ 80/183], total loss: 0.01292, regularization loss: 0.29019, contrastive loss: 0.01292, Loss positive: 0.01220, Loss negative: 0.00071
2018-10-22 14:59:01.823834: Epoch [145/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:59:12.093776: Epoch [145/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 14:59:22.331460: Epoch [145/1000] [140/183], total loss: 0.01804, regularization loss: 0.29019, contrastive loss: 0.01804, Loss positive: 0.01802, Loss negative: 0.00002
2018-10-22 14:59:32.560985: Epoch [145/1000] [160/183], total loss: 0.00048, regularization loss: 0.29019, contrastive loss: 0.00048, Loss positive: 0.00000, Loss negative: 0.00048
2018-10-22 14:59:42.807666: Epoch [145/1000] [180/183], total loss: 0.03024, regularization loss: 0.29019, contrastive loss: 0.03024, Loss positive: 0.02428, Loss negative: 0.00597
2018-10-22 15:00:04.253992: Epoch [146/1000] [ 20/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 15:00:14.457459: Epoch [146/1000] [ 40/183], total loss: 0.02104, regularization loss: 0.29019, contrastive loss: 0.02104, Loss positive: 0.01816, Loss negative: 0.00288
2018-10-22 15:00:24.652058: Epoch [146/1000] [ 60/183], total loss: 0.02555, regularization loss: 0.29019, contrastive loss: 0.02555, Loss positive: 0.02555, Loss negative: 0.00000
2018-10-22 15:00:34.968714: Epoch [146/1000] [ 80/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 15:00:45.324201: Epoch [146/1000] [100/183], total loss: 0.00239, regularization loss: 0.29019, contrastive loss: 0.00239, Loss positive: 0.00000, Loss negative: 0.00239
2018-10-22 15:00:55.535469: Epoch [146/1000] [120/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 15:01:05.745592: Epoch [146/1000] [140/183], total loss: 0.00494, regularization loss: 0.29019, contrastive loss: 0.00494, Loss positive: 0.00000, Loss negative: 0.00494
2018-10-22 15:01:16.000407: Epoch [146/1000] [160/183], total loss: 0.00354, regularization loss: 0.29019, contrastive loss: 0.00354, Loss positive: 0.00000, Loss negative: 0.00354
2018-10-22 15:01:26.326011: Epoch [146/1000] [180/183], total loss: 0.00177, regularization loss: 0.29019, contrastive loss: 0.00177, Loss positive: 0.00000, Loss negative: 0.00177
2018-10-22 15:01:50.770354: Epoch [147/1000] [ 20/183], total loss: 0.00129, regularization loss: 0.29019, contrastive loss: 0.00129, Loss positive: 0.00000, Loss negative: 0.00129
2018-10-22 15:02:01.006508: Epoch [147/1000] [ 40/183], total loss: 0.00324, regularization loss: 0.29019, contrastive loss: 0.00324, Loss positive: 0.00000, Loss negative: 0.00324
2018-10-22 15:02:11.209228: Epoch [147/1000] [ 60/183], total loss: 0.00129, regularization loss: 0.29019, contrastive loss: 0.00129, Loss positive: 0.00000, Loss negative: 0.00129
2018-10-22 15:02:21.454089: Epoch [147/1000] [ 80/183], total loss: 0.00561, regularization loss: 0.29019, contrastive loss: 0.00561, Loss positive: 0.00000, Loss negative: 0.00561
2018-10-22 15:02:31.997793: Epoch [147/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:02:42.353473: Epoch [147/1000] [120/183], total loss: 0.00693, regularization loss: 0.29019, contrastive loss: 0.00693, Loss positive: 0.00693, Loss negative: 0.00000
2018-10-22 15:02:52.723420: Epoch [147/1000] [140/183], total loss: 0.00110, regularization loss: 0.29019, contrastive loss: 0.00110, Loss positive: 0.00000, Loss negative: 0.00110
2018-10-22 15:03:03.008395: Epoch [147/1000] [160/183], total loss: 0.00014, regularization loss: 0.29019, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 15:03:13.458997: Epoch [147/1000] [180/183], total loss: 0.01368, regularization loss: 0.29019, contrastive loss: 0.01368, Loss positive: 0.01366, Loss negative: 0.00002
2018-10-22 15:03:37.803263: Epoch [148/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:03:48.016507: Epoch [148/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:03:58.268516: Epoch [148/1000] [ 60/183], total loss: 0.00317, regularization loss: 0.29019, contrastive loss: 0.00317, Loss positive: 0.00000, Loss negative: 0.00317
2018-10-22 15:04:08.508617: Epoch [148/1000] [ 80/183], total loss: 0.00142, regularization loss: 0.29019, contrastive loss: 0.00142, Loss positive: 0.00000, Loss negative: 0.00142
2018-10-22 15:04:18.785732: Epoch [148/1000] [100/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 15:04:29.063166: Epoch [148/1000] [120/183], total loss: 0.01473, regularization loss: 0.29019, contrastive loss: 0.01473, Loss positive: 0.01334, Loss negative: 0.00139
2018-10-22 15:04:39.468997: Epoch [148/1000] [140/183], total loss: 0.01760, regularization loss: 0.29019, contrastive loss: 0.01760, Loss positive: 0.01760, Loss negative: 0.00000
2018-10-22 15:04:49.918130: Epoch [148/1000] [160/183], total loss: 0.01174, regularization loss: 0.29019, contrastive loss: 0.01174, Loss positive: 0.01130, Loss negative: 0.00044
2018-10-22 15:05:00.373683: Epoch [148/1000] [180/183], total loss: 0.01354, regularization loss: 0.29019, contrastive loss: 0.01354, Loss positive: 0.01318, Loss negative: 0.00035
2018-10-22 15:05:24.816381: Epoch [149/1000] [ 20/183], total loss: 0.00061, regularization loss: 0.29019, contrastive loss: 0.00061, Loss positive: 0.00000, Loss negative: 0.00061
2018-10-22 15:05:35.038298: Epoch [149/1000] [ 40/183], total loss: 0.01877, regularization loss: 0.29019, contrastive loss: 0.01877, Loss positive: 0.01877, Loss negative: 0.00000
2018-10-22 15:05:45.223504: Epoch [149/1000] [ 60/183], total loss: 0.00395, regularization loss: 0.29019, contrastive loss: 0.00395, Loss positive: 0.00000, Loss negative: 0.00395
2018-10-22 15:05:55.422540: Epoch [149/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 15:06:05.689357: Epoch [149/1000] [100/183], total loss: 0.00139, regularization loss: 0.29019, contrastive loss: 0.00139, Loss positive: 0.00000, Loss negative: 0.00139
2018-10-22 15:06:15.933952: Epoch [149/1000] [120/183], total loss: 0.00136, regularization loss: 0.29019, contrastive loss: 0.00136, Loss positive: 0.00000, Loss negative: 0.00136
2018-10-22 15:06:26.340896: Epoch [149/1000] [140/183], total loss: 0.00023, regularization loss: 0.29019, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 15:06:36.843258: Epoch [149/1000] [160/183], total loss: 0.00099, regularization loss: 0.29019, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 15:06:47.259475: Epoch [149/1000] [180/183], total loss: 0.00270, regularization loss: 0.29019, contrastive loss: 0.00270, Loss positive: 0.00000, Loss negative: 0.00270
2018-10-22 15:07:09.761645: Epoch [150/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:07:19.974322: Epoch [150/1000] [ 40/183], total loss: 0.00754, regularization loss: 0.29019, contrastive loss: 0.00754, Loss positive: 0.00000, Loss negative: 0.00754
2018-10-22 15:07:30.183208: Epoch [150/1000] [ 60/183], total loss: 0.00042, regularization loss: 0.29019, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 15:07:40.410475: Epoch [150/1000] [ 80/183], total loss: 0.00202, regularization loss: 0.29019, contrastive loss: 0.00202, Loss positive: 0.00000, Loss negative: 0.00202
2018-10-22 15:07:50.641496: Epoch [150/1000] [100/183], total loss: 0.01576, regularization loss: 0.29019, contrastive loss: 0.01576, Loss positive: 0.01414, Loss negative: 0.00162
2018-10-22 15:08:00.917172: Epoch [150/1000] [120/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 15:08:11.249299: Epoch [150/1000] [140/183], total loss: 0.00064, regularization loss: 0.29019, contrastive loss: 0.00064, Loss positive: 0.00000, Loss negative: 0.00064
2018-10-22 15:08:21.551793: Epoch [150/1000] [160/183], total loss: 0.00112, regularization loss: 0.29019, contrastive loss: 0.00112, Loss positive: 0.00000, Loss negative: 0.00112
2018-10-22 15:08:31.897550: Epoch [150/1000] [180/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
Recall@1: 0.25760
Recall@2: 0.37441
Recall@4: 0.50844
Recall@8: 0.64534
Recall@16: 0.76958
Recall@32: 0.86816
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 69.  56.  57.  55.  68.  51.  54.  51.  44.  57. 108.  48.  47.  42.
  72.  69.  62.  71.  42.  69.  50.  88.  38.  75.  97.  53.  65.  77.
  83.  71.  60.  46.  52.  46.  46.  66.  65.  55.  78.  65.  63.  59.
  84.  76.  34.  58.  60.  54.  53.  66.  65.  78.  48.  37.  37.  40.
  72.  75.  36.  87.  57.  68.  42.  77.  86.  75.  55.  23.  88.  30.
  50.  65.  68.  52.  52.  83.  47.  55.  43.  55.  18.  65.  65.  71.
  63.  39.  66.  62.  78.  54.  73.  52.  52.  61.  50.  59.  54.  28.
  42.  51.]
Purity is 0.234
count_cross = [[5. 0. 0. ... 0. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 1. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 1. 0.]
 [0. 0. 0. ... 1. 0. 1.]]
Mutual information is 2.06838
5924.0
5924
Entropy cluster is 4.56815
Entropy class is 4.60444
normalized_mutual_information is 0.45099
tp_and_fp = 185154.0
tp = 20419.0
fp is 164735.0
fn is 152331.0
RI is 0.9819273063509274
Precision is 0.11028117134925522
Recall is 0.11819971056439942
F_1 is 0.11410322321069337

normalized_mutual_information = 0.4509913891985015
RI = 0.9819273063509274
F_1 = 0.11410322321069337

The NN is 0.25760
The FT is 0.13862
The ST is 0.21631
The DCG is 0.52296
The E is 0.11696
The MAP 0.11212

2018-10-22 15:10:02.133945: Epoch [151/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:10:12.169497: Epoch [151/1000] [ 40/183], total loss: 0.00077, regularization loss: 0.29019, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 15:10:22.252039: Epoch [151/1000] [ 60/183], total loss: 0.00293, regularization loss: 0.29019, contrastive loss: 0.00293, Loss positive: 0.00000, Loss negative: 0.00293
2018-10-22 15:10:32.357001: Epoch [151/1000] [ 80/183], total loss: 0.02230, regularization loss: 0.29019, contrastive loss: 0.02230, Loss positive: 0.01777, Loss negative: 0.00453
2018-10-22 15:10:42.527159: Epoch [151/1000] [100/183], total loss: 0.02635, regularization loss: 0.29019, contrastive loss: 0.02635, Loss positive: 0.02635, Loss negative: 0.00000
2018-10-22 15:10:52.714301: Epoch [151/1000] [120/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:11:02.949948: Epoch [151/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:11:13.185410: Epoch [151/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:11:23.428562: Epoch [151/1000] [180/183], total loss: 0.00196, regularization loss: 0.29019, contrastive loss: 0.00196, Loss positive: 0.00000, Loss negative: 0.00196
2018-10-22 15:11:46.113080: Epoch [152/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:11:56.307269: Epoch [152/1000] [ 40/183], total loss: 0.00531, regularization loss: 0.29019, contrastive loss: 0.00531, Loss positive: 0.00000, Loss negative: 0.00531
2018-10-22 15:12:06.539782: Epoch [152/1000] [ 60/183], total loss: 0.00476, regularization loss: 0.29019, contrastive loss: 0.00476, Loss positive: 0.00000, Loss negative: 0.00476
2018-10-22 15:12:16.789586: Epoch [152/1000] [ 80/183], total loss: 0.02491, regularization loss: 0.29019, contrastive loss: 0.02491, Loss positive: 0.02310, Loss negative: 0.00181
2018-10-22 15:12:26.997008: Epoch [152/1000] [100/183], total loss: 0.00316, regularization loss: 0.29019, contrastive loss: 0.00316, Loss positive: 0.00000, Loss negative: 0.00316
2018-10-22 15:12:37.302289: Epoch [152/1000] [120/183], total loss: 0.00688, regularization loss: 0.29019, contrastive loss: 0.00688, Loss positive: 0.00000, Loss negative: 0.00688
2018-10-22 15:12:47.746331: Epoch [152/1000] [140/183], total loss: 0.01921, regularization loss: 0.29019, contrastive loss: 0.01921, Loss positive: 0.01853, Loss negative: 0.00068
2018-10-22 15:12:58.357182: Epoch [152/1000] [160/183], total loss: 0.05569, regularization loss: 0.29019, contrastive loss: 0.05569, Loss positive: 0.04829, Loss negative: 0.00740
2018-10-22 15:13:08.694302: Epoch [152/1000] [180/183], total loss: 0.00409, regularization loss: 0.29019, contrastive loss: 0.00409, Loss positive: 0.00000, Loss negative: 0.00409
2018-10-22 15:13:33.188205: Epoch [153/1000] [ 20/183], total loss: 0.02182, regularization loss: 0.29019, contrastive loss: 0.02182, Loss positive: 0.01748, Loss negative: 0.00434
2018-10-22 15:13:43.424158: Epoch [153/1000] [ 40/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 15:13:53.653927: Epoch [153/1000] [ 60/183], total loss: 0.05635, regularization loss: 0.29019, contrastive loss: 0.05635, Loss positive: 0.05396, Loss negative: 0.00239
2018-10-22 15:14:03.891540: Epoch [153/1000] [ 80/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 15:14:14.236791: Epoch [153/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:14:24.700534: Epoch [153/1000] [120/183], total loss: 0.00172, regularization loss: 0.29019, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 15:14:35.300024: Epoch [153/1000] [140/183], total loss: 0.00399, regularization loss: 0.29019, contrastive loss: 0.00399, Loss positive: 0.00000, Loss negative: 0.00399
2018-10-22 15:14:45.717159: Epoch [153/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:14:56.322126: Epoch [153/1000] [180/183], total loss: 0.01424, regularization loss: 0.29019, contrastive loss: 0.01424, Loss positive: 0.01000, Loss negative: 0.00424
2018-10-22 15:15:19.930964: Epoch [154/1000] [ 20/183], total loss: 0.00093, regularization loss: 0.29019, contrastive loss: 0.00093, Loss positive: 0.00000, Loss negative: 0.00093
2018-10-22 15:15:30.053150: Epoch [154/1000] [ 40/183], total loss: 0.00228, regularization loss: 0.29019, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 15:15:40.260193: Epoch [154/1000] [ 60/183], total loss: 0.00129, regularization loss: 0.29019, contrastive loss: 0.00129, Loss positive: 0.00000, Loss negative: 0.00129
2018-10-22 15:15:50.421910: Epoch [154/1000] [ 80/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 15:16:00.739509: Epoch [154/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:16:11.013035: Epoch [154/1000] [120/183], total loss: 0.00627, regularization loss: 0.29019, contrastive loss: 0.00627, Loss positive: 0.00000, Loss negative: 0.00627
2018-10-22 15:16:21.238064: Epoch [154/1000] [140/183], total loss: 0.00073, regularization loss: 0.29019, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 15:16:31.495558: Epoch [154/1000] [160/183], total loss: 0.00298, regularization loss: 0.29019, contrastive loss: 0.00298, Loss positive: 0.00000, Loss negative: 0.00298
2018-10-22 15:16:41.709616: Epoch [154/1000] [180/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:17:03.148946: Epoch [155/1000] [ 20/183], total loss: 0.00150, regularization loss: 0.29019, contrastive loss: 0.00150, Loss positive: 0.00000, Loss negative: 0.00150
2018-10-22 15:17:13.358995: Epoch [155/1000] [ 40/183], total loss: 0.00080, regularization loss: 0.29019, contrastive loss: 0.00080, Loss positive: 0.00000, Loss negative: 0.00080
2018-10-22 15:17:23.643282: Epoch [155/1000] [ 60/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 15:17:33.928302: Epoch [155/1000] [ 80/183], total loss: 0.00154, regularization loss: 0.29019, contrastive loss: 0.00154, Loss positive: 0.00000, Loss negative: 0.00154
2018-10-22 15:17:44.160528: Epoch [155/1000] [100/183], total loss: 0.00026, regularization loss: 0.29019, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 15:17:54.398160: Epoch [155/1000] [120/183], total loss: 0.00513, regularization loss: 0.29019, contrastive loss: 0.00513, Loss positive: 0.00000, Loss negative: 0.00513
2018-10-22 15:18:04.634180: Epoch [155/1000] [140/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 15:18:14.871136: Epoch [155/1000] [160/183], total loss: 0.00720, regularization loss: 0.29019, contrastive loss: 0.00720, Loss positive: 0.00663, Loss negative: 0.00057
2018-10-22 15:18:25.118838: Epoch [155/1000] [180/183], total loss: 0.00172, regularization loss: 0.29019, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 15:18:46.517878: Epoch [156/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:18:56.727253: Epoch [156/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:19:06.901409: Epoch [156/1000] [ 60/183], total loss: 0.00114, regularization loss: 0.29019, contrastive loss: 0.00114, Loss positive: 0.00000, Loss negative: 0.00114
2018-10-22 15:19:17.234329: Epoch [156/1000] [ 80/183], total loss: 0.00077, regularization loss: 0.29019, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 15:19:27.477308: Epoch [156/1000] [100/183], total loss: 0.00284, regularization loss: 0.29019, contrastive loss: 0.00284, Loss positive: 0.00000, Loss negative: 0.00284
2018-10-22 15:19:37.728838: Epoch [156/1000] [120/183], total loss: 0.00284, regularization loss: 0.29019, contrastive loss: 0.00284, Loss positive: 0.00000, Loss negative: 0.00284
2018-10-22 15:19:47.960856: Epoch [156/1000] [140/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 15:19:58.219486: Epoch [156/1000] [160/183], total loss: 0.01797, regularization loss: 0.29019, contrastive loss: 0.01797, Loss positive: 0.01641, Loss negative: 0.00156
2018-10-22 15:20:08.460689: Epoch [156/1000] [180/183], total loss: 0.00350, regularization loss: 0.29019, contrastive loss: 0.00350, Loss positive: 0.00000, Loss negative: 0.00350
2018-10-22 15:20:29.907693: Epoch [157/1000] [ 20/183], total loss: 0.03242, regularization loss: 0.29019, contrastive loss: 0.03242, Loss positive: 0.03242, Loss negative: 0.00000
2018-10-22 15:20:40.121085: Epoch [157/1000] [ 40/183], total loss: 0.00208, regularization loss: 0.29019, contrastive loss: 0.00208, Loss positive: 0.00000, Loss negative: 0.00208
2018-10-22 15:20:50.323417: Epoch [157/1000] [ 60/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 15:21:00.556616: Epoch [157/1000] [ 80/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 15:21:10.810595: Epoch [157/1000] [100/183], total loss: 0.02101, regularization loss: 0.29019, contrastive loss: 0.02101, Loss positive: 0.01913, Loss negative: 0.00188
2018-10-22 15:21:21.071302: Epoch [157/1000] [120/183], total loss: 0.00420, regularization loss: 0.29019, contrastive loss: 0.00420, Loss positive: 0.00000, Loss negative: 0.00420
2018-10-22 15:21:31.505683: Epoch [157/1000] [140/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 15:21:41.770862: Epoch [157/1000] [160/183], total loss: 0.00077, regularization loss: 0.29019, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 15:21:52.199161: Epoch [157/1000] [180/183], total loss: 0.02240, regularization loss: 0.29019, contrastive loss: 0.02240, Loss positive: 0.02211, Loss negative: 0.00030
2018-10-22 15:22:15.104139: Epoch [158/1000] [ 20/183], total loss: 0.00070, regularization loss: 0.29019, contrastive loss: 0.00070, Loss positive: 0.00000, Loss negative: 0.00070
2018-10-22 15:22:25.266812: Epoch [158/1000] [ 40/183], total loss: 0.02049, regularization loss: 0.29019, contrastive loss: 0.02049, Loss positive: 0.01795, Loss negative: 0.00254
2018-10-22 15:22:35.535316: Epoch [158/1000] [ 60/183], total loss: 0.04756, regularization loss: 0.29019, contrastive loss: 0.04756, Loss positive: 0.04598, Loss negative: 0.00158
2018-10-22 15:22:45.826877: Epoch [158/1000] [ 80/183], total loss: 0.01233, regularization loss: 0.29019, contrastive loss: 0.01233, Loss positive: 0.01219, Loss negative: 0.00015
2018-10-22 15:22:56.123795: Epoch [158/1000] [100/183], total loss: 0.00099, regularization loss: 0.29019, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 15:23:06.523211: Epoch [158/1000] [120/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 15:23:16.990558: Epoch [158/1000] [140/183], total loss: 0.02583, regularization loss: 0.29019, contrastive loss: 0.02583, Loss positive: 0.01854, Loss negative: 0.00729
2018-10-22 15:23:27.459210: Epoch [158/1000] [160/183], total loss: 0.00135, regularization loss: 0.29019, contrastive loss: 0.00135, Loss positive: 0.00000, Loss negative: 0.00135
2018-10-22 15:23:37.890156: Epoch [158/1000] [180/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 15:24:02.420811: Epoch [159/1000] [ 20/183], total loss: 0.03087, regularization loss: 0.29019, contrastive loss: 0.03087, Loss positive: 0.02300, Loss negative: 0.00787
2018-10-22 15:24:12.645422: Epoch [159/1000] [ 40/183], total loss: 0.01494, regularization loss: 0.29019, contrastive loss: 0.01494, Loss positive: 0.01469, Loss negative: 0.00025
2018-10-22 15:24:22.884594: Epoch [159/1000] [ 60/183], total loss: 0.00461, regularization loss: 0.29019, contrastive loss: 0.00461, Loss positive: 0.00000, Loss negative: 0.00461
2018-10-22 15:24:33.103440: Epoch [159/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:24:43.437157: Epoch [159/1000] [100/183], total loss: 0.00541, regularization loss: 0.29019, contrastive loss: 0.00541, Loss positive: 0.00000, Loss negative: 0.00541
2018-10-22 15:24:53.841829: Epoch [159/1000] [120/183], total loss: 0.00064, regularization loss: 0.29019, contrastive loss: 0.00064, Loss positive: 0.00000, Loss negative: 0.00064
2018-10-22 15:25:04.171589: Epoch [159/1000] [140/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 15:25:14.537802: Epoch [159/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:25:24.793737: Epoch [159/1000] [180/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 15:25:47.294856: Epoch [160/1000] [ 20/183], total loss: 0.05938, regularization loss: 0.29019, contrastive loss: 0.05938, Loss positive: 0.05938, Loss negative: 0.00000
2018-10-22 15:25:57.501418: Epoch [160/1000] [ 40/183], total loss: 0.00010, regularization loss: 0.29019, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 15:26:07.693422: Epoch [160/1000] [ 60/183], total loss: 0.01079, regularization loss: 0.29019, contrastive loss: 0.01079, Loss positive: 0.00793, Loss negative: 0.00287
2018-10-22 15:26:17.904250: Epoch [160/1000] [ 80/183], total loss: 0.00550, regularization loss: 0.29019, contrastive loss: 0.00550, Loss positive: 0.00000, Loss negative: 0.00550
2018-10-22 15:26:28.161161: Epoch [160/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:26:38.399585: Epoch [160/1000] [120/183], total loss: 0.00844, regularization loss: 0.29019, contrastive loss: 0.00844, Loss positive: 0.00000, Loss negative: 0.00844
2018-10-22 15:26:48.638034: Epoch [160/1000] [140/183], total loss: 0.00186, regularization loss: 0.29019, contrastive loss: 0.00186, Loss positive: 0.00000, Loss negative: 0.00186
2018-10-22 15:26:58.870220: Epoch [160/1000] [160/183], total loss: 0.00115, regularization loss: 0.29019, contrastive loss: 0.00115, Loss positive: 0.00000, Loss negative: 0.00115
2018-10-22 15:27:09.136122: Epoch [160/1000] [180/183], total loss: 0.00114, regularization loss: 0.29019, contrastive loss: 0.00114, Loss positive: 0.00000, Loss negative: 0.00114
Recall@1: 0.24865
Recall@2: 0.35533
Recall@4: 0.49139
Recall@8: 0.63690
Recall@16: 0.76131
Recall@32: 0.86546
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [88. 79. 61. 69. 47. 55. 74. 60. 75. 54. 55. 66. 76. 84. 39. 51. 35. 88.
 73. 46. 53. 76. 83. 51. 51. 39. 87. 72. 68. 62. 39. 45. 77. 99. 92. 33.
 50. 46. 72. 32. 79. 58. 80. 51. 39. 20. 43. 54. 56. 53. 70. 67. 62. 32.
 63. 64. 26. 81. 28. 38. 55. 22. 68. 32. 71. 74. 68. 61. 80. 63. 54. 88.
 79. 57. 48. 51. 56. 63. 83. 93. 50. 46. 15. 57. 55. 51. 65. 63. 82. 51.
 38. 58. 39. 75. 49. 83. 83. 45. 56. 31.]
Purity is 0.224
count_cross = [[0. 2. 7. ... 1. 0. 0.]
 [0. 1. 3. ... 1. 0. 0.]
 [0. 0. 0. ... 0. 1. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 1. 0.]]
Mutual information is 2.00748
5924.0
5924
Entropy cluster is 4.55567
Entropy class is 4.60444
normalized_mutual_information is 0.43831
tp_and_fp = 188907.0
tp = 18550.0
fp is 170357.0
fn is 154200.0
RI is 0.9815003209657861
Precision is 0.09819646704463042
Recall is 0.10738060781476122
F_1 is 0.10258338702140427

normalized_mutual_information = 0.43830855532329666
RI = 0.9815003209657861
F_1 = 0.10258338702140427

The NN is 0.24865
The FT is 0.13256
The ST is 0.20558
The DCG is 0.51531
The E is 0.11220
The MAP 0.10505

2018-10-22 15:28:38.644266: Epoch [161/1000] [ 20/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 15:28:48.743540: Epoch [161/1000] [ 40/183], total loss: 0.00137, regularization loss: 0.29019, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 15:28:58.838895: Epoch [161/1000] [ 60/183], total loss: 0.00206, regularization loss: 0.29019, contrastive loss: 0.00206, Loss positive: 0.00000, Loss negative: 0.00206
2018-10-22 15:29:08.961555: Epoch [161/1000] [ 80/183], total loss: 0.02399, regularization loss: 0.29019, contrastive loss: 0.02399, Loss positive: 0.02398, Loss negative: 0.00000
2018-10-22 15:29:19.094280: Epoch [161/1000] [100/183], total loss: 0.00126, regularization loss: 0.29019, contrastive loss: 0.00126, Loss positive: 0.00000, Loss negative: 0.00126
2018-10-22 15:29:29.264568: Epoch [161/1000] [120/183], total loss: 0.01204, regularization loss: 0.29019, contrastive loss: 0.01204, Loss positive: 0.00628, Loss negative: 0.00575
2018-10-22 15:29:39.453503: Epoch [161/1000] [140/183], total loss: 0.00116, regularization loss: 0.29019, contrastive loss: 0.00116, Loss positive: 0.00000, Loss negative: 0.00116
2018-10-22 15:29:49.677346: Epoch [161/1000] [160/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 15:29:59.909200: Epoch [161/1000] [180/183], total loss: 0.00268, regularization loss: 0.29019, contrastive loss: 0.00268, Loss positive: 0.00000, Loss negative: 0.00268
2018-10-22 15:30:21.406508: Epoch [162/1000] [ 20/183], total loss: 0.00278, regularization loss: 0.29019, contrastive loss: 0.00278, Loss positive: 0.00000, Loss negative: 0.00278
2018-10-22 15:30:31.544636: Epoch [162/1000] [ 40/183], total loss: 0.02617, regularization loss: 0.29019, contrastive loss: 0.02617, Loss positive: 0.02274, Loss negative: 0.00343
2018-10-22 15:30:41.719288: Epoch [162/1000] [ 60/183], total loss: 0.05027, regularization loss: 0.29019, contrastive loss: 0.05027, Loss positive: 0.04891, Loss negative: 0.00136
2018-10-22 15:30:51.938235: Epoch [162/1000] [ 80/183], total loss: 0.01835, regularization loss: 0.29019, contrastive loss: 0.01835, Loss positive: 0.01835, Loss negative: 0.00000
2018-10-22 15:31:02.168845: Epoch [162/1000] [100/183], total loss: 0.01486, regularization loss: 0.29019, contrastive loss: 0.01486, Loss positive: 0.01126, Loss negative: 0.00360
2018-10-22 15:31:12.442573: Epoch [162/1000] [120/183], total loss: 0.00931, regularization loss: 0.29019, contrastive loss: 0.00931, Loss positive: 0.00785, Loss negative: 0.00145
2018-10-22 15:31:22.697339: Epoch [162/1000] [140/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 15:31:33.125429: Epoch [162/1000] [160/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 15:31:43.381292: Epoch [162/1000] [180/183], total loss: 0.00505, regularization loss: 0.29019, contrastive loss: 0.00505, Loss positive: 0.00000, Loss negative: 0.00505
2018-10-22 15:32:06.458899: Epoch [163/1000] [ 20/183], total loss: 0.00386, regularization loss: 0.29019, contrastive loss: 0.00386, Loss positive: 0.00000, Loss negative: 0.00386
2018-10-22 15:32:16.657782: Epoch [163/1000] [ 40/183], total loss: 0.00441, regularization loss: 0.29019, contrastive loss: 0.00441, Loss positive: 0.00000, Loss negative: 0.00441
2018-10-22 15:32:26.854971: Epoch [163/1000] [ 60/183], total loss: 0.03233, regularization loss: 0.29019, contrastive loss: 0.03233, Loss positive: 0.03229, Loss negative: 0.00004
2018-10-22 15:32:37.132569: Epoch [163/1000] [ 80/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 15:32:47.483660: Epoch [163/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:32:57.806300: Epoch [163/1000] [120/183], total loss: 0.00309, regularization loss: 0.29019, contrastive loss: 0.00309, Loss positive: 0.00000, Loss negative: 0.00309
2018-10-22 15:33:08.354581: Epoch [163/1000] [140/183], total loss: 0.00690, regularization loss: 0.29019, contrastive loss: 0.00690, Loss positive: 0.00000, Loss negative: 0.00690
2018-10-22 15:33:18.930230: Epoch [163/1000] [160/183], total loss: 0.02113, regularization loss: 0.29019, contrastive loss: 0.02113, Loss positive: 0.01790, Loss negative: 0.00323
2018-10-22 15:33:29.378291: Epoch [163/1000] [180/183], total loss: 0.00121, regularization loss: 0.29019, contrastive loss: 0.00121, Loss positive: 0.00000, Loss negative: 0.00121
2018-10-22 15:33:54.286289: Epoch [164/1000] [ 20/183], total loss: 0.00164, regularization loss: 0.29019, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 15:34:04.507743: Epoch [164/1000] [ 40/183], total loss: 0.00140, regularization loss: 0.29019, contrastive loss: 0.00140, Loss positive: 0.00000, Loss negative: 0.00140
2018-10-22 15:34:14.749633: Epoch [164/1000] [ 60/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 15:34:25.015225: Epoch [164/1000] [ 80/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 15:34:35.313165: Epoch [164/1000] [100/183], total loss: 0.00190, regularization loss: 0.29019, contrastive loss: 0.00190, Loss positive: 0.00000, Loss negative: 0.00190
2018-10-22 15:34:45.760129: Epoch [164/1000] [120/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 15:34:56.142960: Epoch [164/1000] [140/183], total loss: 0.00093, regularization loss: 0.29019, contrastive loss: 0.00093, Loss positive: 0.00000, Loss negative: 0.00093
2018-10-22 15:35:06.637831: Epoch [164/1000] [160/183], total loss: 0.00752, regularization loss: 0.29019, contrastive loss: 0.00752, Loss positive: 0.00000, Loss negative: 0.00752
2018-10-22 15:35:16.897609: Epoch [164/1000] [180/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 15:35:39.449096: Epoch [165/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:35:49.667519: Epoch [165/1000] [ 40/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:35:59.871269: Epoch [165/1000] [ 60/183], total loss: 0.00071, regularization loss: 0.29019, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 15:36:10.203676: Epoch [165/1000] [ 80/183], total loss: 0.02360, regularization loss: 0.29019, contrastive loss: 0.02360, Loss positive: 0.02172, Loss negative: 0.00188
2018-10-22 15:36:20.443741: Epoch [165/1000] [100/183], total loss: 0.00198, regularization loss: 0.29019, contrastive loss: 0.00198, Loss positive: 0.00000, Loss negative: 0.00198
2018-10-22 15:36:30.690938: Epoch [165/1000] [120/183], total loss: 0.00335, regularization loss: 0.29019, contrastive loss: 0.00335, Loss positive: 0.00000, Loss negative: 0.00335
2018-10-22 15:36:40.940779: Epoch [165/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:36:51.196845: Epoch [165/1000] [160/183], total loss: 0.00104, regularization loss: 0.29019, contrastive loss: 0.00104, Loss positive: 0.00000, Loss negative: 0.00104
2018-10-22 15:37:01.450329: Epoch [165/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:37:22.874258: Epoch [166/1000] [ 20/183], total loss: 0.02377, regularization loss: 0.29019, contrastive loss: 0.02377, Loss positive: 0.01968, Loss negative: 0.00408
2018-10-22 15:37:33.051086: Epoch [166/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:37:43.201399: Epoch [166/1000] [ 60/183], total loss: 0.00375, regularization loss: 0.29019, contrastive loss: 0.00375, Loss positive: 0.00000, Loss negative: 0.00375
2018-10-22 15:37:53.458790: Epoch [166/1000] [ 80/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 15:38:03.670800: Epoch [166/1000] [100/183], total loss: 0.00181, regularization loss: 0.29019, contrastive loss: 0.00181, Loss positive: 0.00000, Loss negative: 0.00181
2018-10-22 15:38:13.983091: Epoch [166/1000] [120/183], total loss: 0.00106, regularization loss: 0.29019, contrastive loss: 0.00106, Loss positive: 0.00000, Loss negative: 0.00106
2018-10-22 15:38:24.283446: Epoch [166/1000] [140/183], total loss: 0.02821, regularization loss: 0.29019, contrastive loss: 0.02821, Loss positive: 0.02815, Loss negative: 0.00005
2018-10-22 15:38:34.534338: Epoch [166/1000] [160/183], total loss: 0.00326, regularization loss: 0.29019, contrastive loss: 0.00326, Loss positive: 0.00000, Loss negative: 0.00326
2018-10-22 15:38:44.748237: Epoch [166/1000] [180/183], total loss: 0.00250, regularization loss: 0.29019, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 15:39:06.260942: Epoch [167/1000] [ 20/183], total loss: 0.00360, regularization loss: 0.29019, contrastive loss: 0.00360, Loss positive: 0.00000, Loss negative: 0.00360
2018-10-22 15:39:16.417620: Epoch [167/1000] [ 40/183], total loss: 0.00116, regularization loss: 0.29019, contrastive loss: 0.00116, Loss positive: 0.00000, Loss negative: 0.00116
2018-10-22 15:39:26.618997: Epoch [167/1000] [ 60/183], total loss: 0.05443, regularization loss: 0.29019, contrastive loss: 0.05443, Loss positive: 0.05416, Loss negative: 0.00027
2018-10-22 15:39:36.871792: Epoch [167/1000] [ 80/183], total loss: 0.00059, regularization loss: 0.29019, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 15:39:47.145297: Epoch [167/1000] [100/183], total loss: 0.00032, regularization loss: 0.29019, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 15:39:57.364177: Epoch [167/1000] [120/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 15:40:07.595765: Epoch [167/1000] [140/183], total loss: 0.02262, regularization loss: 0.29019, contrastive loss: 0.02262, Loss positive: 0.02262, Loss negative: 0.00000
2018-10-22 15:40:17.845663: Epoch [167/1000] [160/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 15:40:28.060067: Epoch [167/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:40:49.543696: Epoch [168/1000] [ 20/183], total loss: 0.00912, regularization loss: 0.29019, contrastive loss: 0.00912, Loss positive: 0.00869, Loss negative: 0.00043
2018-10-22 15:40:59.722796: Epoch [168/1000] [ 40/183], total loss: 0.01528, regularization loss: 0.29019, contrastive loss: 0.01528, Loss positive: 0.00414, Loss negative: 0.01115
2018-10-22 15:41:09.887151: Epoch [168/1000] [ 60/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 15:41:20.179812: Epoch [168/1000] [ 80/183], total loss: 0.00899, regularization loss: 0.29019, contrastive loss: 0.00899, Loss positive: 0.00000, Loss negative: 0.00899
2018-10-22 15:41:30.440219: Epoch [168/1000] [100/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 15:41:40.904016: Epoch [168/1000] [120/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 15:41:51.138547: Epoch [168/1000] [140/183], total loss: 0.00181, regularization loss: 0.29019, contrastive loss: 0.00181, Loss positive: 0.00000, Loss negative: 0.00181
2018-10-22 15:42:01.501345: Epoch [168/1000] [160/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 15:42:11.944600: Epoch [168/1000] [180/183], total loss: 0.04622, regularization loss: 0.29019, contrastive loss: 0.04622, Loss positive: 0.04520, Loss negative: 0.00102
2018-10-22 15:42:34.678002: Epoch [169/1000] [ 20/183], total loss: 0.00285, regularization loss: 0.29019, contrastive loss: 0.00285, Loss positive: 0.00000, Loss negative: 0.00285
2018-10-22 15:42:44.920668: Epoch [169/1000] [ 40/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 15:42:55.149843: Epoch [169/1000] [ 60/183], total loss: 0.01391, regularization loss: 0.29019, contrastive loss: 0.01391, Loss positive: 0.01389, Loss negative: 0.00002
2018-10-22 15:43:05.403852: Epoch [169/1000] [ 80/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:43:15.723852: Epoch [169/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:43:26.178253: Epoch [169/1000] [120/183], total loss: 0.00283, regularization loss: 0.29019, contrastive loss: 0.00283, Loss positive: 0.00000, Loss negative: 0.00283
2018-10-22 15:43:36.534993: Epoch [169/1000] [140/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 15:43:46.953867: Epoch [169/1000] [160/183], total loss: 0.00026, regularization loss: 0.29019, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 15:43:57.254396: Epoch [169/1000] [180/183], total loss: 0.05669, regularization loss: 0.29019, contrastive loss: 0.05669, Loss positive: 0.05277, Loss negative: 0.00392
2018-10-22 15:44:21.832548: Epoch [170/1000] [ 20/183], total loss: 0.00544, regularization loss: 0.29019, contrastive loss: 0.00544, Loss positive: 0.00000, Loss negative: 0.00544
2018-10-22 15:44:32.045106: Epoch [170/1000] [ 40/183], total loss: 0.01986, regularization loss: 0.29019, contrastive loss: 0.01986, Loss positive: 0.01558, Loss negative: 0.00428
2018-10-22 15:44:42.285102: Epoch [170/1000] [ 60/183], total loss: 0.01444, regularization loss: 0.29019, contrastive loss: 0.01444, Loss positive: 0.01316, Loss negative: 0.00128
2018-10-22 15:44:52.544713: Epoch [170/1000] [ 80/183], total loss: 0.05407, regularization loss: 0.29019, contrastive loss: 0.05407, Loss positive: 0.05104, Loss negative: 0.00303
2018-10-22 15:45:02.881802: Epoch [170/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:45:13.166072: Epoch [170/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:45:23.424630: Epoch [170/1000] [140/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 15:45:33.669523: Epoch [170/1000] [160/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:45:43.989541: Epoch [170/1000] [180/183], total loss: 0.02660, regularization loss: 0.29019, contrastive loss: 0.02660, Loss positive: 0.02324, Loss negative: 0.00336
Recall@1: 0.24679
Recall@2: 0.35820
Recall@4: 0.49595
Recall@8: 0.63049
Recall@16: 0.76047
Recall@32: 0.86344
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [68. 61. 74. 45. 82. 49. 81. 59. 74. 90. 74. 58. 33. 51. 43. 66. 72. 82.
 66. 41. 52. 46. 32. 50. 66. 82. 92. 74. 53. 61. 53. 82. 52. 52. 42. 56.
 50. 67. 34. 85. 58. 52. 61. 52. 32. 58. 46. 53. 58. 30. 73. 60. 65. 38.
 63. 51. 85. 44. 47. 66. 38. 55. 48. 29. 71. 41. 67. 55. 48. 92. 59. 66.
 42. 46. 38. 86. 72. 63. 48. 49. 58. 34. 87. 69. 70. 59. 75. 80. 82. 49.
 82. 60. 42. 56. 87. 65. 52. 65. 46. 51.]
Purity is 0.227
count_cross = [[5. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 0. 0.]
 [0. 0. 0. ... 0. 2. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 3. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 2.00826
5924.0
5924
Entropy cluster is 4.57000
Entropy class is 4.60444
normalized_mutual_information is 0.43779
tp_and_fp = 184746.0
tp = 18754.0
fp is 165992.0
fn is 153996.0
RI is 0.9817607529808322
Precision is 0.10151234668138959
Recall is 0.108561505065123
F_1 is 0.1049186564325195

normalized_mutual_information = 0.4377941480286556
RI = 0.9817607529808322
F_1 = 0.1049186564325195

The NN is 0.24679
The FT is 0.13233
The ST is 0.20652
The DCG is 0.51665
The E is 0.11194
The MAP 0.10578

2018-10-22 15:47:14.392276: Epoch [171/1000] [ 20/183], total loss: 0.03185, regularization loss: 0.29019, contrastive loss: 0.03185, Loss positive: 0.03107, Loss negative: 0.00079
2018-10-22 15:47:24.496174: Epoch [171/1000] [ 40/183], total loss: 0.00126, regularization loss: 0.29019, contrastive loss: 0.00126, Loss positive: 0.00000, Loss negative: 0.00126
2018-10-22 15:47:34.574554: Epoch [171/1000] [ 60/183], total loss: 0.01594, regularization loss: 0.29019, contrastive loss: 0.01594, Loss positive: 0.01594, Loss negative: 0.00000
2018-10-22 15:47:44.691468: Epoch [171/1000] [ 80/183], total loss: 0.04112, regularization loss: 0.29019, contrastive loss: 0.04112, Loss positive: 0.04041, Loss negative: 0.00071
2018-10-22 15:47:54.896432: Epoch [171/1000] [100/183], total loss: 0.05828, regularization loss: 0.29019, contrastive loss: 0.05828, Loss positive: 0.05688, Loss negative: 0.00140
2018-10-22 15:48:05.092435: Epoch [171/1000] [120/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 15:48:15.356899: Epoch [171/1000] [140/183], total loss: 0.00060, regularization loss: 0.29019, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 15:48:25.587852: Epoch [171/1000] [160/183], total loss: 0.01113, regularization loss: 0.29019, contrastive loss: 0.01113, Loss positive: 0.00000, Loss negative: 0.01113
2018-10-22 15:48:35.839430: Epoch [171/1000] [180/183], total loss: 0.05164, regularization loss: 0.29019, contrastive loss: 0.05164, Loss positive: 0.04423, Loss negative: 0.00741
2018-10-22 15:48:57.358211: Epoch [172/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:49:07.540766: Epoch [172/1000] [ 40/183], total loss: 0.00050, regularization loss: 0.29019, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 15:49:17.739821: Epoch [172/1000] [ 60/183], total loss: 0.00396, regularization loss: 0.29019, contrastive loss: 0.00396, Loss positive: 0.00000, Loss negative: 0.00396
2018-10-22 15:49:27.941727: Epoch [172/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:49:38.148961: Epoch [172/1000] [100/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 15:49:48.415117: Epoch [172/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:49:58.655014: Epoch [172/1000] [140/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 15:50:08.885228: Epoch [172/1000] [160/183], total loss: 0.01033, regularization loss: 0.29019, contrastive loss: 0.01033, Loss positive: 0.00000, Loss negative: 0.01033
2018-10-22 15:50:19.140942: Epoch [172/1000] [180/183], total loss: 0.01806, regularization loss: 0.29019, contrastive loss: 0.01806, Loss positive: 0.01697, Loss negative: 0.00109
2018-10-22 15:50:40.788545: Epoch [173/1000] [ 20/183], total loss: 0.02758, regularization loss: 0.29019, contrastive loss: 0.02758, Loss positive: 0.02746, Loss negative: 0.00012
2018-10-22 15:50:50.951449: Epoch [173/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:51:01.140413: Epoch [173/1000] [ 60/183], total loss: 0.03391, regularization loss: 0.29019, contrastive loss: 0.03391, Loss positive: 0.03382, Loss negative: 0.00008
2018-10-22 15:51:11.360338: Epoch [173/1000] [ 80/183], total loss: 0.00036, regularization loss: 0.29019, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 15:51:21.622487: Epoch [173/1000] [100/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 15:51:32.045280: Epoch [173/1000] [120/183], total loss: 0.00189, regularization loss: 0.29019, contrastive loss: 0.00189, Loss positive: 0.00000, Loss negative: 0.00189
2018-10-22 15:51:42.320375: Epoch [173/1000] [140/183], total loss: 0.00174, regularization loss: 0.29019, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 15:51:52.749897: Epoch [173/1000] [160/183], total loss: 0.00304, regularization loss: 0.29019, contrastive loss: 0.00304, Loss positive: 0.00000, Loss negative: 0.00304
2018-10-22 15:52:02.990037: Epoch [173/1000] [180/183], total loss: 0.00221, regularization loss: 0.29019, contrastive loss: 0.00221, Loss positive: 0.00000, Loss negative: 0.00221
2018-10-22 15:52:26.020059: Epoch [174/1000] [ 20/183], total loss: 0.00819, regularization loss: 0.29019, contrastive loss: 0.00819, Loss positive: 0.00000, Loss negative: 0.00819
2018-10-22 15:52:36.238627: Epoch [174/1000] [ 40/183], total loss: 0.00130, regularization loss: 0.29019, contrastive loss: 0.00130, Loss positive: 0.00000, Loss negative: 0.00130
2018-10-22 15:52:46.500034: Epoch [174/1000] [ 60/183], total loss: 0.00042, regularization loss: 0.29019, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 15:52:56.895030: Epoch [174/1000] [ 80/183], total loss: 0.00083, regularization loss: 0.29019, contrastive loss: 0.00083, Loss positive: 0.00000, Loss negative: 0.00083
2018-10-22 15:53:07.307830: Epoch [174/1000] [100/183], total loss: 0.01659, regularization loss: 0.29019, contrastive loss: 0.01659, Loss positive: 0.01658, Loss negative: 0.00000
2018-10-22 15:53:17.571908: Epoch [174/1000] [120/183], total loss: 0.00053, regularization loss: 0.29019, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 15:53:27.984546: Epoch [174/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:53:38.422644: Epoch [174/1000] [160/183], total loss: 0.02313, regularization loss: 0.29019, contrastive loss: 0.02313, Loss positive: 0.01178, Loss negative: 0.01135
2018-10-22 15:53:48.908855: Epoch [174/1000] [180/183], total loss: 0.03178, regularization loss: 0.29019, contrastive loss: 0.03178, Loss positive: 0.03090, Loss negative: 0.00088
2018-10-22 15:54:13.595187: Epoch [175/1000] [ 20/183], total loss: 0.00219, regularization loss: 0.29019, contrastive loss: 0.00219, Loss positive: 0.00000, Loss negative: 0.00219
2018-10-22 15:54:23.878128: Epoch [175/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:54:34.108572: Epoch [175/1000] [ 60/183], total loss: 0.01967, regularization loss: 0.29019, contrastive loss: 0.01967, Loss positive: 0.01795, Loss negative: 0.00172
2018-10-22 15:54:44.338212: Epoch [175/1000] [ 80/183], total loss: 0.00111, regularization loss: 0.29019, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 15:54:54.698128: Epoch [175/1000] [100/183], total loss: 0.00549, regularization loss: 0.29019, contrastive loss: 0.00549, Loss positive: 0.00000, Loss negative: 0.00549
2018-10-22 15:55:04.999456: Epoch [175/1000] [120/183], total loss: 0.00124, regularization loss: 0.29019, contrastive loss: 0.00124, Loss positive: 0.00000, Loss negative: 0.00124
2018-10-22 15:55:15.320715: Epoch [175/1000] [140/183], total loss: 0.02681, regularization loss: 0.29019, contrastive loss: 0.02681, Loss positive: 0.02618, Loss negative: 0.00063
2018-10-22 15:55:25.736900: Epoch [175/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:55:36.010239: Epoch [175/1000] [180/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 15:55:58.496043: Epoch [176/1000] [ 20/183], total loss: 0.00118, regularization loss: 0.29019, contrastive loss: 0.00118, Loss positive: 0.00000, Loss negative: 0.00118
2018-10-22 15:56:08.684302: Epoch [176/1000] [ 40/183], total loss: 0.01779, regularization loss: 0.29019, contrastive loss: 0.01779, Loss positive: 0.01560, Loss negative: 0.00218
2018-10-22 15:56:18.869554: Epoch [176/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:56:29.180784: Epoch [176/1000] [ 80/183], total loss: 0.02520, regularization loss: 0.29019, contrastive loss: 0.02520, Loss positive: 0.02341, Loss negative: 0.00179
2018-10-22 15:56:39.412406: Epoch [176/1000] [100/183], total loss: 0.00201, regularization loss: 0.29019, contrastive loss: 0.00201, Loss positive: 0.00000, Loss negative: 0.00201
2018-10-22 15:56:49.728600: Epoch [176/1000] [120/183], total loss: 0.00817, regularization loss: 0.29019, contrastive loss: 0.00817, Loss positive: 0.00000, Loss negative: 0.00817
2018-10-22 15:56:59.939339: Epoch [176/1000] [140/183], total loss: 0.01929, regularization loss: 0.29019, contrastive loss: 0.01929, Loss positive: 0.01711, Loss negative: 0.00219
2018-10-22 15:57:10.174347: Epoch [176/1000] [160/183], total loss: 0.00093, regularization loss: 0.29019, contrastive loss: 0.00093, Loss positive: 0.00000, Loss negative: 0.00093
2018-10-22 15:57:20.421230: Epoch [176/1000] [180/183], total loss: 0.00136, regularization loss: 0.29019, contrastive loss: 0.00136, Loss positive: 0.00000, Loss negative: 0.00136
2018-10-22 15:57:41.856124: Epoch [177/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:57:52.017909: Epoch [177/1000] [ 40/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 15:58:02.238384: Epoch [177/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 15:58:12.451101: Epoch [177/1000] [ 80/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 15:58:22.686317: Epoch [177/1000] [100/183], total loss: 0.00693, regularization loss: 0.29019, contrastive loss: 0.00693, Loss positive: 0.00575, Loss negative: 0.00119
2018-10-22 15:58:32.910358: Epoch [177/1000] [120/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 15:58:43.160623: Epoch [177/1000] [140/183], total loss: 0.02427, regularization loss: 0.29019, contrastive loss: 0.02427, Loss positive: 0.02397, Loss negative: 0.00030
2018-10-22 15:58:53.413174: Epoch [177/1000] [160/183], total loss: 0.00146, regularization loss: 0.29019, contrastive loss: 0.00146, Loss positive: 0.00000, Loss negative: 0.00146
2018-10-22 15:59:03.665320: Epoch [177/1000] [180/183], total loss: 0.00760, regularization loss: 0.29019, contrastive loss: 0.00760, Loss positive: 0.00000, Loss negative: 0.00760
2018-10-22 15:59:25.160991: Epoch [178/1000] [ 20/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 15:59:35.303697: Epoch [178/1000] [ 40/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 15:59:45.528130: Epoch [178/1000] [ 60/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 15:59:55.875169: Epoch [178/1000] [ 80/183], total loss: 0.02254, regularization loss: 0.29019, contrastive loss: 0.02254, Loss positive: 0.01898, Loss negative: 0.00356
2018-10-22 16:00:06.101235: Epoch [178/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:00:16.346680: Epoch [178/1000] [120/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 16:00:26.594400: Epoch [178/1000] [140/183], total loss: 0.00097, regularization loss: 0.29019, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 16:00:36.865357: Epoch [178/1000] [160/183], total loss: 0.00122, regularization loss: 0.29019, contrastive loss: 0.00122, Loss positive: 0.00000, Loss negative: 0.00122
2018-10-22 16:00:47.116658: Epoch [178/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:01:08.642338: Epoch [179/1000] [ 20/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 16:01:18.845618: Epoch [179/1000] [ 40/183], total loss: 0.06645, regularization loss: 0.29019, contrastive loss: 0.06645, Loss positive: 0.06518, Loss negative: 0.00127
2018-10-22 16:01:29.072412: Epoch [179/1000] [ 60/183], total loss: 0.00271, regularization loss: 0.29019, contrastive loss: 0.00271, Loss positive: 0.00000, Loss negative: 0.00271
2018-10-22 16:01:39.589188: Epoch [179/1000] [ 80/183], total loss: 0.00218, regularization loss: 0.29019, contrastive loss: 0.00218, Loss positive: 0.00000, Loss negative: 0.00218
2018-10-22 16:01:49.833893: Epoch [179/1000] [100/183], total loss: 0.00627, regularization loss: 0.29019, contrastive loss: 0.00627, Loss positive: 0.00627, Loss negative: 0.00000
2018-10-22 16:02:00.088134: Epoch [179/1000] [120/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 16:02:10.342295: Epoch [179/1000] [140/183], total loss: 0.00458, regularization loss: 0.29019, contrastive loss: 0.00458, Loss positive: 0.00000, Loss negative: 0.00458
2018-10-22 16:02:20.609550: Epoch [179/1000] [160/183], total loss: 0.00757, regularization loss: 0.29019, contrastive loss: 0.00757, Loss positive: 0.00688, Loss negative: 0.00070
2018-10-22 16:02:31.102948: Epoch [179/1000] [180/183], total loss: 0.03257, regularization loss: 0.29019, contrastive loss: 0.03257, Loss positive: 0.03221, Loss negative: 0.00037
2018-10-22 16:02:55.556917: Epoch [180/1000] [ 20/183], total loss: 0.00078, regularization loss: 0.29019, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 16:03:05.788277: Epoch [180/1000] [ 40/183], total loss: 0.03013, regularization loss: 0.29019, contrastive loss: 0.03013, Loss positive: 0.02684, Loss negative: 0.00329
2018-10-22 16:03:16.019160: Epoch [180/1000] [ 60/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 16:03:26.287824: Epoch [180/1000] [ 80/183], total loss: 0.00411, regularization loss: 0.29019, contrastive loss: 0.00411, Loss positive: 0.00000, Loss negative: 0.00411
2018-10-22 16:03:36.577042: Epoch [180/1000] [100/183], total loss: 0.02762, regularization loss: 0.29019, contrastive loss: 0.02762, Loss positive: 0.02645, Loss negative: 0.00117
2018-10-22 16:03:47.022218: Epoch [180/1000] [120/183], total loss: 0.00079, regularization loss: 0.29019, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 16:03:57.347208: Epoch [180/1000] [140/183], total loss: 0.02082, regularization loss: 0.29019, contrastive loss: 0.02082, Loss positive: 0.01423, Loss negative: 0.00659
2018-10-22 16:04:07.715802: Epoch [180/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:04:18.331225: Epoch [180/1000] [180/183], total loss: 0.00854, regularization loss: 0.29019, contrastive loss: 0.00854, Loss positive: 0.00783, Loss negative: 0.00071
Recall@1: 0.26199
Recall@2: 0.37255
Recall@4: 0.49477
Recall@8: 0.63504
Recall@16: 0.76452
Recall@32: 0.87002
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 48.  58.  84.  83.  59.  55.  45.  98.  65.  66.  64.  59.  76.  69.
 101.  76.  80.  52.  57.  19.  56.  55.  49.  68.  73.  74.  71.  67.
  34.  21.  39.  51.  63.  60.  87.  50.  55.  57.  47.  34.  78.  62.
  52.  64.  65. 104.  69.  56.  64.  54.  55.  51.  40.  40.  67.  54.
  67.  76.  60.  78.  70.  47.  37.  32.  62.  58.  49.  76.  50.  67.
  68.  89.  30.  55.  42.  49.  55.  73.  75.  45.  54.  65.  59.  70.
  26.  73.  54.  32.  43.  48.  65.  56.  86.  64.  75.  35.  54.  58.
  54.  43.]
Purity is 0.241
count_cross = [[0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 1. 2. ... 0. 0. 0.]
 ...
 [0. 3. 2. ... 6. 6. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 2. 0. ... 0. 0. 0.]]
Mutual information is 2.04932
5924.0
5924
Entropy cluster is 4.56616
Entropy class is 4.60444
normalized_mutual_information is 0.44693
tp_and_fp = 185734.0
tp = 20233.0
fp is 165501.0
fn is 152517.0
RI is 0.981873042556153
Precision is 0.10893535916956508
Recall is 0.11712301013024602
F_1 is 0.11288090960823914

normalized_mutual_information = 0.4469318597293082
RI = 0.981873042556153
F_1 = 0.11288090960823914

The NN is 0.26199
The FT is 0.14145
The ST is 0.22120
The DCG is 0.52595
The E is 0.11881
The MAP 0.11591

2018-10-22 16:05:57.711209: Epoch [181/1000] [ 20/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 16:06:07.778524: Epoch [181/1000] [ 40/183], total loss: 0.00251, regularization loss: 0.29019, contrastive loss: 0.00251, Loss positive: 0.00000, Loss negative: 0.00251
2018-10-22 16:06:17.900112: Epoch [181/1000] [ 60/183], total loss: 0.00460, regularization loss: 0.29019, contrastive loss: 0.00460, Loss positive: 0.00000, Loss negative: 0.00460
2018-10-22 16:06:27.995551: Epoch [181/1000] [ 80/183], total loss: 0.00072, regularization loss: 0.29019, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 16:06:38.160656: Epoch [181/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:06:48.335942: Epoch [181/1000] [120/183], total loss: 0.00261, regularization loss: 0.29019, contrastive loss: 0.00261, Loss positive: 0.00000, Loss negative: 0.00261
2018-10-22 16:06:58.555958: Epoch [181/1000] [140/183], total loss: 0.00097, regularization loss: 0.29019, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 16:07:08.933143: Epoch [181/1000] [160/183], total loss: 0.00360, regularization loss: 0.29019, contrastive loss: 0.00360, Loss positive: 0.00000, Loss negative: 0.00360
2018-10-22 16:07:19.186427: Epoch [181/1000] [180/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 16:07:40.641162: Epoch [182/1000] [ 20/183], total loss: 0.00103, regularization loss: 0.29019, contrastive loss: 0.00103, Loss positive: 0.00000, Loss negative: 0.00103
2018-10-22 16:07:50.802123: Epoch [182/1000] [ 40/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 16:08:01.014564: Epoch [182/1000] [ 60/183], total loss: 0.07220, regularization loss: 0.29019, contrastive loss: 0.07220, Loss positive: 0.06484, Loss negative: 0.00736
2018-10-22 16:08:11.215478: Epoch [182/1000] [ 80/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 16:08:21.458698: Epoch [182/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:08:31.902911: Epoch [182/1000] [120/183], total loss: 0.00098, regularization loss: 0.29019, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 16:08:42.157595: Epoch [182/1000] [140/183], total loss: 0.01595, regularization loss: 0.29019, contrastive loss: 0.01595, Loss positive: 0.01571, Loss negative: 0.00024
2018-10-22 16:08:52.383629: Epoch [182/1000] [160/183], total loss: 0.00227, regularization loss: 0.29019, contrastive loss: 0.00227, Loss positive: 0.00000, Loss negative: 0.00227
2018-10-22 16:09:02.619921: Epoch [182/1000] [180/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 16:09:24.133720: Epoch [183/1000] [ 20/183], total loss: 0.00357, regularization loss: 0.29019, contrastive loss: 0.00357, Loss positive: 0.00000, Loss negative: 0.00357
2018-10-22 16:09:34.314540: Epoch [183/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:09:44.501557: Epoch [183/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:09:54.729706: Epoch [183/1000] [ 80/183], total loss: 0.00630, regularization loss: 0.29019, contrastive loss: 0.00630, Loss positive: 0.00000, Loss negative: 0.00630
2018-10-22 16:10:04.973397: Epoch [183/1000] [100/183], total loss: 0.00457, regularization loss: 0.29019, contrastive loss: 0.00457, Loss positive: 0.00000, Loss negative: 0.00457
2018-10-22 16:10:15.218046: Epoch [183/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:10:25.450102: Epoch [183/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:10:35.683604: Epoch [183/1000] [160/183], total loss: 0.02991, regularization loss: 0.29019, contrastive loss: 0.02991, Loss positive: 0.02991, Loss negative: 0.00000
2018-10-22 16:10:45.955733: Epoch [183/1000] [180/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:11:07.394319: Epoch [184/1000] [ 20/183], total loss: 0.00026, regularization loss: 0.29019, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 16:11:17.551887: Epoch [184/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:11:27.829815: Epoch [184/1000] [ 60/183], total loss: 0.00053, regularization loss: 0.29019, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 16:11:38.059224: Epoch [184/1000] [ 80/183], total loss: 0.03125, regularization loss: 0.29019, contrastive loss: 0.03125, Loss positive: 0.02710, Loss negative: 0.00415
2018-10-22 16:11:48.299539: Epoch [184/1000] [100/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:11:58.537979: Epoch [184/1000] [120/183], total loss: 0.02105, regularization loss: 0.29019, contrastive loss: 0.02105, Loss positive: 0.01665, Loss negative: 0.00440
2018-10-22 16:12:08.798708: Epoch [184/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:12:19.251603: Epoch [184/1000] [160/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 16:12:29.637780: Epoch [184/1000] [180/183], total loss: 0.00088, regularization loss: 0.29019, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 16:12:54.074525: Epoch [185/1000] [ 20/183], total loss: 0.01909, regularization loss: 0.29019, contrastive loss: 0.01909, Loss positive: 0.01741, Loss negative: 0.00169
2018-10-22 16:13:04.332963: Epoch [185/1000] [ 40/183], total loss: 0.00073, regularization loss: 0.29019, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 16:13:14.584905: Epoch [185/1000] [ 60/183], total loss: 0.00857, regularization loss: 0.29019, contrastive loss: 0.00857, Loss positive: 0.00000, Loss negative: 0.00857
2018-10-22 16:13:24.830598: Epoch [185/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:13:35.118149: Epoch [185/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:13:45.613182: Epoch [185/1000] [120/183], total loss: 0.00022, regularization loss: 0.29019, contrastive loss: 0.00022, Loss positive: 0.00000, Loss negative: 0.00022
2018-10-22 16:13:56.017014: Epoch [185/1000] [140/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:14:06.403346: Epoch [185/1000] [160/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 16:14:17.097510: Epoch [185/1000] [180/183], total loss: 0.02469, regularization loss: 0.29019, contrastive loss: 0.02469, Loss positive: 0.02017, Loss negative: 0.00452
2018-10-22 16:14:41.946891: Epoch [186/1000] [ 20/183], total loss: 0.00072, regularization loss: 0.29019, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 16:14:52.193134: Epoch [186/1000] [ 40/183], total loss: 0.00173, regularization loss: 0.29019, contrastive loss: 0.00173, Loss positive: 0.00000, Loss negative: 0.00173
2018-10-22 16:15:02.420811: Epoch [186/1000] [ 60/183], total loss: 0.02948, regularization loss: 0.29019, contrastive loss: 0.02948, Loss positive: 0.02948, Loss negative: 0.00000
2018-10-22 16:15:12.611993: Epoch [186/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 16:15:23.033263: Epoch [186/1000] [100/183], total loss: 0.00164, regularization loss: 0.29019, contrastive loss: 0.00164, Loss positive: 0.00000, Loss negative: 0.00164
2018-10-22 16:15:33.263865: Epoch [186/1000] [120/183], total loss: 0.02376, regularization loss: 0.29019, contrastive loss: 0.02376, Loss positive: 0.02360, Loss negative: 0.00015
2018-10-22 16:15:43.522396: Epoch [186/1000] [140/183], total loss: 0.01699, regularization loss: 0.29019, contrastive loss: 0.01699, Loss positive: 0.01699, Loss negative: 0.00000
2018-10-22 16:15:53.939011: Epoch [186/1000] [160/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 16:16:04.302541: Epoch [186/1000] [180/183], total loss: 0.00051, regularization loss: 0.29019, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 16:16:26.855465: Epoch [187/1000] [ 20/183], total loss: 0.04780, regularization loss: 0.29019, contrastive loss: 0.04780, Loss positive: 0.04780, Loss negative: 0.00000
2018-10-22 16:16:37.030611: Epoch [187/1000] [ 40/183], total loss: 0.03360, regularization loss: 0.29019, contrastive loss: 0.03360, Loss positive: 0.02579, Loss negative: 0.00780
2018-10-22 16:16:47.209997: Epoch [187/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:16:57.433626: Epoch [187/1000] [ 80/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 16:17:07.670405: Epoch [187/1000] [100/183], total loss: 0.00142, regularization loss: 0.29019, contrastive loss: 0.00142, Loss positive: 0.00000, Loss negative: 0.00142
2018-10-22 16:17:17.944993: Epoch [187/1000] [120/183], total loss: 0.01373, regularization loss: 0.29019, contrastive loss: 0.01373, Loss positive: 0.01348, Loss negative: 0.00025
2018-10-22 16:17:28.256138: Epoch [187/1000] [140/183], total loss: 0.00857, regularization loss: 0.29019, contrastive loss: 0.00857, Loss positive: 0.00727, Loss negative: 0.00129
2018-10-22 16:17:38.545341: Epoch [187/1000] [160/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 16:17:48.867087: Epoch [187/1000] [180/183], total loss: 0.06451, regularization loss: 0.29019, contrastive loss: 0.06451, Loss positive: 0.05490, Loss negative: 0.00961
2018-10-22 16:18:10.317735: Epoch [188/1000] [ 20/183], total loss: 0.00139, regularization loss: 0.29019, contrastive loss: 0.00139, Loss positive: 0.00000, Loss negative: 0.00139
2018-10-22 16:18:20.510629: Epoch [188/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:18:30.754637: Epoch [188/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:18:40.982341: Epoch [188/1000] [ 80/183], total loss: 0.00172, regularization loss: 0.29019, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 16:18:51.207926: Epoch [188/1000] [100/183], total loss: 0.00396, regularization loss: 0.29019, contrastive loss: 0.00396, Loss positive: 0.00000, Loss negative: 0.00396
2018-10-22 16:19:01.453484: Epoch [188/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:19:11.678175: Epoch [188/1000] [140/183], total loss: 0.00155, regularization loss: 0.29019, contrastive loss: 0.00155, Loss positive: 0.00000, Loss negative: 0.00155
2018-10-22 16:19:22.046554: Epoch [188/1000] [160/183], total loss: 0.02831, regularization loss: 0.29019, contrastive loss: 0.02831, Loss positive: 0.02269, Loss negative: 0.00562
2018-10-22 16:19:32.369038: Epoch [188/1000] [180/183], total loss: 0.00680, regularization loss: 0.29019, contrastive loss: 0.00680, Loss positive: 0.00000, Loss negative: 0.00680
2018-10-22 16:19:53.920430: Epoch [189/1000] [ 20/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 16:20:04.093069: Epoch [189/1000] [ 40/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 16:20:14.305441: Epoch [189/1000] [ 60/183], total loss: 0.00044, regularization loss: 0.29019, contrastive loss: 0.00044, Loss positive: 0.00000, Loss negative: 0.00044
2018-10-22 16:20:24.807249: Epoch [189/1000] [ 80/183], total loss: 0.00155, regularization loss: 0.29019, contrastive loss: 0.00155, Loss positive: 0.00000, Loss negative: 0.00155
2018-10-22 16:20:35.044790: Epoch [189/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:20:45.286511: Epoch [189/1000] [120/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 16:20:55.567904: Epoch [189/1000] [140/183], total loss: 0.00014, regularization loss: 0.29019, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 16:21:05.802599: Epoch [189/1000] [160/183], total loss: 0.00562, regularization loss: 0.29019, contrastive loss: 0.00562, Loss positive: 0.00000, Loss negative: 0.00562
2018-10-22 16:21:16.242391: Epoch [189/1000] [180/183], total loss: 0.01754, regularization loss: 0.29019, contrastive loss: 0.01754, Loss positive: 0.01692, Loss negative: 0.00063
2018-10-22 16:21:38.924485: Epoch [190/1000] [ 20/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 16:21:49.105702: Epoch [190/1000] [ 40/183], total loss: 0.00368, regularization loss: 0.29019, contrastive loss: 0.00368, Loss positive: 0.00363, Loss negative: 0.00005
2018-10-22 16:21:59.313798: Epoch [190/1000] [ 60/183], total loss: 0.00222, regularization loss: 0.29019, contrastive loss: 0.00222, Loss positive: 0.00000, Loss negative: 0.00222
2018-10-22 16:22:09.589573: Epoch [190/1000] [ 80/183], total loss: 0.00206, regularization loss: 0.29019, contrastive loss: 0.00206, Loss positive: 0.00000, Loss negative: 0.00206
2018-10-22 16:22:19.845115: Epoch [190/1000] [100/183], total loss: 0.00868, regularization loss: 0.29019, contrastive loss: 0.00868, Loss positive: 0.00831, Loss negative: 0.00038
2018-10-22 16:22:30.162809: Epoch [190/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:22:40.528564: Epoch [190/1000] [140/183], total loss: 0.00038, regularization loss: 0.29019, contrastive loss: 0.00038, Loss positive: 0.00000, Loss negative: 0.00038
2018-10-22 16:22:50.945563: Epoch [190/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:23:01.306067: Epoch [190/1000] [180/183], total loss: 0.00081, regularization loss: 0.29019, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
Recall@1: 0.24629
Recall@2: 0.35939
Recall@4: 0.49510
Recall@8: 0.64196
Recall@16: 0.76469
Recall@32: 0.86529
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 35.  51.  62.  67.  97.  51.  38.  97.  65.  42.  47.  68.  76.  49.
  56.  46.  88.  28.  81.  72.  88.  69.  96.  71.  53.  51.  65.  37.
  46.  61.  65.  42.  58.  47.  45.  52.  62.  69.  36.  84.  71.  47.
  27.  71.  78.  51.  92.  72.  80.  36.  34.  41.  52.  54.  18. 100.
  60.  62.  81.  50.  56.  28.  75.  68.  18.  51.  94.  40.  54.  82.
  71.  65.  47.  26.  52.  43.  42.  56.  57.  72.  36.  72.  52.  99.
  44.  58.  64.  88.  54.  83.  62.  27.  45.  62.  59.  69.  55.  50.
  78.  80.]
Purity is 0.237
count_cross = [[ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  4. 10.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  1.  3.  0.]
 [ 0.  0.  0. ...  0.  2.  0.]]
Mutual information is 2.04976
5924.0
5924
Entropy cluster is 4.55279
Entropy class is 4.60444
normalized_mutual_information is 0.44768
tp_and_fp = 190319.0
tp = 20244.0
fp is 170075.0
fn is 152506.0
RI is 0.9816129525398135
Precision is 0.10636878083638522
Recall is 0.11718668596237337
F_1 is 0.11151599282780959

normalized_mutual_information = 0.4476817733464129
RI = 0.9816129525398135
F_1 = 0.11151599282780959

The NN is 0.24629
The FT is 0.13766
The ST is 0.21509
The DCG is 0.52177
The E is 0.11525
The MAP 0.11204

2018-10-22 16:24:45.686129: Epoch [191/1000] [ 20/183], total loss: 0.00263, regularization loss: 0.29019, contrastive loss: 0.00263, Loss positive: 0.00000, Loss negative: 0.00263
2018-10-22 16:24:55.859915: Epoch [191/1000] [ 40/183], total loss: 0.00723, regularization loss: 0.29020, contrastive loss: 0.00723, Loss positive: 0.00723, Loss negative: 0.00000
2018-10-22 16:25:06.011108: Epoch [191/1000] [ 60/183], total loss: 0.03297, regularization loss: 0.29020, contrastive loss: 0.03297, Loss positive: 0.03195, Loss negative: 0.00102
2018-10-22 16:25:16.129645: Epoch [191/1000] [ 80/183], total loss: 0.00074, regularization loss: 0.29019, contrastive loss: 0.00074, Loss positive: 0.00000, Loss negative: 0.00074
2018-10-22 16:25:26.280679: Epoch [191/1000] [100/183], total loss: 0.00051, regularization loss: 0.29019, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 16:25:36.455572: Epoch [191/1000] [120/183], total loss: 0.00088, regularization loss: 0.29019, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 16:25:46.687932: Epoch [191/1000] [140/183], total loss: 0.00049, regularization loss: 0.29019, contrastive loss: 0.00049, Loss positive: 0.00000, Loss negative: 0.00049
2018-10-22 16:25:56.909465: Epoch [191/1000] [160/183], total loss: 0.00327, regularization loss: 0.29019, contrastive loss: 0.00327, Loss positive: 0.00000, Loss negative: 0.00327
2018-10-22 16:26:07.154228: Epoch [191/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:26:29.383163: Epoch [192/1000] [ 20/183], total loss: 0.00057, regularization loss: 0.29019, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 16:26:39.527416: Epoch [192/1000] [ 40/183], total loss: 0.00192, regularization loss: 0.29019, contrastive loss: 0.00192, Loss positive: 0.00000, Loss negative: 0.00192
2018-10-22 16:26:49.686862: Epoch [192/1000] [ 60/183], total loss: 0.00018, regularization loss: 0.29020, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 16:26:59.884717: Epoch [192/1000] [ 80/183], total loss: 0.00401, regularization loss: 0.29020, contrastive loss: 0.00401, Loss positive: 0.00000, Loss negative: 0.00401
2018-10-22 16:27:10.118185: Epoch [192/1000] [100/183], total loss: 0.02954, regularization loss: 0.29020, contrastive loss: 0.02954, Loss positive: 0.02937, Loss negative: 0.00017
2018-10-22 16:27:20.368021: Epoch [192/1000] [120/183], total loss: 0.01723, regularization loss: 0.29020, contrastive loss: 0.01723, Loss positive: 0.01723, Loss negative: 0.00000
2018-10-22 16:27:30.603907: Epoch [192/1000] [140/183], total loss: 0.00158, regularization loss: 0.29019, contrastive loss: 0.00158, Loss positive: 0.00000, Loss negative: 0.00158
2018-10-22 16:27:40.824225: Epoch [192/1000] [160/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 16:27:51.088651: Epoch [192/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:28:12.588852: Epoch [193/1000] [ 20/183], total loss: 0.00934, regularization loss: 0.29019, contrastive loss: 0.00934, Loss positive: 0.00660, Loss negative: 0.00274
2018-10-22 16:28:22.800894: Epoch [193/1000] [ 40/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 16:28:33.004452: Epoch [193/1000] [ 60/183], total loss: 0.01191, regularization loss: 0.29019, contrastive loss: 0.01191, Loss positive: 0.00957, Loss negative: 0.00234
2018-10-22 16:28:43.202798: Epoch [193/1000] [ 80/183], total loss: 0.00991, regularization loss: 0.29019, contrastive loss: 0.00991, Loss positive: 0.00991, Loss negative: 0.00000
2018-10-22 16:28:53.434246: Epoch [193/1000] [100/183], total loss: 0.00378, regularization loss: 0.29019, contrastive loss: 0.00378, Loss positive: 0.00000, Loss negative: 0.00378
2018-10-22 16:29:03.678094: Epoch [193/1000] [120/183], total loss: 0.00784, regularization loss: 0.29019, contrastive loss: 0.00784, Loss positive: 0.00000, Loss negative: 0.00784
2018-10-22 16:29:13.933603: Epoch [193/1000] [140/183], total loss: 0.00432, regularization loss: 0.29019, contrastive loss: 0.00432, Loss positive: 0.00000, Loss negative: 0.00432
2018-10-22 16:29:24.194033: Epoch [193/1000] [160/183], total loss: 0.00101, regularization loss: 0.29019, contrastive loss: 0.00101, Loss positive: 0.00000, Loss negative: 0.00101
2018-10-22 16:29:34.469572: Epoch [193/1000] [180/183], total loss: 0.01050, regularization loss: 0.29019, contrastive loss: 0.01050, Loss positive: 0.00000, Loss negative: 0.01050
2018-10-22 16:29:55.971431: Epoch [194/1000] [ 20/183], total loss: 0.00286, regularization loss: 0.29019, contrastive loss: 0.00286, Loss positive: 0.00000, Loss negative: 0.00286
2018-10-22 16:30:06.172494: Epoch [194/1000] [ 40/183], total loss: 0.02602, regularization loss: 0.29019, contrastive loss: 0.02602, Loss positive: 0.02378, Loss negative: 0.00224
2018-10-22 16:30:16.438402: Epoch [194/1000] [ 60/183], total loss: 0.01188, regularization loss: 0.29019, contrastive loss: 0.01188, Loss positive: 0.01075, Loss negative: 0.00114
2018-10-22 16:30:26.647207: Epoch [194/1000] [ 80/183], total loss: 0.00305, regularization loss: 0.29019, contrastive loss: 0.00305, Loss positive: 0.00000, Loss negative: 0.00305
2018-10-22 16:30:36.900875: Epoch [194/1000] [100/183], total loss: 0.01314, regularization loss: 0.29019, contrastive loss: 0.01314, Loss positive: 0.01306, Loss negative: 0.00008
2018-10-22 16:30:47.116427: Epoch [194/1000] [120/183], total loss: 0.00188, regularization loss: 0.29019, contrastive loss: 0.00188, Loss positive: 0.00000, Loss negative: 0.00188
2018-10-22 16:30:57.345946: Epoch [194/1000] [140/183], total loss: 0.01686, regularization loss: 0.29019, contrastive loss: 0.01686, Loss positive: 0.01686, Loss negative: 0.00000
2018-10-22 16:31:07.594359: Epoch [194/1000] [160/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:31:17.845697: Epoch [194/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:31:40.634366: Epoch [195/1000] [ 20/183], total loss: 0.02262, regularization loss: 0.29019, contrastive loss: 0.02262, Loss positive: 0.02196, Loss negative: 0.00067
2018-10-22 16:31:50.840763: Epoch [195/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:32:01.057983: Epoch [195/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:32:11.293458: Epoch [195/1000] [ 80/183], total loss: 0.03049, regularization loss: 0.29019, contrastive loss: 0.03049, Loss positive: 0.02624, Loss negative: 0.00426
2018-10-22 16:32:21.552711: Epoch [195/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:32:31.793416: Epoch [195/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:32:42.386276: Epoch [195/1000] [140/183], total loss: 0.00274, regularization loss: 0.29019, contrastive loss: 0.00274, Loss positive: 0.00000, Loss negative: 0.00274
2018-10-22 16:32:52.792788: Epoch [195/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:33:03.198239: Epoch [195/1000] [180/183], total loss: 0.02400, regularization loss: 0.29019, contrastive loss: 0.02400, Loss positive: 0.02368, Loss negative: 0.00031
2018-10-22 16:33:27.605185: Epoch [196/1000] [ 20/183], total loss: 0.00117, regularization loss: 0.29019, contrastive loss: 0.00117, Loss positive: 0.00000, Loss negative: 0.00117
2018-10-22 16:33:37.885006: Epoch [196/1000] [ 40/183], total loss: 0.00212, regularization loss: 0.29019, contrastive loss: 0.00212, Loss positive: 0.00000, Loss negative: 0.00212
2018-10-22 16:33:48.130240: Epoch [196/1000] [ 60/183], total loss: 0.01044, regularization loss: 0.29019, contrastive loss: 0.01044, Loss positive: 0.00839, Loss negative: 0.00206
2018-10-22 16:33:58.375512: Epoch [196/1000] [ 80/183], total loss: 0.00223, regularization loss: 0.29019, contrastive loss: 0.00223, Loss positive: 0.00000, Loss negative: 0.00223
2018-10-22 16:34:08.963897: Epoch [196/1000] [100/183], total loss: 0.02184, regularization loss: 0.29019, contrastive loss: 0.02184, Loss positive: 0.02155, Loss negative: 0.00029
2018-10-22 16:34:19.487544: Epoch [196/1000] [120/183], total loss: 0.00167, regularization loss: 0.29019, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 16:34:29.951541: Epoch [196/1000] [140/183], total loss: 0.01525, regularization loss: 0.29019, contrastive loss: 0.01525, Loss positive: 0.01504, Loss negative: 0.00021
2018-10-22 16:34:40.444516: Epoch [196/1000] [160/183], total loss: 0.00084, regularization loss: 0.29019, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 16:34:51.049226: Epoch [196/1000] [180/183], total loss: 0.01730, regularization loss: 0.29019, contrastive loss: 0.01730, Loss positive: 0.01345, Loss negative: 0.00384
2018-10-22 16:35:15.456838: Epoch [197/1000] [ 20/183], total loss: 0.01225, regularization loss: 0.29019, contrastive loss: 0.01225, Loss positive: 0.01225, Loss negative: 0.00000
2018-10-22 16:35:25.618869: Epoch [197/1000] [ 40/183], total loss: 0.01719, regularization loss: 0.29019, contrastive loss: 0.01719, Loss positive: 0.01687, Loss negative: 0.00032
2018-10-22 16:35:35.845356: Epoch [197/1000] [ 60/183], total loss: 0.00074, regularization loss: 0.29019, contrastive loss: 0.00074, Loss positive: 0.00000, Loss negative: 0.00074
2018-10-22 16:35:46.003933: Epoch [197/1000] [ 80/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:35:56.268995: Epoch [197/1000] [100/183], total loss: 0.00084, regularization loss: 0.29019, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 16:36:06.494394: Epoch [197/1000] [120/183], total loss: 0.00058, regularization loss: 0.29019, contrastive loss: 0.00058, Loss positive: 0.00000, Loss negative: 0.00058
2018-10-22 16:36:16.770175: Epoch [197/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:36:27.008616: Epoch [197/1000] [160/183], total loss: 0.00104, regularization loss: 0.29019, contrastive loss: 0.00104, Loss positive: 0.00000, Loss negative: 0.00104
2018-10-22 16:36:37.242205: Epoch [197/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:36:58.955883: Epoch [198/1000] [ 20/183], total loss: 0.00841, regularization loss: 0.29019, contrastive loss: 0.00841, Loss positive: 0.00000, Loss negative: 0.00841
2018-10-22 16:37:09.125026: Epoch [198/1000] [ 40/183], total loss: 0.00123, regularization loss: 0.29019, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 16:37:19.328837: Epoch [198/1000] [ 60/183], total loss: 0.00097, regularization loss: 0.29019, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 16:37:29.541699: Epoch [198/1000] [ 80/183], total loss: 0.00792, regularization loss: 0.29019, contrastive loss: 0.00792, Loss positive: 0.00791, Loss negative: 0.00000
2018-10-22 16:37:39.795905: Epoch [198/1000] [100/183], total loss: 0.00107, regularization loss: 0.29019, contrastive loss: 0.00107, Loss positive: 0.00000, Loss negative: 0.00107
2018-10-22 16:37:50.034602: Epoch [198/1000] [120/183], total loss: 0.01603, regularization loss: 0.29019, contrastive loss: 0.01603, Loss positive: 0.01588, Loss negative: 0.00014
2018-10-22 16:38:00.264145: Epoch [198/1000] [140/183], total loss: 0.02446, regularization loss: 0.29019, contrastive loss: 0.02446, Loss positive: 0.02439, Loss negative: 0.00007
2018-10-22 16:38:10.517412: Epoch [198/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:38:20.767245: Epoch [198/1000] [180/183], total loss: 0.02864, regularization loss: 0.29019, contrastive loss: 0.02864, Loss positive: 0.02499, Loss negative: 0.00365
2018-10-22 16:38:42.248475: Epoch [199/1000] [ 20/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 16:38:52.399136: Epoch [199/1000] [ 40/183], total loss: 0.01240, regularization loss: 0.29019, contrastive loss: 0.01240, Loss positive: 0.01240, Loss negative: 0.00000
2018-10-22 16:39:02.640220: Epoch [199/1000] [ 60/183], total loss: 0.00273, regularization loss: 0.29019, contrastive loss: 0.00273, Loss positive: 0.00000, Loss negative: 0.00273
2018-10-22 16:39:13.096858: Epoch [199/1000] [ 80/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 16:39:23.343402: Epoch [199/1000] [100/183], total loss: 0.00161, regularization loss: 0.29019, contrastive loss: 0.00161, Loss positive: 0.00000, Loss negative: 0.00161
2018-10-22 16:39:33.574707: Epoch [199/1000] [120/183], total loss: 0.02772, regularization loss: 0.29019, contrastive loss: 0.02772, Loss positive: 0.02725, Loss negative: 0.00047
2018-10-22 16:39:43.798926: Epoch [199/1000] [140/183], total loss: 0.00143, regularization loss: 0.29019, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 16:39:54.035863: Epoch [199/1000] [160/183], total loss: 0.00471, regularization loss: 0.29019, contrastive loss: 0.00471, Loss positive: 0.00000, Loss negative: 0.00471
2018-10-22 16:40:04.283627: Epoch [199/1000] [180/183], total loss: 0.01737, regularization loss: 0.29019, contrastive loss: 0.01737, Loss positive: 0.01507, Loss negative: 0.00230
2018-10-22 16:40:25.748428: Epoch [200/1000] [ 20/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 16:40:35.925435: Epoch [200/1000] [ 40/183], total loss: 0.00232, regularization loss: 0.29019, contrastive loss: 0.00232, Loss positive: 0.00000, Loss negative: 0.00232
2018-10-22 16:40:46.271620: Epoch [200/1000] [ 60/183], total loss: 0.01110, regularization loss: 0.29019, contrastive loss: 0.01110, Loss positive: 0.01110, Loss negative: 0.00000
2018-10-22 16:40:56.530425: Epoch [200/1000] [ 80/183], total loss: 0.02482, regularization loss: 0.29019, contrastive loss: 0.02482, Loss positive: 0.02409, Loss negative: 0.00072
2018-10-22 16:41:06.751970: Epoch [200/1000] [100/183], total loss: 0.00182, regularization loss: 0.29019, contrastive loss: 0.00182, Loss positive: 0.00000, Loss negative: 0.00182
2018-10-22 16:41:16.990656: Epoch [200/1000] [120/183], total loss: 0.00047, regularization loss: 0.29019, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 16:41:27.227729: Epoch [200/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:41:37.500782: Epoch [200/1000] [160/183], total loss: 0.00170, regularization loss: 0.29019, contrastive loss: 0.00170, Loss positive: 0.00000, Loss negative: 0.00170
2018-10-22 16:41:47.834198: Epoch [200/1000] [180/183], total loss: 0.04100, regularization loss: 0.29019, contrastive loss: 0.04100, Loss positive: 0.04037, Loss negative: 0.00063
Recall@1: 0.26013
Recall@2: 0.37897
Recall@4: 0.51621
Recall@8: 0.65631
Recall@16: 0.78038
Recall@32: 0.87441
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [71. 35. 65. 68. 78. 45. 74. 43. 49. 77. 55. 43. 70. 21. 65. 70. 59. 44.
 71. 57. 81. 38. 28. 64. 81. 41. 61. 84. 65. 38. 67. 61. 81. 93. 57. 62.
 55. 49. 34. 79. 28. 80. 61. 43. 46. 54. 64. 87. 78. 41. 64. 66. 79. 93.
 78. 37. 71. 60. 52. 50. 68. 41. 85. 53. 54. 67. 43. 63. 36. 68. 46. 63.
 34. 54. 72. 69. 68. 81. 59. 68. 70. 22. 91. 99. 53. 54. 66. 65. 22. 73.
 25. 83. 42. 50. 45. 32. 38. 97. 56. 38.]
Purity is 0.244
count_cross = [[0. 0. 0. ... 0. 1. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 ...
 [0. 1. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 2.06730
5924.0
5924
Entropy cluster is 4.55658
Entropy class is 4.60444
normalized_mutual_information is 0.45133
tp_and_fp = 188743.0
tp = 20883.0
fp is 167860.0
fn is 151867.0
RI is 0.9817756299245676
Precision is 0.11064251389455503
Recall is 0.12088567293777135
F_1 is 0.11553750695034205

normalized_mutual_information = 0.45132545346148406
RI = 0.9817756299245676
F_1 = 0.11553750695034205

The NN is 0.26013
The FT is 0.14249
The ST is 0.22248
The DCG is 0.52771
The E is 0.12018
The MAP 0.11714

2018-10-22 16:43:29.249758: Epoch [201/1000] [ 20/183], total loss: 0.00070, regularization loss: 0.29019, contrastive loss: 0.00070, Loss positive: 0.00000, Loss negative: 0.00070
2018-10-22 16:43:39.428724: Epoch [201/1000] [ 40/183], total loss: 0.01203, regularization loss: 0.29019, contrastive loss: 0.01203, Loss positive: 0.01175, Loss negative: 0.00028
2018-10-22 16:43:49.602186: Epoch [201/1000] [ 60/183], total loss: 0.01727, regularization loss: 0.29019, contrastive loss: 0.01727, Loss positive: 0.00000, Loss negative: 0.01727
2018-10-22 16:43:59.806297: Epoch [201/1000] [ 80/183], total loss: 0.01885, regularization loss: 0.29019, contrastive loss: 0.01885, Loss positive: 0.01832, Loss negative: 0.00053
2018-10-22 16:44:10.021222: Epoch [201/1000] [100/183], total loss: 0.02898, regularization loss: 0.29019, contrastive loss: 0.02898, Loss positive: 0.02898, Loss negative: 0.00000
2018-10-22 16:44:20.306229: Epoch [201/1000] [120/183], total loss: 0.03030, regularization loss: 0.29019, contrastive loss: 0.03030, Loss positive: 0.03030, Loss negative: 0.00000
2018-10-22 16:44:30.560186: Epoch [201/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:44:41.032810: Epoch [201/1000] [160/183], total loss: 0.00102, regularization loss: 0.29019, contrastive loss: 0.00102, Loss positive: 0.00000, Loss negative: 0.00102
2018-10-22 16:44:51.360261: Epoch [201/1000] [180/183], total loss: 0.00222, regularization loss: 0.29019, contrastive loss: 0.00222, Loss positive: 0.00000, Loss negative: 0.00222
2018-10-22 16:45:15.729909: Epoch [202/1000] [ 20/183], total loss: 0.00801, regularization loss: 0.29019, contrastive loss: 0.00801, Loss positive: 0.00693, Loss negative: 0.00108
2018-10-22 16:45:25.867017: Epoch [202/1000] [ 40/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 16:45:36.083295: Epoch [202/1000] [ 60/183], total loss: 0.00221, regularization loss: 0.29019, contrastive loss: 0.00221, Loss positive: 0.00000, Loss negative: 0.00221
2018-10-22 16:45:46.284637: Epoch [202/1000] [ 80/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 16:45:56.554831: Epoch [202/1000] [100/183], total loss: 0.01637, regularization loss: 0.29019, contrastive loss: 0.01637, Loss positive: 0.01342, Loss negative: 0.00296
2018-10-22 16:46:06.796016: Epoch [202/1000] [120/183], total loss: 0.01914, regularization loss: 0.29019, contrastive loss: 0.01914, Loss positive: 0.00000, Loss negative: 0.01914
2018-10-22 16:46:17.038561: Epoch [202/1000] [140/183], total loss: 0.00865, regularization loss: 0.29019, contrastive loss: 0.00865, Loss positive: 0.00000, Loss negative: 0.00865
2018-10-22 16:46:27.273862: Epoch [202/1000] [160/183], total loss: 0.00313, regularization loss: 0.29019, contrastive loss: 0.00313, Loss positive: 0.00000, Loss negative: 0.00313
2018-10-22 16:46:37.505857: Epoch [202/1000] [180/183], total loss: 0.00028, regularization loss: 0.29019, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 16:46:58.981567: Epoch [203/1000] [ 20/183], total loss: 0.00143, regularization loss: 0.29019, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 16:47:09.140145: Epoch [203/1000] [ 40/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 16:47:19.288910: Epoch [203/1000] [ 60/183], total loss: 0.00451, regularization loss: 0.29019, contrastive loss: 0.00451, Loss positive: 0.00000, Loss negative: 0.00451
2018-10-22 16:47:29.566593: Epoch [203/1000] [ 80/183], total loss: 0.00143, regularization loss: 0.29019, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 16:47:39.773578: Epoch [203/1000] [100/183], total loss: 0.00229, regularization loss: 0.29019, contrastive loss: 0.00229, Loss positive: 0.00000, Loss negative: 0.00229
2018-10-22 16:47:50.051792: Epoch [203/1000] [120/183], total loss: 0.00288, regularization loss: 0.29019, contrastive loss: 0.00288, Loss positive: 0.00000, Loss negative: 0.00288
2018-10-22 16:48:00.351380: Epoch [203/1000] [140/183], total loss: 0.00032, regularization loss: 0.29019, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 16:48:10.623444: Epoch [203/1000] [160/183], total loss: 0.02700, regularization loss: 0.29019, contrastive loss: 0.02700, Loss positive: 0.02621, Loss negative: 0.00079
2018-10-22 16:48:21.137403: Epoch [203/1000] [180/183], total loss: 0.00195, regularization loss: 0.29019, contrastive loss: 0.00195, Loss positive: 0.00000, Loss negative: 0.00195
2018-10-22 16:48:42.799659: Epoch [204/1000] [ 20/183], total loss: 0.03485, regularization loss: 0.29019, contrastive loss: 0.03485, Loss positive: 0.02958, Loss negative: 0.00527
2018-10-22 16:48:53.001187: Epoch [204/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:49:03.199832: Epoch [204/1000] [ 60/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 16:49:13.615931: Epoch [204/1000] [ 80/183], total loss: 0.00646, regularization loss: 0.29019, contrastive loss: 0.00646, Loss positive: 0.00000, Loss negative: 0.00646
2018-10-22 16:49:23.816162: Epoch [204/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:49:34.058407: Epoch [204/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:49:44.282465: Epoch [204/1000] [140/183], total loss: 0.00145, regularization loss: 0.29019, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 16:49:54.535299: Epoch [204/1000] [160/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 16:50:04.751751: Epoch [204/1000] [180/183], total loss: 0.00017, regularization loss: 0.29019, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 16:50:26.292435: Epoch [205/1000] [ 20/183], total loss: 0.00209, regularization loss: 0.29019, contrastive loss: 0.00209, Loss positive: 0.00000, Loss negative: 0.00209
2018-10-22 16:50:36.460073: Epoch [205/1000] [ 40/183], total loss: 0.00345, regularization loss: 0.29019, contrastive loss: 0.00345, Loss positive: 0.00000, Loss negative: 0.00345
2018-10-22 16:50:46.693849: Epoch [205/1000] [ 60/183], total loss: 0.02688, regularization loss: 0.29019, contrastive loss: 0.02688, Loss positive: 0.02687, Loss negative: 0.00001
2018-10-22 16:50:56.909927: Epoch [205/1000] [ 80/183], total loss: 0.00782, regularization loss: 0.29019, contrastive loss: 0.00782, Loss positive: 0.00000, Loss negative: 0.00782
2018-10-22 16:51:07.128141: Epoch [205/1000] [100/183], total loss: 0.00461, regularization loss: 0.29019, contrastive loss: 0.00461, Loss positive: 0.00461, Loss negative: 0.00000
2018-10-22 16:51:17.373138: Epoch [205/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:51:27.665817: Epoch [205/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:51:38.151314: Epoch [205/1000] [160/183], total loss: 0.00352, regularization loss: 0.29019, contrastive loss: 0.00352, Loss positive: 0.00000, Loss negative: 0.00352
2018-10-22 16:51:48.406957: Epoch [205/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:52:11.109033: Epoch [206/1000] [ 20/183], total loss: 0.00257, regularization loss: 0.29019, contrastive loss: 0.00257, Loss positive: 0.00000, Loss negative: 0.00257
2018-10-22 16:52:21.311030: Epoch [206/1000] [ 40/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 16:52:31.530376: Epoch [206/1000] [ 60/183], total loss: 0.01241, regularization loss: 0.29019, contrastive loss: 0.01241, Loss positive: 0.01230, Loss negative: 0.00011
2018-10-22 16:52:41.792430: Epoch [206/1000] [ 80/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 16:52:52.448118: Epoch [206/1000] [100/183], total loss: 0.02507, regularization loss: 0.29019, contrastive loss: 0.02507, Loss positive: 0.02479, Loss negative: 0.00028
2018-10-22 16:53:02.858426: Epoch [206/1000] [120/183], total loss: 0.00215, regularization loss: 0.29019, contrastive loss: 0.00215, Loss positive: 0.00000, Loss negative: 0.00215
2018-10-22 16:53:13.195044: Epoch [206/1000] [140/183], total loss: 0.01170, regularization loss: 0.29019, contrastive loss: 0.01170, Loss positive: 0.01006, Loss negative: 0.00164
2018-10-22 16:53:23.564052: Epoch [206/1000] [160/183], total loss: 0.00932, regularization loss: 0.29019, contrastive loss: 0.00932, Loss positive: 0.00931, Loss negative: 0.00000
2018-10-22 16:53:34.094638: Epoch [206/1000] [180/183], total loss: 0.00238, regularization loss: 0.29019, contrastive loss: 0.00238, Loss positive: 0.00000, Loss negative: 0.00238
2018-10-22 16:53:58.879594: Epoch [207/1000] [ 20/183], total loss: 0.00090, regularization loss: 0.29019, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 16:54:09.132833: Epoch [207/1000] [ 40/183], total loss: 0.00117, regularization loss: 0.29019, contrastive loss: 0.00117, Loss positive: 0.00000, Loss negative: 0.00117
2018-10-22 16:54:19.367812: Epoch [207/1000] [ 60/183], total loss: 0.00039, regularization loss: 0.29019, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 16:54:29.643281: Epoch [207/1000] [ 80/183], total loss: 0.00073, regularization loss: 0.29019, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 16:54:40.046387: Epoch [207/1000] [100/183], total loss: 0.00248, regularization loss: 0.29019, contrastive loss: 0.00248, Loss positive: 0.00000, Loss negative: 0.00248
2018-10-22 16:54:50.350233: Epoch [207/1000] [120/183], total loss: 0.00068, regularization loss: 0.29019, contrastive loss: 0.00068, Loss positive: 0.00000, Loss negative: 0.00068
2018-10-22 16:55:00.651819: Epoch [207/1000] [140/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 16:55:11.012329: Epoch [207/1000] [160/183], total loss: 0.04388, regularization loss: 0.29019, contrastive loss: 0.04388, Loss positive: 0.04180, Loss negative: 0.00207
2018-10-22 16:55:21.336101: Epoch [207/1000] [180/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 16:55:43.960136: Epoch [208/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:55:54.150194: Epoch [208/1000] [ 40/183], total loss: 0.00081, regularization loss: 0.29019, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 16:56:04.370347: Epoch [208/1000] [ 60/183], total loss: 0.01290, regularization loss: 0.29019, contrastive loss: 0.01290, Loss positive: 0.01164, Loss negative: 0.00125
2018-10-22 16:56:14.582450: Epoch [208/1000] [ 80/183], total loss: 0.00927, regularization loss: 0.29019, contrastive loss: 0.00927, Loss positive: 0.00444, Loss negative: 0.00483
2018-10-22 16:56:24.778168: Epoch [208/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:56:35.013814: Epoch [208/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:56:45.383030: Epoch [208/1000] [140/183], total loss: 0.02420, regularization loss: 0.29019, contrastive loss: 0.02420, Loss positive: 0.02420, Loss negative: 0.00000
2018-10-22 16:56:55.602402: Epoch [208/1000] [160/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 16:57:05.864215: Epoch [208/1000] [180/183], total loss: 0.01158, regularization loss: 0.29019, contrastive loss: 0.01158, Loss positive: 0.00824, Loss negative: 0.00334
2018-10-22 16:57:27.670231: Epoch [209/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:57:37.809794: Epoch [209/1000] [ 40/183], total loss: 0.00568, regularization loss: 0.29019, contrastive loss: 0.00568, Loss positive: 0.00000, Loss negative: 0.00568
2018-10-22 16:57:48.019010: Epoch [209/1000] [ 60/183], total loss: 0.00424, regularization loss: 0.29019, contrastive loss: 0.00424, Loss positive: 0.00000, Loss negative: 0.00424
2018-10-22 16:57:58.398381: Epoch [209/1000] [ 80/183], total loss: 0.05333, regularization loss: 0.29019, contrastive loss: 0.05333, Loss positive: 0.05261, Loss negative: 0.00072
2018-10-22 16:58:08.608836: Epoch [209/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 16:58:18.856804: Epoch [209/1000] [120/183], total loss: 0.00284, regularization loss: 0.29019, contrastive loss: 0.00284, Loss positive: 0.00000, Loss negative: 0.00284
2018-10-22 16:58:29.128159: Epoch [209/1000] [140/183], total loss: 0.00248, regularization loss: 0.29019, contrastive loss: 0.00248, Loss positive: 0.00000, Loss negative: 0.00248
2018-10-22 16:58:39.390777: Epoch [209/1000] [160/183], total loss: 0.00456, regularization loss: 0.29019, contrastive loss: 0.00456, Loss positive: 0.00000, Loss negative: 0.00456
2018-10-22 16:58:50.031951: Epoch [209/1000] [180/183], total loss: 0.00349, regularization loss: 0.29019, contrastive loss: 0.00349, Loss positive: 0.00000, Loss negative: 0.00349
2018-10-22 16:59:11.601800: Epoch [210/1000] [ 20/183], total loss: 0.03937, regularization loss: 0.29019, contrastive loss: 0.03937, Loss positive: 0.03894, Loss negative: 0.00043
2018-10-22 16:59:21.762433: Epoch [210/1000] [ 40/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 16:59:31.946265: Epoch [210/1000] [ 60/183], total loss: 0.02352, regularization loss: 0.29019, contrastive loss: 0.02352, Loss positive: 0.01850, Loss negative: 0.00503
2018-10-22 16:59:42.293509: Epoch [210/1000] [ 80/183], total loss: 0.01673, regularization loss: 0.29019, contrastive loss: 0.01673, Loss positive: 0.01637, Loss negative: 0.00036
2018-10-22 16:59:52.890275: Epoch [210/1000] [100/183], total loss: 0.00720, regularization loss: 0.29019, contrastive loss: 0.00720, Loss positive: 0.00550, Loss negative: 0.00170
2018-10-22 17:00:03.123442: Epoch [210/1000] [120/183], total loss: 0.00942, regularization loss: 0.29019, contrastive loss: 0.00942, Loss positive: 0.00000, Loss negative: 0.00942
2018-10-22 17:00:13.353774: Epoch [210/1000] [140/183], total loss: 0.00082, regularization loss: 0.29019, contrastive loss: 0.00082, Loss positive: 0.00000, Loss negative: 0.00082
2018-10-22 17:00:24.012659: Epoch [210/1000] [160/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 17:00:34.244027: Epoch [210/1000] [180/183], total loss: 0.01287, regularization loss: 0.29019, contrastive loss: 0.01287, Loss positive: 0.01287, Loss negative: 0.00000
Recall@1: 0.25658
Recall@2: 0.37205
Recall@4: 0.51215
Recall@8: 0.64619
Recall@16: 0.77549
Recall@32: 0.87238
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 66.  87.  44.  37.  47.  42.  79.  94.  49.  58.  41.  66.  70.  31.
  66.  86.  22.  73.  56.  31.  61.  43.  60.  74.  48.  59.  75.  69.
  56.  61.  11.  91.  35.  70.  61.  27.  34.  65.  87.  84.  54.  39.
  58.  89. 110.  55.  92.  71.  40.  35.  68.  72.  37.  32.  77.  28.
  59.  49.  79.  41.  60.  79.  43.  51.  77.  71.  71.  85.  54.  50.
  74.  88.  75.  73.  36.  64. 105.  67.  41.  40.  39.  68.  56.  47.
  93.  50.  71.  51.  50.  67.  46.  56.  51.  69.  94.  67.  40.  38.
  31.  35.]
Purity is 0.242
count_cross = [[ 0.  0.  0. ...  0.  1.  0.]
 [ 0.  2.  0. ...  3. 14.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]]
Mutual information is 2.06105
5924.0
5924
Entropy cluster is 4.54775
Entropy class is 4.60444
normalized_mutual_information is 0.45040
tp_and_fp = 191988.0
tp = 20391.0
fp is 171597.0
fn is 152359.0
RI is 0.9815345778362267
Precision is 0.1062097631101944
Recall is 0.11803762662807525
F_1 is 0.11181176625413311

normalized_mutual_information = 0.45039551279548623
RI = 0.9815345778362267
F_1 = 0.11181176625413311

The NN is 0.25658
The FT is 0.14243
The ST is 0.22341
The DCG is 0.52827
The E is 0.11933
The MAP 0.11775

2018-10-22 17:02:08.663184: Epoch [211/1000] [ 20/183], total loss: 0.00067, regularization loss: 0.29019, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 17:02:18.757904: Epoch [211/1000] [ 40/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 17:02:28.861160: Epoch [211/1000] [ 60/183], total loss: 0.01344, regularization loss: 0.29019, contrastive loss: 0.01344, Loss positive: 0.01131, Loss negative: 0.00213
2018-10-22 17:02:39.061256: Epoch [211/1000] [ 80/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 17:02:49.339711: Epoch [211/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:02:59.614385: Epoch [211/1000] [120/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 17:03:10.034651: Epoch [211/1000] [140/183], total loss: 0.00328, regularization loss: 0.29019, contrastive loss: 0.00328, Loss positive: 0.00000, Loss negative: 0.00328
2018-10-22 17:03:20.539499: Epoch [211/1000] [160/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 17:03:30.815928: Epoch [211/1000] [180/183], total loss: 0.00130, regularization loss: 0.29019, contrastive loss: 0.00130, Loss positive: 0.00000, Loss negative: 0.00130
2018-10-22 17:03:55.405304: Epoch [212/1000] [ 20/183], total loss: 0.02759, regularization loss: 0.29019, contrastive loss: 0.02759, Loss positive: 0.02642, Loss negative: 0.00117
2018-10-22 17:04:05.667388: Epoch [212/1000] [ 40/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 17:04:15.919053: Epoch [212/1000] [ 60/183], total loss: 0.00091, regularization loss: 0.29019, contrastive loss: 0.00091, Loss positive: 0.00000, Loss negative: 0.00091
2018-10-22 17:04:26.187698: Epoch [212/1000] [ 80/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 17:04:36.466616: Epoch [212/1000] [100/183], total loss: 0.02681, regularization loss: 0.29019, contrastive loss: 0.02681, Loss positive: 0.02572, Loss negative: 0.00109
2018-10-22 17:04:46.804820: Epoch [212/1000] [120/183], total loss: 0.00385, regularization loss: 0.29019, contrastive loss: 0.00385, Loss positive: 0.00000, Loss negative: 0.00385
2018-10-22 17:04:57.136328: Epoch [212/1000] [140/183], total loss: 0.00193, regularization loss: 0.29019, contrastive loss: 0.00193, Loss positive: 0.00000, Loss negative: 0.00193
2018-10-22 17:05:07.602253: Epoch [212/1000] [160/183], total loss: 0.04859, regularization loss: 0.29019, contrastive loss: 0.04859, Loss positive: 0.04859, Loss negative: 0.00000
2018-10-22 17:05:18.021626: Epoch [212/1000] [180/183], total loss: 0.00026, regularization loss: 0.29019, contrastive loss: 0.00026, Loss positive: 0.00000, Loss negative: 0.00026
2018-10-22 17:05:40.688313: Epoch [213/1000] [ 20/183], total loss: 0.00891, regularization loss: 0.29019, contrastive loss: 0.00891, Loss positive: 0.00891, Loss negative: 0.00000
2018-10-22 17:05:50.873389: Epoch [213/1000] [ 40/183], total loss: 0.02328, regularization loss: 0.29019, contrastive loss: 0.02328, Loss positive: 0.01843, Loss negative: 0.00485
2018-10-22 17:06:01.078166: Epoch [213/1000] [ 60/183], total loss: 0.00458, regularization loss: 0.29019, contrastive loss: 0.00458, Loss positive: 0.00000, Loss negative: 0.00458
2018-10-22 17:06:11.265173: Epoch [213/1000] [ 80/183], total loss: 0.01356, regularization loss: 0.29019, contrastive loss: 0.01356, Loss positive: 0.01344, Loss negative: 0.00012
2018-10-22 17:06:21.526215: Epoch [213/1000] [100/183], total loss: 0.00310, regularization loss: 0.29019, contrastive loss: 0.00310, Loss positive: 0.00000, Loss negative: 0.00310
2018-10-22 17:06:31.727049: Epoch [213/1000] [120/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 17:06:41.972151: Epoch [213/1000] [140/183], total loss: 0.01386, regularization loss: 0.29019, contrastive loss: 0.01386, Loss positive: 0.01386, Loss negative: 0.00000
2018-10-22 17:06:52.224506: Epoch [213/1000] [160/183], total loss: 0.00280, regularization loss: 0.29019, contrastive loss: 0.00280, Loss positive: 0.00000, Loss negative: 0.00280
2018-10-22 17:07:02.466909: Epoch [213/1000] [180/183], total loss: 0.00038, regularization loss: 0.29019, contrastive loss: 0.00038, Loss positive: 0.00000, Loss negative: 0.00038
2018-10-22 17:07:23.922649: Epoch [214/1000] [ 20/183], total loss: 0.01583, regularization loss: 0.29019, contrastive loss: 0.01583, Loss positive: 0.01556, Loss negative: 0.00027
2018-10-22 17:07:34.089700: Epoch [214/1000] [ 40/183], total loss: 0.00141, regularization loss: 0.29019, contrastive loss: 0.00141, Loss positive: 0.00000, Loss negative: 0.00141
2018-10-22 17:07:44.290344: Epoch [214/1000] [ 60/183], total loss: 0.00494, regularization loss: 0.29019, contrastive loss: 0.00494, Loss positive: 0.00000, Loss negative: 0.00494
2018-10-22 17:07:54.494497: Epoch [214/1000] [ 80/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 17:08:04.765892: Epoch [214/1000] [100/183], total loss: 0.00766, regularization loss: 0.29019, contrastive loss: 0.00766, Loss positive: 0.00000, Loss negative: 0.00766
2018-10-22 17:08:14.996534: Epoch [214/1000] [120/183], total loss: 0.00313, regularization loss: 0.29019, contrastive loss: 0.00313, Loss positive: 0.00000, Loss negative: 0.00313
2018-10-22 17:08:25.230616: Epoch [214/1000] [140/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 17:08:35.457146: Epoch [214/1000] [160/183], total loss: 0.04184, regularization loss: 0.29019, contrastive loss: 0.04184, Loss positive: 0.03929, Loss negative: 0.00254
2018-10-22 17:08:45.697707: Epoch [214/1000] [180/183], total loss: 0.04850, regularization loss: 0.29019, contrastive loss: 0.04850, Loss positive: 0.04559, Loss negative: 0.00291
2018-10-22 17:09:07.296299: Epoch [215/1000] [ 20/183], total loss: 0.00367, regularization loss: 0.29019, contrastive loss: 0.00367, Loss positive: 0.00000, Loss negative: 0.00367
2018-10-22 17:09:17.457992: Epoch [215/1000] [ 40/183], total loss: 0.02296, regularization loss: 0.29019, contrastive loss: 0.02296, Loss positive: 0.02269, Loss negative: 0.00027
2018-10-22 17:09:27.672061: Epoch [215/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:09:37.930926: Epoch [215/1000] [ 80/183], total loss: 0.02589, regularization loss: 0.29019, contrastive loss: 0.02589, Loss positive: 0.02072, Loss negative: 0.00517
2018-10-22 17:09:48.169052: Epoch [215/1000] [100/183], total loss: 0.01026, regularization loss: 0.29019, contrastive loss: 0.01026, Loss positive: 0.00000, Loss negative: 0.01026
2018-10-22 17:09:58.397410: Epoch [215/1000] [120/183], total loss: 0.02854, regularization loss: 0.29019, contrastive loss: 0.02854, Loss positive: 0.02852, Loss negative: 0.00002
2018-10-22 17:10:08.640754: Epoch [215/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:10:18.872352: Epoch [215/1000] [160/183], total loss: 0.02280, regularization loss: 0.29019, contrastive loss: 0.02280, Loss positive: 0.02226, Loss negative: 0.00054
2018-10-22 17:10:29.110974: Epoch [215/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:10:50.565035: Epoch [216/1000] [ 20/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 17:11:00.718678: Epoch [216/1000] [ 40/183], total loss: 0.00049, regularization loss: 0.29019, contrastive loss: 0.00049, Loss positive: 0.00000, Loss negative: 0.00049
2018-10-22 17:11:10.896428: Epoch [216/1000] [ 60/183], total loss: 0.02305, regularization loss: 0.29019, contrastive loss: 0.02305, Loss positive: 0.02301, Loss negative: 0.00003
2018-10-22 17:11:21.181911: Epoch [216/1000] [ 80/183], total loss: 0.01671, regularization loss: 0.29019, contrastive loss: 0.01671, Loss positive: 0.01671, Loss negative: 0.00000
2018-10-22 17:11:31.476803: Epoch [216/1000] [100/183], total loss: 0.02671, regularization loss: 0.29019, contrastive loss: 0.02671, Loss positive: 0.02282, Loss negative: 0.00389
2018-10-22 17:11:41.755546: Epoch [216/1000] [120/183], total loss: 0.00228, regularization loss: 0.29019, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 17:11:52.038242: Epoch [216/1000] [140/183], total loss: 0.00028, regularization loss: 0.29019, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 17:12:02.323128: Epoch [216/1000] [160/183], total loss: 0.02009, regularization loss: 0.29019, contrastive loss: 0.02009, Loss positive: 0.01949, Loss negative: 0.00060
2018-10-22 17:12:12.803685: Epoch [216/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:12:35.640398: Epoch [217/1000] [ 20/183], total loss: 0.00177, regularization loss: 0.29019, contrastive loss: 0.00177, Loss positive: 0.00000, Loss negative: 0.00177
2018-10-22 17:12:45.934273: Epoch [217/1000] [ 40/183], total loss: 0.00206, regularization loss: 0.29019, contrastive loss: 0.00206, Loss positive: 0.00000, Loss negative: 0.00206
2018-10-22 17:12:56.173146: Epoch [217/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:13:06.422304: Epoch [217/1000] [ 80/183], total loss: 0.00499, regularization loss: 0.29019, contrastive loss: 0.00499, Loss positive: 0.00000, Loss negative: 0.00499
2018-10-22 17:13:16.825944: Epoch [217/1000] [100/183], total loss: 0.00686, regularization loss: 0.29019, contrastive loss: 0.00686, Loss positive: 0.00616, Loss negative: 0.00070
2018-10-22 17:13:27.106692: Epoch [217/1000] [120/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 17:13:37.598118: Epoch [217/1000] [140/183], total loss: 0.00356, regularization loss: 0.29019, contrastive loss: 0.00356, Loss positive: 0.00000, Loss negative: 0.00356
2018-10-22 17:13:48.173385: Epoch [217/1000] [160/183], total loss: 0.00023, regularization loss: 0.29019, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 17:13:58.524701: Epoch [217/1000] [180/183], total loss: 0.05726, regularization loss: 0.29019, contrastive loss: 0.05726, Loss positive: 0.04974, Loss negative: 0.00751
2018-10-22 17:14:23.067498: Epoch [218/1000] [ 20/183], total loss: 0.00780, regularization loss: 0.29019, contrastive loss: 0.00780, Loss positive: 0.00000, Loss negative: 0.00780
2018-10-22 17:14:33.311498: Epoch [218/1000] [ 40/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 17:14:43.571477: Epoch [218/1000] [ 60/183], total loss: 0.02528, regularization loss: 0.29019, contrastive loss: 0.02528, Loss positive: 0.02525, Loss negative: 0.00003
2018-10-22 17:14:53.939630: Epoch [218/1000] [ 80/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 17:15:04.498821: Epoch [218/1000] [100/183], total loss: 0.00059, regularization loss: 0.29019, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 17:15:14.827270: Epoch [218/1000] [120/183], total loss: 0.00071, regularization loss: 0.29019, contrastive loss: 0.00071, Loss positive: 0.00000, Loss negative: 0.00071
2018-10-22 17:15:25.084992: Epoch [218/1000] [140/183], total loss: 0.00090, regularization loss: 0.29019, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 17:15:35.340141: Epoch [218/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:15:45.638243: Epoch [218/1000] [180/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 17:16:08.258921: Epoch [219/1000] [ 20/183], total loss: 0.01128, regularization loss: 0.29019, contrastive loss: 0.01128, Loss positive: 0.00618, Loss negative: 0.00510
2018-10-22 17:16:18.475898: Epoch [219/1000] [ 40/183], total loss: 0.00076, regularization loss: 0.29019, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 17:16:28.691934: Epoch [219/1000] [ 60/183], total loss: 0.00079, regularization loss: 0.29019, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 17:16:38.902748: Epoch [219/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:16:49.141696: Epoch [219/1000] [100/183], total loss: 0.00051, regularization loss: 0.29019, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 17:16:59.390586: Epoch [219/1000] [120/183], total loss: 0.01919, regularization loss: 0.29019, contrastive loss: 0.01919, Loss positive: 0.01781, Loss negative: 0.00139
2018-10-22 17:17:09.615877: Epoch [219/1000] [140/183], total loss: 0.00170, regularization loss: 0.29019, contrastive loss: 0.00170, Loss positive: 0.00000, Loss negative: 0.00170
2018-10-22 17:17:19.865484: Epoch [219/1000] [160/183], total loss: 0.03403, regularization loss: 0.29019, contrastive loss: 0.03403, Loss positive: 0.03313, Loss negative: 0.00090
2018-10-22 17:17:30.149871: Epoch [219/1000] [180/183], total loss: 0.02797, regularization loss: 0.29019, contrastive loss: 0.02797, Loss positive: 0.02336, Loss negative: 0.00461
2018-10-22 17:17:51.788238: Epoch [220/1000] [ 20/183], total loss: 0.03225, regularization loss: 0.29019, contrastive loss: 0.03225, Loss positive: 0.03205, Loss negative: 0.00020
2018-10-22 17:18:02.014096: Epoch [220/1000] [ 40/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 17:18:12.191810: Epoch [220/1000] [ 60/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 17:18:22.435470: Epoch [220/1000] [ 80/183], total loss: 0.00072, regularization loss: 0.29019, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 17:18:32.650299: Epoch [220/1000] [100/183], total loss: 0.00342, regularization loss: 0.29019, contrastive loss: 0.00342, Loss positive: 0.00000, Loss negative: 0.00342
2018-10-22 17:18:43.241786: Epoch [220/1000] [120/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 17:18:53.462371: Epoch [220/1000] [140/183], total loss: 0.00032, regularization loss: 0.29019, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 17:19:03.699800: Epoch [220/1000] [160/183], total loss: 0.00436, regularization loss: 0.29019, contrastive loss: 0.00436, Loss positive: 0.00000, Loss negative: 0.00436
2018-10-22 17:19:13.956827: Epoch [220/1000] [180/183], total loss: 0.01678, regularization loss: 0.29019, contrastive loss: 0.01678, Loss positive: 0.00958, Loss negative: 0.00720
Recall@1: 0.27127
Recall@2: 0.38217
Recall@4: 0.51401
Recall@8: 0.65699
Recall@16: 0.78612
Recall@32: 0.87762
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 78.  53.  42.  39.  71.  45.  41.  50.  56.  77.  78. 107.  36.  45.
  59.  48.  42.  76.  75.  72.  37.  79.  61.  59.  74.  55.  61.  98.
  61.  54.  66.  44.  52.  63.  72.  69.  62.  71.  59.  45.  71.  54.
 116.  92.  57.  29.  32.  46.  41.  38.  43.  25.  81.  70.  80.  65.
  63.  53.  54.  64.  52.  85.  61.  65.  50.  52.  62.  65.  52.  36.
  33.  71.  44.  50.   9.  32.  55.  63.  52.  80.  66.  82.  12.  71.
  55.  60.  81.  63.  30.  37.  77.  48.  93.  91.  97.  43.  69.  68.
  61.  45.]
Purity is 0.244
count_cross = [[0. 2. 1. ... 1. 1. 0.]
 [0. 0. 0. ... 1. 2. 0.]
 [0. 0. 0. ... 1. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 2.09219
5924.0
5924
Entropy cluster is 4.55153
Entropy class is 4.60444
normalized_mutual_information is 0.45701
tp_and_fp = 190435.0
tp = 20505.0
fp is 169930.0
fn is 152245.0
RI is 0.9816360944522908
Precision is 0.10767453461811116
Recall is 0.11869753979739509
F_1 is 0.11291765904428873

normalized_mutual_information = 0.45701030039898505
RI = 0.9816360944522908
F_1 = 0.11291765904428873

The NN is 0.27127
The FT is 0.14287
The ST is 0.22369
The DCG is 0.52987
The E is 0.11998
The MAP 0.11722

2018-10-22 17:20:45.100050: Epoch [221/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:20:55.192944: Epoch [221/1000] [ 40/183], total loss: 0.00226, regularization loss: 0.29019, contrastive loss: 0.00226, Loss positive: 0.00000, Loss negative: 0.00226
2018-10-22 17:21:05.299881: Epoch [221/1000] [ 60/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 17:21:15.448235: Epoch [221/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:21:25.670278: Epoch [221/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:21:35.899809: Epoch [221/1000] [120/183], total loss: 0.00190, regularization loss: 0.29019, contrastive loss: 0.00190, Loss positive: 0.00000, Loss negative: 0.00190
2018-10-22 17:21:46.121389: Epoch [221/1000] [140/183], total loss: 0.00107, regularization loss: 0.29019, contrastive loss: 0.00107, Loss positive: 0.00000, Loss negative: 0.00107
2018-10-22 17:21:56.392949: Epoch [221/1000] [160/183], total loss: 0.00373, regularization loss: 0.29019, contrastive loss: 0.00373, Loss positive: 0.00000, Loss negative: 0.00373
2018-10-22 17:22:06.698459: Epoch [221/1000] [180/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 17:22:29.313249: Epoch [222/1000] [ 20/183], total loss: 0.00392, regularization loss: 0.29019, contrastive loss: 0.00392, Loss positive: 0.00000, Loss negative: 0.00392
2018-10-22 17:22:39.500080: Epoch [222/1000] [ 40/183], total loss: 0.00969, regularization loss: 0.29019, contrastive loss: 0.00969, Loss positive: 0.00931, Loss negative: 0.00038
2018-10-22 17:22:49.760149: Epoch [222/1000] [ 60/183], total loss: 0.00085, regularization loss: 0.29019, contrastive loss: 0.00085, Loss positive: 0.00000, Loss negative: 0.00085
2018-10-22 17:23:00.163125: Epoch [222/1000] [ 80/183], total loss: 0.04310, regularization loss: 0.29019, contrastive loss: 0.04310, Loss positive: 0.04236, Loss negative: 0.00074
2018-10-22 17:23:10.403524: Epoch [222/1000] [100/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 17:23:20.743028: Epoch [222/1000] [120/183], total loss: 0.00041, regularization loss: 0.29019, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 17:23:31.096294: Epoch [222/1000] [140/183], total loss: 0.00138, regularization loss: 0.29019, contrastive loss: 0.00138, Loss positive: 0.00000, Loss negative: 0.00138
2018-10-22 17:23:41.423403: Epoch [222/1000] [160/183], total loss: 0.00550, regularization loss: 0.29019, contrastive loss: 0.00550, Loss positive: 0.00532, Loss negative: 0.00018
2018-10-22 17:23:51.877076: Epoch [222/1000] [180/183], total loss: 0.00074, regularization loss: 0.29019, contrastive loss: 0.00074, Loss positive: 0.00000, Loss negative: 0.00074
2018-10-22 17:24:16.406427: Epoch [223/1000] [ 20/183], total loss: 0.00677, regularization loss: 0.29019, contrastive loss: 0.00677, Loss positive: 0.00667, Loss negative: 0.00010
2018-10-22 17:24:26.643131: Epoch [223/1000] [ 40/183], total loss: 0.00665, regularization loss: 0.29019, contrastive loss: 0.00665, Loss positive: 0.00642, Loss negative: 0.00023
2018-10-22 17:24:36.873141: Epoch [223/1000] [ 60/183], total loss: 0.00230, regularization loss: 0.29019, contrastive loss: 0.00230, Loss positive: 0.00000, Loss negative: 0.00230
2018-10-22 17:24:47.146408: Epoch [223/1000] [ 80/183], total loss: 0.01014, regularization loss: 0.29019, contrastive loss: 0.01014, Loss positive: 0.00000, Loss negative: 0.01014
2018-10-22 17:24:57.446250: Epoch [223/1000] [100/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 17:25:07.767364: Epoch [223/1000] [120/183], total loss: 0.00011, regularization loss: 0.29019, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 17:25:18.196319: Epoch [223/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:25:28.422937: Epoch [223/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:25:38.761205: Epoch [223/1000] [180/183], total loss: 0.00045, regularization loss: 0.29019, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 17:26:01.495655: Epoch [224/1000] [ 20/183], total loss: 0.01301, regularization loss: 0.29019, contrastive loss: 0.01301, Loss positive: 0.01017, Loss negative: 0.00284
2018-10-22 17:26:11.695899: Epoch [224/1000] [ 40/183], total loss: 0.00036, regularization loss: 0.29019, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 17:26:21.862587: Epoch [224/1000] [ 60/183], total loss: 0.00235, regularization loss: 0.29019, contrastive loss: 0.00235, Loss positive: 0.00000, Loss negative: 0.00235
2018-10-22 17:26:32.168451: Epoch [224/1000] [ 80/183], total loss: 0.00062, regularization loss: 0.29019, contrastive loss: 0.00062, Loss positive: 0.00000, Loss negative: 0.00062
2018-10-22 17:26:42.582631: Epoch [224/1000] [100/183], total loss: 0.00309, regularization loss: 0.29019, contrastive loss: 0.00309, Loss positive: 0.00000, Loss negative: 0.00309
2018-10-22 17:26:52.811766: Epoch [224/1000] [120/183], total loss: 0.03570, regularization loss: 0.29019, contrastive loss: 0.03570, Loss positive: 0.03570, Loss negative: 0.00000
2018-10-22 17:27:03.055185: Epoch [224/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:27:13.323011: Epoch [224/1000] [160/183], total loss: 0.00174, regularization loss: 0.29019, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 17:27:23.577637: Epoch [224/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:27:45.163870: Epoch [225/1000] [ 20/183], total loss: 0.04511, regularization loss: 0.29019, contrastive loss: 0.04511, Loss positive: 0.03980, Loss negative: 0.00531
2018-10-22 17:27:55.345286: Epoch [225/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:28:05.548359: Epoch [225/1000] [ 60/183], total loss: 0.02146, regularization loss: 0.29019, contrastive loss: 0.02146, Loss positive: 0.02085, Loss negative: 0.00061
2018-10-22 17:28:15.784085: Epoch [225/1000] [ 80/183], total loss: 0.00133, regularization loss: 0.29019, contrastive loss: 0.00133, Loss positive: 0.00000, Loss negative: 0.00133
2018-10-22 17:28:26.002595: Epoch [225/1000] [100/183], total loss: 0.00019, regularization loss: 0.29019, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 17:28:36.221980: Epoch [225/1000] [120/183], total loss: 0.05073, regularization loss: 0.29019, contrastive loss: 0.05073, Loss positive: 0.04737, Loss negative: 0.00336
2018-10-22 17:28:46.467203: Epoch [225/1000] [140/183], total loss: 0.01739, regularization loss: 0.29019, contrastive loss: 0.01739, Loss positive: 0.01584, Loss negative: 0.00155
2018-10-22 17:28:56.706242: Epoch [225/1000] [160/183], total loss: 0.00097, regularization loss: 0.29019, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 17:29:06.939473: Epoch [225/1000] [180/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 17:29:28.415608: Epoch [226/1000] [ 20/183], total loss: 0.01470, regularization loss: 0.29019, contrastive loss: 0.01470, Loss positive: 0.01274, Loss negative: 0.00196
2018-10-22 17:29:38.636444: Epoch [226/1000] [ 40/183], total loss: 0.00075, regularization loss: 0.29019, contrastive loss: 0.00075, Loss positive: 0.00000, Loss negative: 0.00075
2018-10-22 17:29:48.862105: Epoch [226/1000] [ 60/183], total loss: 0.00692, regularization loss: 0.29019, contrastive loss: 0.00692, Loss positive: 0.00537, Loss negative: 0.00156
2018-10-22 17:29:59.067144: Epoch [226/1000] [ 80/183], total loss: 0.00252, regularization loss: 0.29019, contrastive loss: 0.00252, Loss positive: 0.00000, Loss negative: 0.00252
2018-10-22 17:30:09.307696: Epoch [226/1000] [100/183], total loss: 0.01545, regularization loss: 0.29019, contrastive loss: 0.01545, Loss positive: 0.01377, Loss negative: 0.00168
2018-10-22 17:30:19.572947: Epoch [226/1000] [120/183], total loss: 0.00341, regularization loss: 0.29019, contrastive loss: 0.00341, Loss positive: 0.00000, Loss negative: 0.00341
2018-10-22 17:30:29.801895: Epoch [226/1000] [140/183], total loss: 0.00446, regularization loss: 0.29019, contrastive loss: 0.00446, Loss positive: 0.00000, Loss negative: 0.00446
2018-10-22 17:30:40.033181: Epoch [226/1000] [160/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 17:30:50.266348: Epoch [226/1000] [180/183], total loss: 0.00643, regularization loss: 0.29019, contrastive loss: 0.00643, Loss positive: 0.00000, Loss negative: 0.00643
2018-10-22 17:31:11.865204: Epoch [227/1000] [ 20/183], total loss: 0.01419, regularization loss: 0.29019, contrastive loss: 0.01419, Loss positive: 0.00765, Loss negative: 0.00654
2018-10-22 17:31:22.047398: Epoch [227/1000] [ 40/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 17:31:32.261655: Epoch [227/1000] [ 60/183], total loss: 0.00091, regularization loss: 0.29019, contrastive loss: 0.00091, Loss positive: 0.00000, Loss negative: 0.00091
2018-10-22 17:31:42.626629: Epoch [227/1000] [ 80/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 17:31:53.103385: Epoch [227/1000] [100/183], total loss: 0.00043, regularization loss: 0.29019, contrastive loss: 0.00043, Loss positive: 0.00000, Loss negative: 0.00043
2018-10-22 17:32:03.418688: Epoch [227/1000] [120/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 17:32:13.824929: Epoch [227/1000] [140/183], total loss: 0.00256, regularization loss: 0.29019, contrastive loss: 0.00256, Loss positive: 0.00000, Loss negative: 0.00256
2018-10-22 17:32:24.093046: Epoch [227/1000] [160/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 17:32:34.568513: Epoch [227/1000] [180/183], total loss: 0.02455, regularization loss: 0.29019, contrastive loss: 0.02455, Loss positive: 0.02196, Loss negative: 0.00258
2018-10-22 17:32:59.083679: Epoch [228/1000] [ 20/183], total loss: 0.01937, regularization loss: 0.29019, contrastive loss: 0.01937, Loss positive: 0.01937, Loss negative: 0.00000
2018-10-22 17:33:09.302390: Epoch [228/1000] [ 40/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 17:33:19.532092: Epoch [228/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:33:29.812516: Epoch [228/1000] [ 80/183], total loss: 0.00113, regularization loss: 0.29019, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 17:33:40.304134: Epoch [228/1000] [100/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 17:33:50.730132: Epoch [228/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:34:01.089369: Epoch [228/1000] [140/183], total loss: 0.00180, regularization loss: 0.29019, contrastive loss: 0.00180, Loss positive: 0.00000, Loss negative: 0.00180
2018-10-22 17:34:11.420019: Epoch [228/1000] [160/183], total loss: 0.00019, regularization loss: 0.29019, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 17:34:21.886651: Epoch [228/1000] [180/183], total loss: 0.01101, regularization loss: 0.29019, contrastive loss: 0.01101, Loss positive: 0.01101, Loss negative: 0.00000
2018-10-22 17:34:46.445154: Epoch [229/1000] [ 20/183], total loss: 0.00023, regularization loss: 0.29019, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 17:34:56.683902: Epoch [229/1000] [ 40/183], total loss: 0.01856, regularization loss: 0.29019, contrastive loss: 0.01856, Loss positive: 0.01037, Loss negative: 0.00819
2018-10-22 17:35:06.955961: Epoch [229/1000] [ 60/183], total loss: 0.00635, regularization loss: 0.29019, contrastive loss: 0.00635, Loss positive: 0.00000, Loss negative: 0.00635
2018-10-22 17:35:17.182475: Epoch [229/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:35:27.624340: Epoch [229/1000] [100/183], total loss: 0.00042, regularization loss: 0.29019, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 17:35:37.869260: Epoch [229/1000] [120/183], total loss: 0.03296, regularization loss: 0.29019, contrastive loss: 0.03296, Loss positive: 0.01868, Loss negative: 0.01428
2018-10-22 17:35:48.180411: Epoch [229/1000] [140/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 17:35:58.434259: Epoch [229/1000] [160/183], total loss: 0.01936, regularization loss: 0.29019, contrastive loss: 0.01936, Loss positive: 0.01936, Loss negative: 0.00000
2018-10-22 17:36:08.743000: Epoch [229/1000] [180/183], total loss: 0.00136, regularization loss: 0.29019, contrastive loss: 0.00136, Loss positive: 0.00000, Loss negative: 0.00136
2018-10-22 17:36:31.329753: Epoch [230/1000] [ 20/183], total loss: 0.04727, regularization loss: 0.29019, contrastive loss: 0.04727, Loss positive: 0.04725, Loss negative: 0.00003
2018-10-22 17:36:41.472528: Epoch [230/1000] [ 40/183], total loss: 0.00660, regularization loss: 0.29019, contrastive loss: 0.00660, Loss positive: 0.00000, Loss negative: 0.00660
2018-10-22 17:36:51.656369: Epoch [230/1000] [ 60/183], total loss: 0.00031, regularization loss: 0.29019, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 17:37:01.894638: Epoch [230/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:37:12.134426: Epoch [230/1000] [100/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 17:37:22.390651: Epoch [230/1000] [120/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 17:37:32.628791: Epoch [230/1000] [140/183], total loss: 0.01234, regularization loss: 0.29019, contrastive loss: 0.01234, Loss positive: 0.01145, Loss negative: 0.00089
2018-10-22 17:37:42.863440: Epoch [230/1000] [160/183], total loss: 0.02364, regularization loss: 0.29019, contrastive loss: 0.02364, Loss positive: 0.02247, Loss negative: 0.00117
2018-10-22 17:37:53.085269: Epoch [230/1000] [180/183], total loss: 0.02112, regularization loss: 0.29019, contrastive loss: 0.02112, Loss positive: 0.01874, Loss negative: 0.00237
Recall@1: 0.25608
Recall@2: 0.38184
Recall@4: 0.51215
Recall@8: 0.64956
Recall@16: 0.77616
Recall@32: 0.87373
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 60.  62.  52.  77.  39.  34.  62.  74.  50.  43.  68.  26.  96.  70.
  69.  41.  54.  66.  50.  47.  58.  53.  51.  83.  71.  19.  64.  39.
  45.  30.  42.  72.  52.  75.  59.  91. 101.  57.  63.  73.  99.  98.
  77.  26.  52.  59.  55.  63.  67.  51.  72.  72.  39.  50.  67.  77.
  41.  64.  38.  70.  70.  37.  65.  59.  54.  76.  70.  58.  41.  94.
  42.  35.  63.  54.  41.  76.  66.  89.  35.  36.  71.  34.  36.  63.
  57.  39.  90.  85.  72.  61.  57.  43.  46.  52.  56.  73.  81.  61.
  61.  50.]
Purity is 0.240
count_cross = [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 1. 0.]
 [0. 1. 0. ... 2. 1. 0.]
 ...
 [0. 3. 0. ... 4. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]]
Mutual information is 2.06882
5924.0
5924
Entropy cluster is 4.56033
Entropy class is 4.60444
normalized_mutual_information is 0.45147
tp_and_fp = 187932.0
tp = 20386.0
fp is 167546.0
fn is 152364.0
RI is 0.9817651989640175
Precision is 0.10847540599791414
Recall is 0.11800868306801737
F_1 is 0.1130414048940618

normalized_mutual_information = 0.45147138982718416
RI = 0.9817651989640175
F_1 = 0.1130414048940618

The NN is 0.25608
The FT is 0.14075
The ST is 0.22007
The DCG is 0.52675
The E is 0.11864
The MAP 0.11483

2018-10-22 17:39:23.251552: Epoch [231/1000] [ 20/183], total loss: 0.00094, regularization loss: 0.29019, contrastive loss: 0.00094, Loss positive: 0.00000, Loss negative: 0.00094
2018-10-22 17:39:33.343586: Epoch [231/1000] [ 40/183], total loss: 0.01415, regularization loss: 0.29019, contrastive loss: 0.01415, Loss positive: 0.01391, Loss negative: 0.00023
2018-10-22 17:39:43.472204: Epoch [231/1000] [ 60/183], total loss: 0.02733, regularization loss: 0.29019, contrastive loss: 0.02733, Loss positive: 0.01880, Loss negative: 0.00853
2018-10-22 17:39:53.579759: Epoch [231/1000] [ 80/183], total loss: 0.00039, regularization loss: 0.29019, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 17:40:03.773185: Epoch [231/1000] [100/183], total loss: 0.00463, regularization loss: 0.29019, contrastive loss: 0.00463, Loss positive: 0.00000, Loss negative: 0.00463
2018-10-22 17:40:13.951901: Epoch [231/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:40:24.336528: Epoch [231/1000] [140/183], total loss: 0.00187, regularization loss: 0.29019, contrastive loss: 0.00187, Loss positive: 0.00000, Loss negative: 0.00187
2018-10-22 17:40:34.598339: Epoch [231/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:40:44.824928: Epoch [231/1000] [180/183], total loss: 0.00086, regularization loss: 0.29019, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 17:41:06.681169: Epoch [232/1000] [ 20/183], total loss: 0.00552, regularization loss: 0.29019, contrastive loss: 0.00552, Loss positive: 0.00000, Loss negative: 0.00552
2018-10-22 17:41:16.832428: Epoch [232/1000] [ 40/183], total loss: 0.03106, regularization loss: 0.29019, contrastive loss: 0.03106, Loss positive: 0.03106, Loss negative: 0.00000
2018-10-22 17:41:27.001112: Epoch [232/1000] [ 60/183], total loss: 0.01695, regularization loss: 0.29019, contrastive loss: 0.01695, Loss positive: 0.01164, Loss negative: 0.00531
2018-10-22 17:41:37.387262: Epoch [232/1000] [ 80/183], total loss: 0.03140, regularization loss: 0.29019, contrastive loss: 0.03140, Loss positive: 0.02512, Loss negative: 0.00628
2018-10-22 17:41:47.641299: Epoch [232/1000] [100/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 17:41:57.966480: Epoch [232/1000] [120/183], total loss: 0.00434, regularization loss: 0.29019, contrastive loss: 0.00434, Loss positive: 0.00000, Loss negative: 0.00434
2018-10-22 17:42:08.221727: Epoch [232/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:42:18.672290: Epoch [232/1000] [160/183], total loss: 0.01675, regularization loss: 0.29019, contrastive loss: 0.01675, Loss positive: 0.01675, Loss negative: 0.00000
2018-10-22 17:42:29.340256: Epoch [232/1000] [180/183], total loss: 0.01394, regularization loss: 0.29019, contrastive loss: 0.01394, Loss positive: 0.01391, Loss negative: 0.00004
2018-10-22 17:42:53.258243: Epoch [233/1000] [ 20/183], total loss: 0.00961, regularization loss: 0.29019, contrastive loss: 0.00961, Loss positive: 0.00672, Loss negative: 0.00288
2018-10-22 17:43:03.490333: Epoch [233/1000] [ 40/183], total loss: 0.00357, regularization loss: 0.29019, contrastive loss: 0.00357, Loss positive: 0.00000, Loss negative: 0.00357
2018-10-22 17:43:13.729899: Epoch [233/1000] [ 60/183], total loss: 0.02591, regularization loss: 0.29019, contrastive loss: 0.02591, Loss positive: 0.02575, Loss negative: 0.00016
2018-10-22 17:43:23.965579: Epoch [233/1000] [ 80/183], total loss: 0.00138, regularization loss: 0.29019, contrastive loss: 0.00138, Loss positive: 0.00000, Loss negative: 0.00138
2018-10-22 17:43:34.215773: Epoch [233/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:43:44.468680: Epoch [233/1000] [120/183], total loss: 0.00382, regularization loss: 0.29019, contrastive loss: 0.00382, Loss positive: 0.00000, Loss negative: 0.00382
2018-10-22 17:43:54.915696: Epoch [233/1000] [140/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 17:44:05.275695: Epoch [233/1000] [160/183], total loss: 0.00786, regularization loss: 0.29019, contrastive loss: 0.00786, Loss positive: 0.00703, Loss negative: 0.00083
2018-10-22 17:44:15.902276: Epoch [233/1000] [180/183], total loss: 0.00417, regularization loss: 0.29019, contrastive loss: 0.00417, Loss positive: 0.00000, Loss negative: 0.00417
2018-10-22 17:44:40.751381: Epoch [234/1000] [ 20/183], total loss: 0.00107, regularization loss: 0.29019, contrastive loss: 0.00107, Loss positive: 0.00000, Loss negative: 0.00107
2018-10-22 17:44:50.928817: Epoch [234/1000] [ 40/183], total loss: 0.00217, regularization loss: 0.29019, contrastive loss: 0.00217, Loss positive: 0.00000, Loss negative: 0.00217
2018-10-22 17:45:01.165330: Epoch [234/1000] [ 60/183], total loss: 0.00384, regularization loss: 0.29019, contrastive loss: 0.00384, Loss positive: 0.00000, Loss negative: 0.00384
2018-10-22 17:45:11.426575: Epoch [234/1000] [ 80/183], total loss: 0.00968, regularization loss: 0.29019, contrastive loss: 0.00968, Loss positive: 0.00000, Loss negative: 0.00968
2018-10-22 17:45:21.644539: Epoch [234/1000] [100/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 17:45:31.892001: Epoch [234/1000] [120/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 17:45:42.149186: Epoch [234/1000] [140/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 17:45:52.797154: Epoch [234/1000] [160/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 17:46:03.107295: Epoch [234/1000] [180/183], total loss: 0.03247, regularization loss: 0.29019, contrastive loss: 0.03247, Loss positive: 0.03247, Loss negative: 0.00000
2018-10-22 17:46:25.774158: Epoch [235/1000] [ 20/183], total loss: 0.00354, regularization loss: 0.29019, contrastive loss: 0.00354, Loss positive: 0.00000, Loss negative: 0.00354
2018-10-22 17:46:35.925787: Epoch [235/1000] [ 40/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 17:46:46.151636: Epoch [235/1000] [ 60/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 17:46:56.369445: Epoch [235/1000] [ 80/183], total loss: 0.01933, regularization loss: 0.29019, contrastive loss: 0.01933, Loss positive: 0.01933, Loss negative: 0.00000
2018-10-22 17:47:06.583029: Epoch [235/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:47:16.811155: Epoch [235/1000] [120/183], total loss: 0.01025, regularization loss: 0.29019, contrastive loss: 0.01025, Loss positive: 0.00974, Loss negative: 0.00051
2018-10-22 17:47:27.054571: Epoch [235/1000] [140/183], total loss: 0.02005, regularization loss: 0.29019, contrastive loss: 0.02005, Loss positive: 0.02005, Loss negative: 0.00000
2018-10-22 17:47:37.295859: Epoch [235/1000] [160/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 17:47:47.553883: Epoch [235/1000] [180/183], total loss: 0.00189, regularization loss: 0.29019, contrastive loss: 0.00189, Loss positive: 0.00000, Loss negative: 0.00189
2018-10-22 17:48:09.176187: Epoch [236/1000] [ 20/183], total loss: 0.00054, regularization loss: 0.29019, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 17:48:19.367535: Epoch [236/1000] [ 40/183], total loss: 0.04693, regularization loss: 0.29019, contrastive loss: 0.04693, Loss positive: 0.04655, Loss negative: 0.00038
2018-10-22 17:48:29.568413: Epoch [236/1000] [ 60/183], total loss: 0.00050, regularization loss: 0.29019, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 17:48:39.796971: Epoch [236/1000] [ 80/183], total loss: 0.00098, regularization loss: 0.29019, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 17:48:50.033668: Epoch [236/1000] [100/183], total loss: 0.00289, regularization loss: 0.29019, contrastive loss: 0.00289, Loss positive: 0.00000, Loss negative: 0.00289
2018-10-22 17:49:00.292769: Epoch [236/1000] [120/183], total loss: 0.00009, regularization loss: 0.29019, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 17:49:10.524876: Epoch [236/1000] [140/183], total loss: 0.02282, regularization loss: 0.29019, contrastive loss: 0.02282, Loss positive: 0.02278, Loss negative: 0.00004
2018-10-22 17:49:21.103904: Epoch [236/1000] [160/183], total loss: 0.05681, regularization loss: 0.29019, contrastive loss: 0.05681, Loss positive: 0.05169, Loss negative: 0.00512
2018-10-22 17:49:31.483625: Epoch [236/1000] [180/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 17:49:52.980507: Epoch [237/1000] [ 20/183], total loss: 0.02768, regularization loss: 0.29019, contrastive loss: 0.02768, Loss positive: 0.02712, Loss negative: 0.00056
2018-10-22 17:50:03.177032: Epoch [237/1000] [ 40/183], total loss: 0.04382, regularization loss: 0.29019, contrastive loss: 0.04382, Loss positive: 0.04382, Loss negative: 0.00000
2018-10-22 17:50:13.381558: Epoch [237/1000] [ 60/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 17:50:23.720431: Epoch [237/1000] [ 80/183], total loss: 0.00109, regularization loss: 0.29019, contrastive loss: 0.00109, Loss positive: 0.00000, Loss negative: 0.00109
2018-10-22 17:50:33.969004: Epoch [237/1000] [100/183], total loss: 0.01388, regularization loss: 0.29019, contrastive loss: 0.01388, Loss positive: 0.01383, Loss negative: 0.00005
2018-10-22 17:50:44.192166: Epoch [237/1000] [120/183], total loss: 0.00082, regularization loss: 0.29019, contrastive loss: 0.00082, Loss positive: 0.00000, Loss negative: 0.00082
2018-10-22 17:50:54.438905: Epoch [237/1000] [140/183], total loss: 0.00182, regularization loss: 0.29019, contrastive loss: 0.00182, Loss positive: 0.00000, Loss negative: 0.00182
2018-10-22 17:51:04.686559: Epoch [237/1000] [160/183], total loss: 0.00785, regularization loss: 0.29019, contrastive loss: 0.00785, Loss positive: 0.00000, Loss negative: 0.00785
2018-10-22 17:51:14.917232: Epoch [237/1000] [180/183], total loss: 0.00280, regularization loss: 0.29019, contrastive loss: 0.00280, Loss positive: 0.00000, Loss negative: 0.00280
2018-10-22 17:51:37.816341: Epoch [238/1000] [ 20/183], total loss: 0.00666, regularization loss: 0.29019, contrastive loss: 0.00666, Loss positive: 0.00000, Loss negative: 0.00666
2018-10-22 17:51:48.051142: Epoch [238/1000] [ 40/183], total loss: 0.03450, regularization loss: 0.29019, contrastive loss: 0.03450, Loss positive: 0.02688, Loss negative: 0.00763
2018-10-22 17:51:58.306396: Epoch [238/1000] [ 60/183], total loss: 0.00047, regularization loss: 0.29019, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 17:52:08.732074: Epoch [238/1000] [ 80/183], total loss: 0.00324, regularization loss: 0.29019, contrastive loss: 0.00324, Loss positive: 0.00000, Loss negative: 0.00324
2018-10-22 17:52:19.158919: Epoch [238/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:52:29.405249: Epoch [238/1000] [120/183], total loss: 0.05970, regularization loss: 0.29019, contrastive loss: 0.05970, Loss positive: 0.05965, Loss negative: 0.00005
2018-10-22 17:52:39.729100: Epoch [238/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:52:50.351212: Epoch [238/1000] [160/183], total loss: 0.00201, regularization loss: 0.29019, contrastive loss: 0.00201, Loss positive: 0.00000, Loss negative: 0.00201
2018-10-22 17:53:00.835327: Epoch [238/1000] [180/183], total loss: 0.00152, regularization loss: 0.29019, contrastive loss: 0.00152, Loss positive: 0.00000, Loss negative: 0.00152
2018-10-22 17:53:25.306920: Epoch [239/1000] [ 20/183], total loss: 0.04484, regularization loss: 0.29019, contrastive loss: 0.04484, Loss positive: 0.03819, Loss negative: 0.00664
2018-10-22 17:53:35.533954: Epoch [239/1000] [ 40/183], total loss: 0.05837, regularization loss: 0.29019, contrastive loss: 0.05837, Loss positive: 0.04925, Loss negative: 0.00912
2018-10-22 17:53:45.800875: Epoch [239/1000] [ 60/183], total loss: 0.01040, regularization loss: 0.29019, contrastive loss: 0.01040, Loss positive: 0.00901, Loss negative: 0.00139
2018-10-22 17:53:56.043731: Epoch [239/1000] [ 80/183], total loss: 0.00066, regularization loss: 0.29019, contrastive loss: 0.00066, Loss positive: 0.00000, Loss negative: 0.00066
2018-10-22 17:54:06.544347: Epoch [239/1000] [100/183], total loss: 0.00838, regularization loss: 0.29019, contrastive loss: 0.00838, Loss positive: 0.00000, Loss negative: 0.00838
2018-10-22 17:54:17.008106: Epoch [239/1000] [120/183], total loss: 0.00084, regularization loss: 0.29019, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 17:54:27.420649: Epoch [239/1000] [140/183], total loss: 0.00866, regularization loss: 0.29019, contrastive loss: 0.00866, Loss positive: 0.00566, Loss negative: 0.00300
2018-10-22 17:54:37.711813: Epoch [239/1000] [160/183], total loss: 0.02804, regularization loss: 0.29019, contrastive loss: 0.02804, Loss positive: 0.02415, Loss negative: 0.00390
2018-10-22 17:54:48.239279: Epoch [239/1000] [180/183], total loss: 0.00037, regularization loss: 0.29019, contrastive loss: 0.00037, Loss positive: 0.00000, Loss negative: 0.00037
2018-10-22 17:55:12.859070: Epoch [240/1000] [ 20/183], total loss: 0.00081, regularization loss: 0.29019, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 17:55:23.011882: Epoch [240/1000] [ 40/183], total loss: 0.00204, regularization loss: 0.29019, contrastive loss: 0.00204, Loss positive: 0.00000, Loss negative: 0.00204
2018-10-22 17:55:33.206028: Epoch [240/1000] [ 60/183], total loss: 0.00046, regularization loss: 0.29019, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 17:55:43.423192: Epoch [240/1000] [ 80/183], total loss: 0.00117, regularization loss: 0.29019, contrastive loss: 0.00117, Loss positive: 0.00000, Loss negative: 0.00117
2018-10-22 17:55:53.674536: Epoch [240/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 17:56:03.931775: Epoch [240/1000] [120/183], total loss: 0.00530, regularization loss: 0.29019, contrastive loss: 0.00530, Loss positive: 0.00000, Loss negative: 0.00530
2018-10-22 17:56:14.147217: Epoch [240/1000] [140/183], total loss: 0.01730, regularization loss: 0.29019, contrastive loss: 0.01730, Loss positive: 0.01701, Loss negative: 0.00030
2018-10-22 17:56:24.397005: Epoch [240/1000] [160/183], total loss: 0.02039, regularization loss: 0.29019, contrastive loss: 0.02039, Loss positive: 0.01993, Loss negative: 0.00047
2018-10-22 17:56:34.655894: Epoch [240/1000] [180/183], total loss: 0.01015, regularization loss: 0.29019, contrastive loss: 0.01015, Loss positive: 0.00000, Loss negative: 0.01015
Recall@1: 0.26182
Recall@2: 0.37880
Recall@4: 0.50574
Recall@8: 0.64315
Recall@16: 0.76992
Recall@32: 0.87745
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 59.  74.  60.  88.  35.  90.  70.  93.  70.  88.  74.  77. 117.  45.
  53.  70.  18.  15.  37.  42.  42.  57.  41.  68.  67.  63.  56.  89.
  70.  51.  86.  44. 112.  46.  40.  90.  53.  29.  39.  70.  81.  34.
  59.  71.  38.  68.  22.  30.  54.  60.  45.  61.  74.  59.  59.  69.
  99.  43.  65.  66.  63.  64.  86.  59.  53.  79.  61.  43.  43.  45.
  86.  44.  92.  56.  13.  81.  48.  29.  72.  46.  38.  54.  83.  44.
  44.  49.  41.  59.  67.  57.  35.  50.  38.  75.  73.  53.  90.  32.
  62.  72.]
Purity is 0.239
count_cross = [[ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  5.  4. ...  4.  0.  0.]
 [ 0.  2.  5. ...  8.  1.  0.]
 ...
 [ 1.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  5.]
 [ 0. 11.  6. ...  4.  2.  0.]]
Mutual information is 2.03307
5924.0
5924
Entropy cluster is 4.54325
Entropy class is 4.60444
normalized_mutual_information is 0.44450
tp_and_fp = 193467.0
tp = 19812.0
fp is 173655.0
fn is 152938.0
RI is 0.9813842694046931
Precision is 0.10240506132828854
Recall is 0.11468596237337192
F_1 is 0.10819814481577862

normalized_mutual_information = 0.4444980180935368
RI = 0.9813842694046931
F_1 = 0.10819814481577862

The NN is 0.26182
The FT is 0.13822
The ST is 0.21505
The DCG is 0.52300
The E is 0.11668
The MAP 0.11236

2018-10-22 17:58:04.062409: Epoch [241/1000] [ 20/183], total loss: 0.03011, regularization loss: 0.29019, contrastive loss: 0.03011, Loss positive: 0.02980, Loss negative: 0.00031
2018-10-22 17:58:14.179371: Epoch [241/1000] [ 40/183], total loss: 0.00003, regularization loss: 0.29019, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 17:58:24.316962: Epoch [241/1000] [ 60/183], total loss: 0.02644, regularization loss: 0.29019, contrastive loss: 0.02644, Loss positive: 0.02478, Loss negative: 0.00166
2018-10-22 17:58:34.484039: Epoch [241/1000] [ 80/183], total loss: 0.00141, regularization loss: 0.29019, contrastive loss: 0.00141, Loss positive: 0.00000, Loss negative: 0.00141
2018-10-22 17:58:44.654991: Epoch [241/1000] [100/183], total loss: 0.02004, regularization loss: 0.29019, contrastive loss: 0.02004, Loss positive: 0.02004, Loss negative: 0.00000
2018-10-22 17:58:54.858602: Epoch [241/1000] [120/183], total loss: 0.00799, regularization loss: 0.29019, contrastive loss: 0.00799, Loss positive: 0.00000, Loss negative: 0.00799
2018-10-22 17:59:05.161465: Epoch [241/1000] [140/183], total loss: 0.01494, regularization loss: 0.29019, contrastive loss: 0.01494, Loss positive: 0.00000, Loss negative: 0.01494
2018-10-22 17:59:15.383813: Epoch [241/1000] [160/183], total loss: 0.00028, regularization loss: 0.29019, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 17:59:25.615218: Epoch [241/1000] [180/183], total loss: 0.00425, regularization loss: 0.29020, contrastive loss: 0.00425, Loss positive: 0.00000, Loss negative: 0.00425
2018-10-22 17:59:47.100673: Epoch [242/1000] [ 20/183], total loss: 0.01676, regularization loss: 0.29020, contrastive loss: 0.01676, Loss positive: 0.01164, Loss negative: 0.00512
2018-10-22 17:59:57.295987: Epoch [242/1000] [ 40/183], total loss: 0.01007, regularization loss: 0.29020, contrastive loss: 0.01007, Loss positive: 0.00898, Loss negative: 0.00109
2018-10-22 18:00:07.491073: Epoch [242/1000] [ 60/183], total loss: 0.00089, regularization loss: 0.29020, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 18:00:17.851005: Epoch [242/1000] [ 80/183], total loss: 0.00274, regularization loss: 0.29020, contrastive loss: 0.00274, Loss positive: 0.00000, Loss negative: 0.00274
2018-10-22 18:00:28.336280: Epoch [242/1000] [100/183], total loss: 0.04699, regularization loss: 0.29020, contrastive loss: 0.04699, Loss positive: 0.04694, Loss negative: 0.00004
2018-10-22 18:00:38.536478: Epoch [242/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 18:00:48.782002: Epoch [242/1000] [140/183], total loss: 0.00227, regularization loss: 0.29019, contrastive loss: 0.00227, Loss positive: 0.00000, Loss negative: 0.00227
2018-10-22 18:00:59.013211: Epoch [242/1000] [160/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 18:01:09.246811: Epoch [242/1000] [180/183], total loss: 0.03117, regularization loss: 0.29019, contrastive loss: 0.03117, Loss positive: 0.01665, Loss negative: 0.01452
2018-10-22 18:01:31.588617: Epoch [243/1000] [ 20/183], total loss: 0.00329, regularization loss: 0.29020, contrastive loss: 0.00329, Loss positive: 0.00000, Loss negative: 0.00329
2018-10-22 18:01:41.791304: Epoch [243/1000] [ 40/183], total loss: 0.00523, regularization loss: 0.29020, contrastive loss: 0.00523, Loss positive: 0.00000, Loss negative: 0.00523
2018-10-22 18:01:51.981210: Epoch [243/1000] [ 60/183], total loss: 0.00016, regularization loss: 0.29020, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 18:02:02.210898: Epoch [243/1000] [ 80/183], total loss: 0.00239, regularization loss: 0.29020, contrastive loss: 0.00239, Loss positive: 0.00218, Loss negative: 0.00021
2018-10-22 18:02:12.483478: Epoch [243/1000] [100/183], total loss: 0.00562, regularization loss: 0.29020, contrastive loss: 0.00562, Loss positive: 0.00000, Loss negative: 0.00562
2018-10-22 18:02:22.802766: Epoch [243/1000] [120/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 18:02:33.076288: Epoch [243/1000] [140/183], total loss: 0.01469, regularization loss: 0.29020, contrastive loss: 0.01469, Loss positive: 0.01469, Loss negative: 0.00000
2018-10-22 18:02:43.643580: Epoch [243/1000] [160/183], total loss: 0.00372, regularization loss: 0.29019, contrastive loss: 0.00372, Loss positive: 0.00000, Loss negative: 0.00372
2018-10-22 18:02:54.062212: Epoch [243/1000] [180/183], total loss: 0.01904, regularization loss: 0.29019, contrastive loss: 0.01904, Loss positive: 0.01904, Loss negative: 0.00000
2018-10-22 18:03:19.090490: Epoch [244/1000] [ 20/183], total loss: 0.00592, regularization loss: 0.29019, contrastive loss: 0.00592, Loss positive: 0.00000, Loss negative: 0.00592
2018-10-22 18:03:29.350305: Epoch [244/1000] [ 40/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 18:03:39.659030: Epoch [244/1000] [ 60/183], total loss: 0.00745, regularization loss: 0.29019, contrastive loss: 0.00745, Loss positive: 0.00745, Loss negative: 0.00000
2018-10-22 18:03:49.950728: Epoch [244/1000] [ 80/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 18:04:00.473859: Epoch [244/1000] [100/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 18:04:10.723960: Epoch [244/1000] [120/183], total loss: 0.00272, regularization loss: 0.29019, contrastive loss: 0.00272, Loss positive: 0.00000, Loss negative: 0.00272
2018-10-22 18:04:21.073837: Epoch [244/1000] [140/183], total loss: 0.00170, regularization loss: 0.29019, contrastive loss: 0.00170, Loss positive: 0.00000, Loss negative: 0.00170
2018-10-22 18:04:31.417238: Epoch [244/1000] [160/183], total loss: 0.01941, regularization loss: 0.29019, contrastive loss: 0.01941, Loss positive: 0.01914, Loss negative: 0.00026
2018-10-22 18:04:41.752419: Epoch [244/1000] [180/183], total loss: 0.00252, regularization loss: 0.29019, contrastive loss: 0.00252, Loss positive: 0.00000, Loss negative: 0.00252
2018-10-22 18:05:06.262388: Epoch [245/1000] [ 20/183], total loss: 0.00516, regularization loss: 0.29019, contrastive loss: 0.00516, Loss positive: 0.00000, Loss negative: 0.00516
2018-10-22 18:05:16.517121: Epoch [245/1000] [ 40/183], total loss: 0.00024, regularization loss: 0.29019, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 18:05:26.707893: Epoch [245/1000] [ 60/183], total loss: 0.02458, regularization loss: 0.29019, contrastive loss: 0.02458, Loss positive: 0.02311, Loss negative: 0.00147
2018-10-22 18:05:36.893535: Epoch [245/1000] [ 80/183], total loss: 0.00617, regularization loss: 0.29019, contrastive loss: 0.00617, Loss positive: 0.00000, Loss negative: 0.00617
2018-10-22 18:05:47.403017: Epoch [245/1000] [100/183], total loss: 0.00406, regularization loss: 0.29019, contrastive loss: 0.00406, Loss positive: 0.00000, Loss negative: 0.00406
2018-10-22 18:05:57.648923: Epoch [245/1000] [120/183], total loss: 0.00656, regularization loss: 0.29019, contrastive loss: 0.00656, Loss positive: 0.00000, Loss negative: 0.00656
2018-10-22 18:06:07.913932: Epoch [245/1000] [140/183], total loss: 0.00542, regularization loss: 0.29020, contrastive loss: 0.00542, Loss positive: 0.00000, Loss negative: 0.00542
2018-10-22 18:06:18.224191: Epoch [245/1000] [160/183], total loss: 0.00060, regularization loss: 0.29020, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 18:06:28.442175: Epoch [245/1000] [180/183], total loss: 0.00095, regularization loss: 0.29020, contrastive loss: 0.00095, Loss positive: 0.00000, Loss negative: 0.00095
2018-10-22 18:06:50.011579: Epoch [246/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:07:00.170792: Epoch [246/1000] [ 40/183], total loss: 0.00743, regularization loss: 0.29020, contrastive loss: 0.00743, Loss positive: 0.00000, Loss negative: 0.00743
2018-10-22 18:07:10.363326: Epoch [246/1000] [ 60/183], total loss: 0.00053, regularization loss: 0.29020, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 18:07:20.578128: Epoch [246/1000] [ 80/183], total loss: 0.02510, regularization loss: 0.29020, contrastive loss: 0.02510, Loss positive: 0.02486, Loss negative: 0.00024
2018-10-22 18:07:30.817044: Epoch [246/1000] [100/183], total loss: 0.00363, regularization loss: 0.29020, contrastive loss: 0.00363, Loss positive: 0.00000, Loss negative: 0.00363
2018-10-22 18:07:41.182569: Epoch [246/1000] [120/183], total loss: 0.00086, regularization loss: 0.29020, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 18:07:51.501487: Epoch [246/1000] [140/183], total loss: 0.00143, regularization loss: 0.29019, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 18:08:01.750448: Epoch [246/1000] [160/183], total loss: 0.00047, regularization loss: 0.29019, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 18:08:11.991514: Epoch [246/1000] [180/183], total loss: 0.00517, regularization loss: 0.29020, contrastive loss: 0.00517, Loss positive: 0.00000, Loss negative: 0.00517
2018-10-22 18:08:33.536158: Epoch [247/1000] [ 20/183], total loss: 0.00767, regularization loss: 0.29020, contrastive loss: 0.00767, Loss positive: 0.00737, Loss negative: 0.00029
2018-10-22 18:08:43.711028: Epoch [247/1000] [ 40/183], total loss: 0.00204, regularization loss: 0.29020, contrastive loss: 0.00204, Loss positive: 0.00000, Loss negative: 0.00204
2018-10-22 18:08:54.026638: Epoch [247/1000] [ 60/183], total loss: 0.00252, regularization loss: 0.29020, contrastive loss: 0.00252, Loss positive: 0.00000, Loss negative: 0.00252
2018-10-22 18:09:04.376765: Epoch [247/1000] [ 80/183], total loss: 0.00876, regularization loss: 0.29020, contrastive loss: 0.00876, Loss positive: 0.00762, Loss negative: 0.00114
2018-10-22 18:09:14.685522: Epoch [247/1000] [100/183], total loss: 0.02452, regularization loss: 0.29020, contrastive loss: 0.02452, Loss positive: 0.02362, Loss negative: 0.00090
2018-10-22 18:09:25.014752: Epoch [247/1000] [120/183], total loss: 0.01101, regularization loss: 0.29020, contrastive loss: 0.01101, Loss positive: 0.01098, Loss negative: 0.00003
2018-10-22 18:09:35.261226: Epoch [247/1000] [140/183], total loss: 0.00029, regularization loss: 0.29020, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 18:09:45.484914: Epoch [247/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:09:55.712792: Epoch [247/1000] [180/183], total loss: 0.00518, regularization loss: 0.29020, contrastive loss: 0.00518, Loss positive: 0.00000, Loss negative: 0.00518
2018-10-22 18:10:17.219447: Epoch [248/1000] [ 20/183], total loss: 0.01493, regularization loss: 0.29020, contrastive loss: 0.01493, Loss positive: 0.01473, Loss negative: 0.00020
2018-10-22 18:10:27.371368: Epoch [248/1000] [ 40/183], total loss: 0.00809, regularization loss: 0.29020, contrastive loss: 0.00809, Loss positive: 0.00714, Loss negative: 0.00096
2018-10-22 18:10:37.717477: Epoch [248/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:10:47.935323: Epoch [248/1000] [ 80/183], total loss: 0.00131, regularization loss: 0.29019, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 18:10:58.195684: Epoch [248/1000] [100/183], total loss: 0.01654, regularization loss: 0.29019, contrastive loss: 0.01654, Loss positive: 0.01652, Loss negative: 0.00003
2018-10-22 18:11:08.425738: Epoch [248/1000] [120/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 18:11:18.674293: Epoch [248/1000] [140/183], total loss: 0.01801, regularization loss: 0.29019, contrastive loss: 0.01801, Loss positive: 0.01689, Loss negative: 0.00112
2018-10-22 18:11:28.925774: Epoch [248/1000] [160/183], total loss: 0.00084, regularization loss: 0.29019, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 18:11:39.218177: Epoch [248/1000] [180/183], total loss: 0.03371, regularization loss: 0.29019, contrastive loss: 0.03371, Loss positive: 0.03299, Loss negative: 0.00072
2018-10-22 18:12:02.158145: Epoch [249/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:12:12.325508: Epoch [249/1000] [ 40/183], total loss: 0.01996, regularization loss: 0.29019, contrastive loss: 0.01996, Loss positive: 0.01892, Loss negative: 0.00104
2018-10-22 18:12:22.577716: Epoch [249/1000] [ 60/183], total loss: 0.01048, regularization loss: 0.29019, contrastive loss: 0.01048, Loss positive: 0.00976, Loss negative: 0.00072
2018-10-22 18:12:32.862327: Epoch [249/1000] [ 80/183], total loss: 0.01752, regularization loss: 0.29019, contrastive loss: 0.01752, Loss positive: 0.01745, Loss negative: 0.00006
2018-10-22 18:12:43.170661: Epoch [249/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:12:53.711589: Epoch [249/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:13:04.081972: Epoch [249/1000] [140/183], total loss: 0.00059, regularization loss: 0.29020, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 18:13:14.488551: Epoch [249/1000] [160/183], total loss: 0.00172, regularization loss: 0.29020, contrastive loss: 0.00172, Loss positive: 0.00000, Loss negative: 0.00172
2018-10-22 18:13:24.841510: Epoch [249/1000] [180/183], total loss: 0.00492, regularization loss: 0.29019, contrastive loss: 0.00492, Loss positive: 0.00000, Loss negative: 0.00492
2018-10-22 18:13:49.300562: Epoch [250/1000] [ 20/183], total loss: 0.00643, regularization loss: 0.29020, contrastive loss: 0.00643, Loss positive: 0.00594, Loss negative: 0.00050
2018-10-22 18:13:59.521459: Epoch [250/1000] [ 40/183], total loss: 0.05673, regularization loss: 0.29020, contrastive loss: 0.05673, Loss positive: 0.05475, Loss negative: 0.00198
2018-10-22 18:14:09.764977: Epoch [250/1000] [ 60/183], total loss: 0.05207, regularization loss: 0.29019, contrastive loss: 0.05207, Loss positive: 0.04877, Loss negative: 0.00330
2018-10-22 18:14:20.012136: Epoch [250/1000] [ 80/183], total loss: 0.00050, regularization loss: 0.29019, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 18:14:30.387003: Epoch [250/1000] [100/183], total loss: 0.00205, regularization loss: 0.29019, contrastive loss: 0.00205, Loss positive: 0.00000, Loss negative: 0.00205
2018-10-22 18:14:40.719307: Epoch [250/1000] [120/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 18:14:51.350516: Epoch [250/1000] [140/183], total loss: 0.01677, regularization loss: 0.29019, contrastive loss: 0.01677, Loss positive: 0.01394, Loss negative: 0.00282
2018-10-22 18:15:01.728029: Epoch [250/1000] [160/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:15:12.206882: Epoch [250/1000] [180/183], total loss: 0.00778, regularization loss: 0.29019, contrastive loss: 0.00778, Loss positive: 0.00000, Loss negative: 0.00778
Recall@1: 0.26367
Recall@2: 0.38082
Recall@4: 0.51384
Recall@8: 0.65041
Recall@16: 0.77633
Recall@32: 0.87795
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 38.  69.  57.  77.  70.  81.  62.  75.  27.  90.  58.  75.  76.  81.
  52.  66.  24.  37.  87.  38.  49.  69.  51.  66.  42.  49.  74.  57.
  47.  28.  37.  51.  46.  67.  56.  42.  43.  71.  48.  53.  62.  54.
  58.  49.  69.  72.  92.  96.  70.  51.  65.  31.  45.  52.  39.  64.
  54. 103.  74.  44.  61.  51.  48.  49.  61.  47.  67.  62.  58.  39.
  84.  73.  50.  52.  86.  69.  71.  85.  54.  56.  67.  48.  66.  82.
  47.  38.  75.  65.  68.  67.  79.  43.  38.  39.  62.  78.  46.  61.
  70.  32.]
Purity is 0.241
count_cross = [[ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  4.  4. ... 10.  8.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]]
Mutual information is 2.07198
5924.0
5924
Entropy cluster is 4.56662
Entropy class is 4.60444
normalized_mutual_information is 0.45185
tp_and_fp = 185817.0
tp = 20590.0
fp is 165227.0
fn is 152160.0
RI is 0.9819090094201264
Precision is 0.11080794545170786
Recall is 0.11918958031837916
F_1 is 0.11484603993117046

normalized_mutual_information = 0.4518507094265957
RI = 0.9819090094201264
F_1 = 0.11484603993117046

The NN is 0.26367
The FT is 0.14042
The ST is 0.21888
The DCG is 0.52656
The E is 0.11814
The MAP 0.11452

2018-10-22 18:16:45.936227: Epoch [251/1000] [ 20/183], total loss: 0.02995, regularization loss: 0.29019, contrastive loss: 0.02995, Loss positive: 0.02967, Loss negative: 0.00028
2018-10-22 18:16:56.014592: Epoch [251/1000] [ 40/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 18:17:06.118593: Epoch [251/1000] [ 60/183], total loss: 0.00179, regularization loss: 0.29019, contrastive loss: 0.00179, Loss positive: 0.00000, Loss negative: 0.00179
2018-10-22 18:17:16.232278: Epoch [251/1000] [ 80/183], total loss: 0.00158, regularization loss: 0.29019, contrastive loss: 0.00158, Loss positive: 0.00000, Loss negative: 0.00158
2018-10-22 18:17:26.425818: Epoch [251/1000] [100/183], total loss: 0.01294, regularization loss: 0.29019, contrastive loss: 0.01294, Loss positive: 0.01269, Loss negative: 0.00024
2018-10-22 18:17:36.611545: Epoch [251/1000] [120/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 18:17:46.882665: Epoch [251/1000] [140/183], total loss: 0.00238, regularization loss: 0.29019, contrastive loss: 0.00238, Loss positive: 0.00000, Loss negative: 0.00238
2018-10-22 18:17:57.116305: Epoch [251/1000] [160/183], total loss: 0.00226, regularization loss: 0.29020, contrastive loss: 0.00226, Loss positive: 0.00000, Loss negative: 0.00226
2018-10-22 18:18:07.339358: Epoch [251/1000] [180/183], total loss: 0.01317, regularization loss: 0.29020, contrastive loss: 0.01317, Loss positive: 0.01167, Loss negative: 0.00151
2018-10-22 18:18:28.910554: Epoch [252/1000] [ 20/183], total loss: 0.01261, regularization loss: 0.29020, contrastive loss: 0.01261, Loss positive: 0.01086, Loss negative: 0.00174
2018-10-22 18:18:39.106537: Epoch [252/1000] [ 40/183], total loss: 0.00042, regularization loss: 0.29020, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 18:18:49.272448: Epoch [252/1000] [ 60/183], total loss: 0.00049, regularization loss: 0.29019, contrastive loss: 0.00049, Loss positive: 0.00000, Loss negative: 0.00049
2018-10-22 18:18:59.482560: Epoch [252/1000] [ 80/183], total loss: 0.01795, regularization loss: 0.29019, contrastive loss: 0.01795, Loss positive: 0.01160, Loss negative: 0.00635
2018-10-22 18:19:09.707541: Epoch [252/1000] [100/183], total loss: 0.00114, regularization loss: 0.29020, contrastive loss: 0.00114, Loss positive: 0.00000, Loss negative: 0.00114
2018-10-22 18:19:19.956113: Epoch [252/1000] [120/183], total loss: 0.00944, regularization loss: 0.29020, contrastive loss: 0.00944, Loss positive: 0.00824, Loss negative: 0.00119
2018-10-22 18:19:30.217997: Epoch [252/1000] [140/183], total loss: 0.03059, regularization loss: 0.29020, contrastive loss: 0.03059, Loss positive: 0.02585, Loss negative: 0.00475
2018-10-22 18:19:40.467283: Epoch [252/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:19:50.702157: Epoch [252/1000] [180/183], total loss: 0.03501, regularization loss: 0.29020, contrastive loss: 0.03501, Loss positive: 0.02880, Loss negative: 0.00622
2018-10-22 18:20:12.334764: Epoch [253/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:20:22.516407: Epoch [253/1000] [ 40/183], total loss: 0.00216, regularization loss: 0.29020, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 18:20:32.847312: Epoch [253/1000] [ 60/183], total loss: 0.00227, regularization loss: 0.29020, contrastive loss: 0.00227, Loss positive: 0.00000, Loss negative: 0.00227
2018-10-22 18:20:43.218832: Epoch [253/1000] [ 80/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 18:20:53.450155: Epoch [253/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:21:03.685371: Epoch [253/1000] [120/183], total loss: 0.02902, regularization loss: 0.29020, contrastive loss: 0.02902, Loss positive: 0.02902, Loss negative: 0.00000
2018-10-22 18:21:13.901265: Epoch [253/1000] [140/183], total loss: 0.00162, regularization loss: 0.29020, contrastive loss: 0.00162, Loss positive: 0.00000, Loss negative: 0.00162
2018-10-22 18:21:24.183924: Epoch [253/1000] [160/183], total loss: 0.01494, regularization loss: 0.29020, contrastive loss: 0.01494, Loss positive: 0.01489, Loss negative: 0.00006
2018-10-22 18:21:34.506137: Epoch [253/1000] [180/183], total loss: 0.00906, regularization loss: 0.29020, contrastive loss: 0.00906, Loss positive: 0.00903, Loss negative: 0.00002
2018-10-22 18:21:57.310313: Epoch [254/1000] [ 20/183], total loss: 0.01243, regularization loss: 0.29019, contrastive loss: 0.01243, Loss positive: 0.01243, Loss negative: 0.00000
2018-10-22 18:22:07.518759: Epoch [254/1000] [ 40/183], total loss: 0.00007, regularization loss: 0.29019, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 18:22:17.717571: Epoch [254/1000] [ 60/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 18:22:27.964439: Epoch [254/1000] [ 80/183], total loss: 0.00961, regularization loss: 0.29019, contrastive loss: 0.00961, Loss positive: 0.00781, Loss negative: 0.00179
2018-10-22 18:22:38.227583: Epoch [254/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:22:48.550317: Epoch [254/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:22:59.333202: Epoch [254/1000] [140/183], total loss: 0.00542, regularization loss: 0.29019, contrastive loss: 0.00542, Loss positive: 0.00000, Loss negative: 0.00542
2018-10-22 18:23:09.671749: Epoch [254/1000] [160/183], total loss: 0.02427, regularization loss: 0.29019, contrastive loss: 0.02427, Loss positive: 0.02187, Loss negative: 0.00240
2018-10-22 18:23:19.994839: Epoch [254/1000] [180/183], total loss: 0.01215, regularization loss: 0.29019, contrastive loss: 0.01215, Loss positive: 0.01215, Loss negative: 0.00000
2018-10-22 18:23:44.758831: Epoch [255/1000] [ 20/183], total loss: 0.00463, regularization loss: 0.29019, contrastive loss: 0.00463, Loss positive: 0.00000, Loss negative: 0.00463
2018-10-22 18:23:54.999795: Epoch [255/1000] [ 40/183], total loss: 0.00257, regularization loss: 0.29019, contrastive loss: 0.00257, Loss positive: 0.00000, Loss negative: 0.00257
2018-10-22 18:24:05.225828: Epoch [255/1000] [ 60/183], total loss: 0.00973, regularization loss: 0.29019, contrastive loss: 0.00973, Loss positive: 0.00973, Loss negative: 0.00000
2018-10-22 18:24:15.477269: Epoch [255/1000] [ 80/183], total loss: 0.00443, regularization loss: 0.29019, contrastive loss: 0.00443, Loss positive: 0.00000, Loss negative: 0.00443
2018-10-22 18:24:25.747925: Epoch [255/1000] [100/183], total loss: 0.00090, regularization loss: 0.29019, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 18:24:36.430447: Epoch [255/1000] [120/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 18:24:47.041416: Epoch [255/1000] [140/183], total loss: 0.00207, regularization loss: 0.29019, contrastive loss: 0.00207, Loss positive: 0.00000, Loss negative: 0.00207
2018-10-22 18:24:57.380855: Epoch [255/1000] [160/183], total loss: 0.00064, regularization loss: 0.29019, contrastive loss: 0.00064, Loss positive: 0.00000, Loss negative: 0.00064
2018-10-22 18:25:07.739926: Epoch [255/1000] [180/183], total loss: 0.00024, regularization loss: 0.29019, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 18:25:31.174462: Epoch [256/1000] [ 20/183], total loss: 0.00150, regularization loss: 0.29019, contrastive loss: 0.00150, Loss positive: 0.00000, Loss negative: 0.00150
2018-10-22 18:25:41.358267: Epoch [256/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:25:51.540311: Epoch [256/1000] [ 60/183], total loss: 0.01409, regularization loss: 0.29019, contrastive loss: 0.01409, Loss positive: 0.01409, Loss negative: 0.00000
2018-10-22 18:26:01.776078: Epoch [256/1000] [ 80/183], total loss: 0.00882, regularization loss: 0.29019, contrastive loss: 0.00882, Loss positive: 0.00000, Loss negative: 0.00882
2018-10-22 18:26:12.009896: Epoch [256/1000] [100/183], total loss: 0.00129, regularization loss: 0.29019, contrastive loss: 0.00129, Loss positive: 0.00000, Loss negative: 0.00129
2018-10-22 18:26:22.277042: Epoch [256/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:26:32.541876: Epoch [256/1000] [140/183], total loss: 0.00400, regularization loss: 0.29019, contrastive loss: 0.00400, Loss positive: 0.00000, Loss negative: 0.00400
2018-10-22 18:26:42.771933: Epoch [256/1000] [160/183], total loss: 0.00050, regularization loss: 0.29019, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 18:26:53.291511: Epoch [256/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:27:14.879835: Epoch [257/1000] [ 20/183], total loss: 0.00440, regularization loss: 0.29019, contrastive loss: 0.00440, Loss positive: 0.00000, Loss negative: 0.00440
2018-10-22 18:27:25.074143: Epoch [257/1000] [ 40/183], total loss: 0.00077, regularization loss: 0.29019, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 18:27:35.288529: Epoch [257/1000] [ 60/183], total loss: 0.00006, regularization loss: 0.29019, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 18:27:45.519994: Epoch [257/1000] [ 80/183], total loss: 0.01122, regularization loss: 0.29019, contrastive loss: 0.01122, Loss positive: 0.01086, Loss negative: 0.00036
2018-10-22 18:27:55.746357: Epoch [257/1000] [100/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 18:28:05.992870: Epoch [257/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:28:16.231269: Epoch [257/1000] [140/183], total loss: 0.00052, regularization loss: 0.29020, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 18:28:26.467765: Epoch [257/1000] [160/183], total loss: 0.01334, regularization loss: 0.29019, contrastive loss: 0.01334, Loss positive: 0.01334, Loss negative: 0.00000
2018-10-22 18:28:36.752419: Epoch [257/1000] [180/183], total loss: 0.00150, regularization loss: 0.29019, contrastive loss: 0.00150, Loss positive: 0.00000, Loss negative: 0.00150
2018-10-22 18:28:58.356363: Epoch [258/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:29:08.545684: Epoch [258/1000] [ 40/183], total loss: 0.00183, regularization loss: 0.29019, contrastive loss: 0.00183, Loss positive: 0.00000, Loss negative: 0.00183
2018-10-22 18:29:18.910668: Epoch [258/1000] [ 60/183], total loss: 0.00124, regularization loss: 0.29019, contrastive loss: 0.00124, Loss positive: 0.00000, Loss negative: 0.00124
2018-10-22 18:29:29.147739: Epoch [258/1000] [ 80/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 18:29:39.371674: Epoch [258/1000] [100/183], total loss: 0.01627, regularization loss: 0.29019, contrastive loss: 0.01627, Loss positive: 0.01574, Loss negative: 0.00053
2018-10-22 18:29:49.624648: Epoch [258/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:29:59.861476: Epoch [258/1000] [140/183], total loss: 0.00513, regularization loss: 0.29019, contrastive loss: 0.00513, Loss positive: 0.00513, Loss negative: 0.00000
2018-10-22 18:30:10.095763: Epoch [258/1000] [160/183], total loss: 0.00588, regularization loss: 0.29019, contrastive loss: 0.00588, Loss positive: 0.00000, Loss negative: 0.00588
2018-10-22 18:30:20.355995: Epoch [258/1000] [180/183], total loss: 0.00216, regularization loss: 0.29019, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 18:30:41.821816: Epoch [259/1000] [ 20/183], total loss: 0.00650, regularization loss: 0.29019, contrastive loss: 0.00650, Loss positive: 0.00605, Loss negative: 0.00044
2018-10-22 18:30:52.011014: Epoch [259/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:31:02.215946: Epoch [259/1000] [ 60/183], total loss: 0.01789, regularization loss: 0.29019, contrastive loss: 0.01789, Loss positive: 0.01653, Loss negative: 0.00135
2018-10-22 18:31:12.563169: Epoch [259/1000] [ 80/183], total loss: 0.00078, regularization loss: 0.29019, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 18:31:22.813167: Epoch [259/1000] [100/183], total loss: 0.00553, regularization loss: 0.29020, contrastive loss: 0.00553, Loss positive: 0.00000, Loss negative: 0.00553
2018-10-22 18:31:33.039171: Epoch [259/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:31:43.299085: Epoch [259/1000] [140/183], total loss: 0.00004, regularization loss: 0.29019, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 18:31:53.552144: Epoch [259/1000] [160/183], total loss: 0.00211, regularization loss: 0.29019, contrastive loss: 0.00211, Loss positive: 0.00000, Loss negative: 0.00211
2018-10-22 18:32:04.234665: Epoch [259/1000] [180/183], total loss: 0.01107, regularization loss: 0.29019, contrastive loss: 0.01107, Loss positive: 0.00545, Loss negative: 0.00561
2018-10-22 18:32:27.066969: Epoch [260/1000] [ 20/183], total loss: 0.01395, regularization loss: 0.29019, contrastive loss: 0.01395, Loss positive: 0.01265, Loss negative: 0.00130
2018-10-22 18:32:37.270322: Epoch [260/1000] [ 40/183], total loss: 0.00014, regularization loss: 0.29019, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 18:32:47.516065: Epoch [260/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:32:57.838970: Epoch [260/1000] [ 80/183], total loss: 0.02726, regularization loss: 0.29019, contrastive loss: 0.02726, Loss positive: 0.02668, Loss negative: 0.00058
2018-10-22 18:33:08.181089: Epoch [260/1000] [100/183], total loss: 0.00210, regularization loss: 0.29019, contrastive loss: 0.00210, Loss positive: 0.00000, Loss negative: 0.00210
2018-10-22 18:33:18.867720: Epoch [260/1000] [120/183], total loss: 0.00372, regularization loss: 0.29019, contrastive loss: 0.00372, Loss positive: 0.00000, Loss negative: 0.00372
2018-10-22 18:33:29.382139: Epoch [260/1000] [140/183], total loss: 0.00231, regularization loss: 0.29019, contrastive loss: 0.00231, Loss positive: 0.00000, Loss negative: 0.00231
2018-10-22 18:33:39.663097: Epoch [260/1000] [160/183], total loss: 0.00289, regularization loss: 0.29019, contrastive loss: 0.00289, Loss positive: 0.00000, Loss negative: 0.00289
2018-10-22 18:33:50.274217: Epoch [260/1000] [180/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
Recall@1: 0.27279
Recall@2: 0.38538
Recall@4: 0.51182
Recall@8: 0.64956
Recall@16: 0.77397
Recall@32: 0.87120
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 50.  46.  57.  84.  56.  50.  67.  69.  59.  86.  60.  36.  63.  63.
  64.  68.  74.  67.  53.  90.  42.  63.  81.  23. 105.  56.  62.  77.
  43.  26.  19.  35.  68.  33.  66.  73.  53.  56.  65.  77.  59.  96.
  49.  57.  28.  99.  48.  65.  50.  96.  75.  64.  68.  48. 120.  65.
  53.  69.  39.  91.  56.  59.  41.  61.  35.  67.  46.  70.  37.  69.
  55.  36.  74.  48.  51.  41.  75.  51.  25.  79.  55.  45.  53.  85.
  64.  54.  33.  60.  27.  36.  62. 104.  76.  65.  31.  41.  85.  76.
  28.  44.]
Purity is 0.238
count_cross = [[1. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 3. 0.]
 [0. 3. 6. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 3. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
Mutual information is 2.06405
5924.0
5924
Entropy cluster is 4.54955
Entropy class is 4.60444
normalized_mutual_information is 0.45096
tp_and_fp = 191782.0
tp = 20163.0
fp is 171619.0
fn is 152587.0
RI is 0.9815203278901199
Precision is 0.1051349970278754
Recall is 0.1167178002894356
F_1 is 0.11062403300670448

normalized_mutual_information = 0.4509623352766601
RI = 0.9815203278901199
F_1 = 0.11062403300670448

The NN is 0.27279
The FT is 0.14047
The ST is 0.21825
The DCG is 0.52584
The E is 0.11783
The MAP 0.11455

2018-10-22 18:35:33.181996: Epoch [261/1000] [ 20/183], total loss: 0.00820, regularization loss: 0.29019, contrastive loss: 0.00820, Loss positive: 0.00818, Loss negative: 0.00002
2018-10-22 18:35:43.308735: Epoch [261/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:35:53.416864: Epoch [261/1000] [ 60/183], total loss: 0.01184, regularization loss: 0.29019, contrastive loss: 0.01184, Loss positive: 0.01184, Loss negative: 0.00000
2018-10-22 18:36:03.526729: Epoch [261/1000] [ 80/183], total loss: 0.00128, regularization loss: 0.29019, contrastive loss: 0.00128, Loss positive: 0.00000, Loss negative: 0.00128
2018-10-22 18:36:13.727151: Epoch [261/1000] [100/183], total loss: 0.02133, regularization loss: 0.29019, contrastive loss: 0.02133, Loss positive: 0.02004, Loss negative: 0.00129
2018-10-22 18:36:23.915319: Epoch [261/1000] [120/183], total loss: 0.00270, regularization loss: 0.29019, contrastive loss: 0.00270, Loss positive: 0.00000, Loss negative: 0.00270
2018-10-22 18:36:34.143747: Epoch [261/1000] [140/183], total loss: 0.00801, regularization loss: 0.29019, contrastive loss: 0.00801, Loss positive: 0.00782, Loss negative: 0.00018
2018-10-22 18:36:44.356998: Epoch [261/1000] [160/183], total loss: 0.00186, regularization loss: 0.29019, contrastive loss: 0.00186, Loss positive: 0.00000, Loss negative: 0.00186
2018-10-22 18:36:54.608526: Epoch [261/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:37:16.130365: Epoch [262/1000] [ 20/183], total loss: 0.00411, regularization loss: 0.29019, contrastive loss: 0.00411, Loss positive: 0.00000, Loss negative: 0.00411
2018-10-22 18:37:26.331063: Epoch [262/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:37:36.493478: Epoch [262/1000] [ 60/183], total loss: 0.00561, regularization loss: 0.29019, contrastive loss: 0.00561, Loss positive: 0.00000, Loss negative: 0.00561
2018-10-22 18:37:46.819420: Epoch [262/1000] [ 80/183], total loss: 0.00229, regularization loss: 0.29019, contrastive loss: 0.00229, Loss positive: 0.00000, Loss negative: 0.00229
2018-10-22 18:37:57.065130: Epoch [262/1000] [100/183], total loss: 0.00100, regularization loss: 0.29019, contrastive loss: 0.00100, Loss positive: 0.00000, Loss negative: 0.00100
2018-10-22 18:38:07.296593: Epoch [262/1000] [120/183], total loss: 0.00763, regularization loss: 0.29019, contrastive loss: 0.00763, Loss positive: 0.00604, Loss negative: 0.00158
2018-10-22 18:38:17.542790: Epoch [262/1000] [140/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:38:27.771410: Epoch [262/1000] [160/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 18:38:38.028711: Epoch [262/1000] [180/183], total loss: 0.00719, regularization loss: 0.29019, contrastive loss: 0.00719, Loss positive: 0.00606, Loss negative: 0.00113
2018-10-22 18:38:59.583326: Epoch [263/1000] [ 20/183], total loss: 0.00471, regularization loss: 0.29019, contrastive loss: 0.00471, Loss positive: 0.00000, Loss negative: 0.00471
2018-10-22 18:39:09.767449: Epoch [263/1000] [ 40/183], total loss: 0.00271, regularization loss: 0.29019, contrastive loss: 0.00271, Loss positive: 0.00000, Loss negative: 0.00271
2018-10-22 18:39:19.946042: Epoch [263/1000] [ 60/183], total loss: 0.00002, regularization loss: 0.29019, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 18:39:30.164337: Epoch [263/1000] [ 80/183], total loss: 0.01522, regularization loss: 0.29019, contrastive loss: 0.01522, Loss positive: 0.01499, Loss negative: 0.00023
2018-10-22 18:39:40.410089: Epoch [263/1000] [100/183], total loss: 0.01100, regularization loss: 0.29019, contrastive loss: 0.01100, Loss positive: 0.00569, Loss negative: 0.00530
2018-10-22 18:39:50.644487: Epoch [263/1000] [120/183], total loss: 0.00167, regularization loss: 0.29019, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 18:40:00.892919: Epoch [263/1000] [140/183], total loss: 0.00291, regularization loss: 0.29019, contrastive loss: 0.00291, Loss positive: 0.00000, Loss negative: 0.00291
2018-10-22 18:40:11.121010: Epoch [263/1000] [160/183], total loss: 0.00100, regularization loss: 0.29019, contrastive loss: 0.00100, Loss positive: 0.00000, Loss negative: 0.00100
2018-10-22 18:40:21.353446: Epoch [263/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:40:42.889266: Epoch [264/1000] [ 20/183], total loss: 0.01350, regularization loss: 0.29020, contrastive loss: 0.01350, Loss positive: 0.01349, Loss negative: 0.00001
2018-10-22 18:40:53.076563: Epoch [264/1000] [ 40/183], total loss: 0.00534, regularization loss: 0.29019, contrastive loss: 0.00534, Loss positive: 0.00000, Loss negative: 0.00534
2018-10-22 18:41:03.273494: Epoch [264/1000] [ 60/183], total loss: 0.00962, regularization loss: 0.29019, contrastive loss: 0.00962, Loss positive: 0.00952, Loss negative: 0.00009
2018-10-22 18:41:13.488019: Epoch [264/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:41:23.713334: Epoch [264/1000] [100/183], total loss: 0.00015, regularization loss: 0.29019, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 18:41:33.958265: Epoch [264/1000] [120/183], total loss: 0.00806, regularization loss: 0.29019, contrastive loss: 0.00806, Loss positive: 0.00000, Loss negative: 0.00806
2018-10-22 18:41:44.201159: Epoch [264/1000] [140/183], total loss: 0.02069, regularization loss: 0.29019, contrastive loss: 0.02069, Loss positive: 0.02069, Loss negative: 0.00000
2018-10-22 18:41:54.476128: Epoch [264/1000] [160/183], total loss: 0.00015, regularization loss: 0.29020, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 18:42:04.890852: Epoch [264/1000] [180/183], total loss: 0.00262, regularization loss: 0.29019, contrastive loss: 0.00262, Loss positive: 0.00000, Loss negative: 0.00262
2018-10-22 18:42:27.742197: Epoch [265/1000] [ 20/183], total loss: 0.00242, regularization loss: 0.29019, contrastive loss: 0.00242, Loss positive: 0.00000, Loss negative: 0.00242
2018-10-22 18:42:37.911621: Epoch [265/1000] [ 40/183], total loss: 0.00088, regularization loss: 0.29019, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 18:42:48.193909: Epoch [265/1000] [ 60/183], total loss: 0.02826, regularization loss: 0.29019, contrastive loss: 0.02826, Loss positive: 0.02824, Loss negative: 0.00002
2018-10-22 18:42:58.477167: Epoch [265/1000] [ 80/183], total loss: 0.00089, regularization loss: 0.29019, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 18:43:08.807425: Epoch [265/1000] [100/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 18:43:19.260045: Epoch [265/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:43:29.698457: Epoch [265/1000] [140/183], total loss: 0.00101, regularization loss: 0.29019, contrastive loss: 0.00101, Loss positive: 0.00000, Loss negative: 0.00101
2018-10-22 18:43:40.069311: Epoch [265/1000] [160/183], total loss: 0.05217, regularization loss: 0.29019, contrastive loss: 0.05217, Loss positive: 0.05035, Loss negative: 0.00181
2018-10-22 18:43:50.394911: Epoch [265/1000] [180/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 18:44:15.118758: Epoch [266/1000] [ 20/183], total loss: 0.00012, regularization loss: 0.29019, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 18:44:25.355078: Epoch [266/1000] [ 40/183], total loss: 0.00035, regularization loss: 0.29019, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 18:44:35.601577: Epoch [266/1000] [ 60/183], total loss: 0.01686, regularization loss: 0.29019, contrastive loss: 0.01686, Loss positive: 0.01443, Loss negative: 0.00243
2018-10-22 18:44:45.849693: Epoch [266/1000] [ 80/183], total loss: 0.00033, regularization loss: 0.29019, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 18:44:56.145673: Epoch [266/1000] [100/183], total loss: 0.02763, regularization loss: 0.29019, contrastive loss: 0.02763, Loss positive: 0.02351, Loss negative: 0.00413
2018-10-22 18:45:06.468927: Epoch [266/1000] [120/183], total loss: 0.01521, regularization loss: 0.29019, contrastive loss: 0.01521, Loss positive: 0.01163, Loss negative: 0.00358
2018-10-22 18:45:17.023378: Epoch [266/1000] [140/183], total loss: 0.01222, regularization loss: 0.29019, contrastive loss: 0.01222, Loss positive: 0.01222, Loss negative: 0.00000
2018-10-22 18:45:27.322855: Epoch [266/1000] [160/183], total loss: 0.00029, regularization loss: 0.29019, contrastive loss: 0.00029, Loss positive: 0.00000, Loss negative: 0.00029
2018-10-22 18:45:37.602447: Epoch [266/1000] [180/183], total loss: 0.02223, regularization loss: 0.29019, contrastive loss: 0.02223, Loss positive: 0.02223, Loss negative: 0.00000
2018-10-22 18:46:00.407657: Epoch [267/1000] [ 20/183], total loss: 0.00080, regularization loss: 0.29019, contrastive loss: 0.00080, Loss positive: 0.00000, Loss negative: 0.00080
2018-10-22 18:46:10.639428: Epoch [267/1000] [ 40/183], total loss: 0.00417, regularization loss: 0.29019, contrastive loss: 0.00417, Loss positive: 0.00000, Loss negative: 0.00417
2018-10-22 18:46:20.858834: Epoch [267/1000] [ 60/183], total loss: 0.03646, regularization loss: 0.29019, contrastive loss: 0.03646, Loss positive: 0.02929, Loss negative: 0.00717
2018-10-22 18:46:31.203127: Epoch [267/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29019, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 18:46:41.545312: Epoch [267/1000] [100/183], total loss: 0.00628, regularization loss: 0.29019, contrastive loss: 0.00628, Loss positive: 0.00000, Loss negative: 0.00628
2018-10-22 18:46:52.090732: Epoch [267/1000] [120/183], total loss: 0.00829, regularization loss: 0.29019, contrastive loss: 0.00829, Loss positive: 0.00000, Loss negative: 0.00829
2018-10-22 18:47:02.450718: Epoch [267/1000] [140/183], total loss: 0.02974, regularization loss: 0.29019, contrastive loss: 0.02974, Loss positive: 0.02207, Loss negative: 0.00767
2018-10-22 18:47:12.701514: Epoch [267/1000] [160/183], total loss: 0.00135, regularization loss: 0.29019, contrastive loss: 0.00135, Loss positive: 0.00000, Loss negative: 0.00135
2018-10-22 18:47:22.940387: Epoch [267/1000] [180/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:47:44.576830: Epoch [268/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:47:54.743480: Epoch [268/1000] [ 40/183], total loss: 0.00016, regularization loss: 0.29019, contrastive loss: 0.00016, Loss positive: 0.00000, Loss negative: 0.00016
2018-10-22 18:48:04.949111: Epoch [268/1000] [ 60/183], total loss: 0.00575, regularization loss: 0.29019, contrastive loss: 0.00575, Loss positive: 0.00000, Loss negative: 0.00575
2018-10-22 18:48:15.165638: Epoch [268/1000] [ 80/183], total loss: 0.03352, regularization loss: 0.29019, contrastive loss: 0.03352, Loss positive: 0.03352, Loss negative: 0.00000
2018-10-22 18:48:25.405245: Epoch [268/1000] [100/183], total loss: 0.01066, regularization loss: 0.29019, contrastive loss: 0.01066, Loss positive: 0.01066, Loss negative: 0.00000
2018-10-22 18:48:35.638933: Epoch [268/1000] [120/183], total loss: 0.03736, regularization loss: 0.29019, contrastive loss: 0.03736, Loss positive: 0.03565, Loss negative: 0.00171
2018-10-22 18:48:45.879135: Epoch [268/1000] [140/183], total loss: 0.01763, regularization loss: 0.29019, contrastive loss: 0.01763, Loss positive: 0.01128, Loss negative: 0.00636
2018-10-22 18:48:56.131894: Epoch [268/1000] [160/183], total loss: 0.01005, regularization loss: 0.29019, contrastive loss: 0.01005, Loss positive: 0.00000, Loss negative: 0.01005
2018-10-22 18:49:06.361299: Epoch [268/1000] [180/183], total loss: 0.01831, regularization loss: 0.29019, contrastive loss: 0.01831, Loss positive: 0.01779, Loss negative: 0.00052
2018-10-22 18:49:27.988159: Epoch [269/1000] [ 20/183], total loss: 0.00202, regularization loss: 0.29019, contrastive loss: 0.00202, Loss positive: 0.00000, Loss negative: 0.00202
2018-10-22 18:49:38.170463: Epoch [269/1000] [ 40/183], total loss: 0.01702, regularization loss: 0.29019, contrastive loss: 0.01702, Loss positive: 0.00620, Loss negative: 0.01082
2018-10-22 18:49:48.364518: Epoch [269/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:49:58.577380: Epoch [269/1000] [ 80/183], total loss: 0.00067, regularization loss: 0.29019, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 18:50:08.826079: Epoch [269/1000] [100/183], total loss: 0.00114, regularization loss: 0.29019, contrastive loss: 0.00114, Loss positive: 0.00000, Loss negative: 0.00114
2018-10-22 18:50:19.052411: Epoch [269/1000] [120/183], total loss: 0.00175, regularization loss: 0.29019, contrastive loss: 0.00175, Loss positive: 0.00000, Loss negative: 0.00175
2018-10-22 18:50:29.346493: Epoch [269/1000] [140/183], total loss: 0.00005, regularization loss: 0.29019, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 18:50:39.583365: Epoch [269/1000] [160/183], total loss: 0.00018, regularization loss: 0.29019, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 18:50:49.821729: Epoch [269/1000] [180/183], total loss: 0.00058, regularization loss: 0.29019, contrastive loss: 0.00058, Loss positive: 0.00000, Loss negative: 0.00058
2018-10-22 18:51:11.401767: Epoch [270/1000] [ 20/183], total loss: 0.01576, regularization loss: 0.29020, contrastive loss: 0.01576, Loss positive: 0.01576, Loss negative: 0.00000
2018-10-22 18:51:21.562588: Epoch [270/1000] [ 40/183], total loss: 0.00966, regularization loss: 0.29020, contrastive loss: 0.00966, Loss positive: 0.00822, Loss negative: 0.00144
2018-10-22 18:51:31.784888: Epoch [270/1000] [ 60/183], total loss: 0.00008, regularization loss: 0.29019, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 18:51:42.016703: Epoch [270/1000] [ 80/183], total loss: 0.00579, regularization loss: 0.29019, contrastive loss: 0.00579, Loss positive: 0.00579, Loss negative: 0.00000
2018-10-22 18:51:52.276043: Epoch [270/1000] [100/183], total loss: 0.00179, regularization loss: 0.29019, contrastive loss: 0.00179, Loss positive: 0.00000, Loss negative: 0.00179
2018-10-22 18:52:02.540807: Epoch [270/1000] [120/183], total loss: 0.00518, regularization loss: 0.29019, contrastive loss: 0.00518, Loss positive: 0.00000, Loss negative: 0.00518
2018-10-22 18:52:12.849625: Epoch [270/1000] [140/183], total loss: 0.00160, regularization loss: 0.29019, contrastive loss: 0.00160, Loss positive: 0.00000, Loss negative: 0.00160
2018-10-22 18:52:23.115787: Epoch [270/1000] [160/183], total loss: 0.00250, regularization loss: 0.29019, contrastive loss: 0.00250, Loss positive: 0.00000, Loss negative: 0.00250
2018-10-22 18:52:33.662950: Epoch [270/1000] [180/183], total loss: 0.01337, regularization loss: 0.29019, contrastive loss: 0.01337, Loss positive: 0.01247, Loss negative: 0.00091
Recall@1: 0.27684
Recall@2: 0.38960
Recall@4: 0.50878
Recall@8: 0.65209
Recall@16: 0.77279
Recall@32: 0.87593
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 49.  57.  48.  63.  78.  39.  48.  63.  78.  43.  61.  37.  73.  48.
  87.  65.  58.  80.  48.  53.  61.  56.  66.  30.  68.  99.  61.  85.
  32.  89.  56.  59.  44.  86.  38.  59.  56. 113.  48.  74.  83.  88.
  43.  46.  48. 111.  43.  74.  65.  66.  53.  50.  97.  86.  56.  33.
  68.  76.  39.  92.  70.  61.  40.  62.  76.  83.  36.  33.  32.  66.
  43.  32.  68.  50.  40.  40.  65.  52.  41.  74.  95.  50.  37.  63.
  91.  26.  53.  70.  49.  14.  58.  48.  60.  69.  55.  42.  30.  51.
  45.  81.]
Purity is 0.243
count_cross = [[ 1.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  2.  0.  0.]
 ...
 [ 0.  0.  1. ...  0.  0.  0.]
 [ 1.  0.  0. ...  0.  0.  0.]
 [ 0.  1.  0. ...  5. 17.  0.]]
Mutual information is 2.06213
5924.0
5924
Entropy cluster is 4.55119
Entropy class is 4.60444
normalized_mutual_information is 0.45046
tp_and_fp = 191427.0
tp = 20341.0
fp is 171086.0
fn is 152409.0
RI is 0.9815608547368474
Precision is 0.10625982750604669
Recall is 0.11774819102749638
F_1 is 0.11170941602572376

normalized_mutual_information = 0.4504625503936531
RI = 0.9815608547368474
F_1 = 0.11170941602572376

The NN is 0.27684
The FT is 0.14174
The ST is 0.22036
The DCG is 0.52666
The E is 0.11924
The MAP 0.11608

2018-10-22 18:54:17.546506: Epoch [271/1000] [ 20/183], total loss: 0.03425, regularization loss: 0.29019, contrastive loss: 0.03425, Loss positive: 0.03423, Loss negative: 0.00001
2018-10-22 18:54:27.705141: Epoch [271/1000] [ 40/183], total loss: 0.00464, regularization loss: 0.29019, contrastive loss: 0.00464, Loss positive: 0.00000, Loss negative: 0.00464
2018-10-22 18:54:37.885131: Epoch [271/1000] [ 60/183], total loss: 0.01692, regularization loss: 0.29019, contrastive loss: 0.01692, Loss positive: 0.01580, Loss negative: 0.00112
2018-10-22 18:54:48.080952: Epoch [271/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:54:58.347328: Epoch [271/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 18:55:08.622564: Epoch [271/1000] [120/183], total loss: 0.00188, regularization loss: 0.29020, contrastive loss: 0.00188, Loss positive: 0.00000, Loss negative: 0.00188
2018-10-22 18:55:19.034770: Epoch [271/1000] [140/183], total loss: 0.01091, regularization loss: 0.29020, contrastive loss: 0.01091, Loss positive: 0.00959, Loss negative: 0.00132
2018-10-22 18:55:29.299910: Epoch [271/1000] [160/183], total loss: 0.00025, regularization loss: 0.29019, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 18:55:39.806688: Epoch [271/1000] [180/183], total loss: 0.00439, regularization loss: 0.29019, contrastive loss: 0.00439, Loss positive: 0.00436, Loss negative: 0.00003
2018-10-22 18:56:02.366207: Epoch [272/1000] [ 20/183], total loss: 0.00275, regularization loss: 0.29019, contrastive loss: 0.00275, Loss positive: 0.00000, Loss negative: 0.00275
2018-10-22 18:56:12.550073: Epoch [272/1000] [ 40/183], total loss: 0.02253, regularization loss: 0.29019, contrastive loss: 0.02253, Loss positive: 0.02244, Loss negative: 0.00009
2018-10-22 18:56:22.713140: Epoch [272/1000] [ 60/183], total loss: 0.01443, regularization loss: 0.29019, contrastive loss: 0.01443, Loss positive: 0.01000, Loss negative: 0.00444
2018-10-22 18:56:32.927238: Epoch [272/1000] [ 80/183], total loss: 0.00217, regularization loss: 0.29019, contrastive loss: 0.00217, Loss positive: 0.00000, Loss negative: 0.00217
2018-10-22 18:56:43.407520: Epoch [272/1000] [100/183], total loss: 0.00283, regularization loss: 0.29019, contrastive loss: 0.00283, Loss positive: 0.00000, Loss negative: 0.00283
2018-10-22 18:56:53.739218: Epoch [272/1000] [120/183], total loss: 0.00865, regularization loss: 0.29019, contrastive loss: 0.00865, Loss positive: 0.00561, Loss negative: 0.00304
2018-10-22 18:57:03.978719: Epoch [272/1000] [140/183], total loss: 0.00079, regularization loss: 0.29019, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 18:57:14.236091: Epoch [272/1000] [160/183], total loss: 0.03448, regularization loss: 0.29019, contrastive loss: 0.03448, Loss positive: 0.03446, Loss negative: 0.00002
2018-10-22 18:57:24.480524: Epoch [272/1000] [180/183], total loss: 0.02112, regularization loss: 0.29019, contrastive loss: 0.02112, Loss positive: 0.02096, Loss negative: 0.00016
2018-10-22 18:57:46.049858: Epoch [273/1000] [ 20/183], total loss: 0.00145, regularization loss: 0.29019, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 18:57:56.243060: Epoch [273/1000] [ 40/183], total loss: 0.00063, regularization loss: 0.29019, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 18:58:06.459614: Epoch [273/1000] [ 60/183], total loss: 0.00247, regularization loss: 0.29019, contrastive loss: 0.00247, Loss positive: 0.00000, Loss negative: 0.00247
2018-10-22 18:58:16.666192: Epoch [273/1000] [ 80/183], total loss: 0.00060, regularization loss: 0.29019, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 18:58:26.947702: Epoch [273/1000] [100/183], total loss: 0.01664, regularization loss: 0.29019, contrastive loss: 0.01664, Loss positive: 0.01543, Loss negative: 0.00120
2018-10-22 18:58:37.177370: Epoch [273/1000] [120/183], total loss: 0.00133, regularization loss: 0.29019, contrastive loss: 0.00133, Loss positive: 0.00000, Loss negative: 0.00133
2018-10-22 18:58:47.409580: Epoch [273/1000] [140/183], total loss: 0.00178, regularization loss: 0.29020, contrastive loss: 0.00178, Loss positive: 0.00000, Loss negative: 0.00178
2018-10-22 18:58:57.652334: Epoch [273/1000] [160/183], total loss: 0.00254, regularization loss: 0.29020, contrastive loss: 0.00254, Loss positive: 0.00000, Loss negative: 0.00254
2018-10-22 18:59:07.882064: Epoch [273/1000] [180/183], total loss: 0.00234, regularization loss: 0.29020, contrastive loss: 0.00234, Loss positive: 0.00000, Loss negative: 0.00234
2018-10-22 18:59:29.483523: Epoch [274/1000] [ 20/183], total loss: 0.00090, regularization loss: 0.29020, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 18:59:39.682707: Epoch [274/1000] [ 40/183], total loss: 0.03342, regularization loss: 0.29020, contrastive loss: 0.03342, Loss positive: 0.03259, Loss negative: 0.00083
2018-10-22 18:59:49.859649: Epoch [274/1000] [ 60/183], total loss: 0.00456, regularization loss: 0.29020, contrastive loss: 0.00456, Loss positive: 0.00000, Loss negative: 0.00456
2018-10-22 19:00:00.090831: Epoch [274/1000] [ 80/183], total loss: 0.02339, regularization loss: 0.29020, contrastive loss: 0.02339, Loss positive: 0.02288, Loss negative: 0.00051
2018-10-22 19:00:10.343558: Epoch [274/1000] [100/183], total loss: 0.01304, regularization loss: 0.29020, contrastive loss: 0.01304, Loss positive: 0.01227, Loss negative: 0.00077
2018-10-22 19:00:20.604460: Epoch [274/1000] [120/183], total loss: 0.00159, regularization loss: 0.29019, contrastive loss: 0.00159, Loss positive: 0.00000, Loss negative: 0.00159
2018-10-22 19:00:30.861596: Epoch [274/1000] [140/183], total loss: 0.03696, regularization loss: 0.29020, contrastive loss: 0.03696, Loss positive: 0.03696, Loss negative: 0.00000
2018-10-22 19:00:41.295397: Epoch [274/1000] [160/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 19:00:51.553441: Epoch [274/1000] [180/183], total loss: 0.00081, regularization loss: 0.29020, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 19:01:13.106374: Epoch [275/1000] [ 20/183], total loss: 0.01644, regularization loss: 0.29020, contrastive loss: 0.01644, Loss positive: 0.01577, Loss negative: 0.00067
2018-10-22 19:01:23.283808: Epoch [275/1000] [ 40/183], total loss: 0.00012, regularization loss: 0.29020, contrastive loss: 0.00012, Loss positive: 0.00000, Loss negative: 0.00012
2018-10-22 19:01:33.500853: Epoch [275/1000] [ 60/183], total loss: 0.00054, regularization loss: 0.29020, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 19:01:43.758883: Epoch [275/1000] [ 80/183], total loss: 0.00091, regularization loss: 0.29020, contrastive loss: 0.00091, Loss positive: 0.00000, Loss negative: 0.00091
2018-10-22 19:01:54.001316: Epoch [275/1000] [100/183], total loss: 0.00479, regularization loss: 0.29020, contrastive loss: 0.00479, Loss positive: 0.00000, Loss negative: 0.00479
2018-10-22 19:02:04.251708: Epoch [275/1000] [120/183], total loss: 0.00113, regularization loss: 0.29020, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 19:02:14.950310: Epoch [275/1000] [140/183], total loss: 0.00014, regularization loss: 0.29020, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 19:02:25.240351: Epoch [275/1000] [160/183], total loss: 0.01203, regularization loss: 0.29020, contrastive loss: 0.01203, Loss positive: 0.01173, Loss negative: 0.00031
2018-10-22 19:02:35.671652: Epoch [275/1000] [180/183], total loss: 0.00921, regularization loss: 0.29020, contrastive loss: 0.00921, Loss positive: 0.00000, Loss negative: 0.00921
2018-10-22 19:02:59.970863: Epoch [276/1000] [ 20/183], total loss: 0.01938, regularization loss: 0.29020, contrastive loss: 0.01938, Loss positive: 0.01750, Loss negative: 0.00187
2018-10-22 19:03:10.174326: Epoch [276/1000] [ 40/183], total loss: 0.00609, regularization loss: 0.29020, contrastive loss: 0.00609, Loss positive: 0.00000, Loss negative: 0.00609
2018-10-22 19:03:20.429119: Epoch [276/1000] [ 60/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 19:03:30.671848: Epoch [276/1000] [ 80/183], total loss: 0.00019, regularization loss: 0.29020, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 19:03:41.177101: Epoch [276/1000] [100/183], total loss: 0.00424, regularization loss: 0.29020, contrastive loss: 0.00424, Loss positive: 0.00000, Loss negative: 0.00424
2018-10-22 19:03:51.717574: Epoch [276/1000] [120/183], total loss: 0.01500, regularization loss: 0.29020, contrastive loss: 0.01500, Loss positive: 0.01500, Loss negative: 0.00000
2018-10-22 19:04:02.035677: Epoch [276/1000] [140/183], total loss: 0.00203, regularization loss: 0.29020, contrastive loss: 0.00203, Loss positive: 0.00000, Loss negative: 0.00203
2018-10-22 19:04:12.367713: Epoch [276/1000] [160/183], total loss: 0.01566, regularization loss: 0.29020, contrastive loss: 0.01566, Loss positive: 0.01511, Loss negative: 0.00055
2018-10-22 19:04:22.784033: Epoch [276/1000] [180/183], total loss: 0.00011, regularization loss: 0.29020, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 19:04:47.337940: Epoch [277/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:04:57.552171: Epoch [277/1000] [ 40/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:05:07.806550: Epoch [277/1000] [ 60/183], total loss: 0.03191, regularization loss: 0.29020, contrastive loss: 0.03191, Loss positive: 0.02738, Loss negative: 0.00454
2018-10-22 19:05:18.074827: Epoch [277/1000] [ 80/183], total loss: 0.00167, regularization loss: 0.29020, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 19:05:28.302253: Epoch [277/1000] [100/183], total loss: 0.02004, regularization loss: 0.29020, contrastive loss: 0.02004, Loss positive: 0.01982, Loss negative: 0.00021
2018-10-22 19:05:38.589279: Epoch [277/1000] [120/183], total loss: 0.00011, regularization loss: 0.29020, contrastive loss: 0.00011, Loss positive: 0.00000, Loss negative: 0.00011
2018-10-22 19:05:48.919264: Epoch [277/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:05:59.312341: Epoch [277/1000] [160/183], total loss: 0.00216, regularization loss: 0.29020, contrastive loss: 0.00216, Loss positive: 0.00000, Loss negative: 0.00216
2018-10-22 19:06:09.612561: Epoch [277/1000] [180/183], total loss: 0.00074, regularization loss: 0.29020, contrastive loss: 0.00074, Loss positive: 0.00000, Loss negative: 0.00074
2018-10-22 19:06:32.020148: Epoch [278/1000] [ 20/183], total loss: 0.04183, regularization loss: 0.29020, contrastive loss: 0.04183, Loss positive: 0.04124, Loss negative: 0.00059
2018-10-22 19:06:42.185761: Epoch [278/1000] [ 40/183], total loss: 0.00040, regularization loss: 0.29020, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 19:06:52.392423: Epoch [278/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:07:02.640046: Epoch [278/1000] [ 80/183], total loss: 0.00152, regularization loss: 0.29020, contrastive loss: 0.00152, Loss positive: 0.00000, Loss negative: 0.00152
2018-10-22 19:07:12.896063: Epoch [278/1000] [100/183], total loss: 0.00014, regularization loss: 0.29020, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 19:07:23.114967: Epoch [278/1000] [120/183], total loss: 0.00444, regularization loss: 0.29019, contrastive loss: 0.00444, Loss positive: 0.00000, Loss negative: 0.00444
2018-10-22 19:07:33.362950: Epoch [278/1000] [140/183], total loss: 0.02449, regularization loss: 0.29019, contrastive loss: 0.02449, Loss positive: 0.02200, Loss negative: 0.00249
2018-10-22 19:07:43.619751: Epoch [278/1000] [160/183], total loss: 0.02915, regularization loss: 0.29019, contrastive loss: 0.02915, Loss positive: 0.02879, Loss negative: 0.00037
2018-10-22 19:07:53.956851: Epoch [278/1000] [180/183], total loss: 0.00627, regularization loss: 0.29019, contrastive loss: 0.00627, Loss positive: 0.00000, Loss negative: 0.00627
2018-10-22 19:08:15.814098: Epoch [279/1000] [ 20/183], total loss: 0.00985, regularization loss: 0.29020, contrastive loss: 0.00985, Loss positive: 0.00985, Loss negative: 0.00000
2018-10-22 19:08:25.986781: Epoch [279/1000] [ 40/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:08:36.204187: Epoch [279/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:08:46.546305: Epoch [279/1000] [ 80/183], total loss: 0.00027, regularization loss: 0.29020, contrastive loss: 0.00027, Loss positive: 0.00000, Loss negative: 0.00027
2018-10-22 19:08:56.794463: Epoch [279/1000] [100/183], total loss: 0.00063, regularization loss: 0.29020, contrastive loss: 0.00063, Loss positive: 0.00000, Loss negative: 0.00063
2018-10-22 19:09:07.039718: Epoch [279/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:09:17.293088: Epoch [279/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:09:27.523190: Epoch [279/1000] [160/183], total loss: 0.00067, regularization loss: 0.29020, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 19:09:37.781606: Epoch [279/1000] [180/183], total loss: 0.02735, regularization loss: 0.29020, contrastive loss: 0.02735, Loss positive: 0.02735, Loss negative: 0.00000
2018-10-22 19:09:59.339917: Epoch [280/1000] [ 20/183], total loss: 0.00387, regularization loss: 0.29020, contrastive loss: 0.00387, Loss positive: 0.00000, Loss negative: 0.00387
2018-10-22 19:10:09.530547: Epoch [280/1000] [ 40/183], total loss: 0.00599, regularization loss: 0.29020, contrastive loss: 0.00599, Loss positive: 0.00000, Loss negative: 0.00599
2018-10-22 19:10:19.718797: Epoch [280/1000] [ 60/183], total loss: 0.00021, regularization loss: 0.29020, contrastive loss: 0.00021, Loss positive: 0.00000, Loss negative: 0.00021
2018-10-22 19:10:30.086973: Epoch [280/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 19:10:40.317695: Epoch [280/1000] [100/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:10:50.553801: Epoch [280/1000] [120/183], total loss: 0.01492, regularization loss: 0.29019, contrastive loss: 0.01492, Loss positive: 0.01070, Loss negative: 0.00422
2018-10-22 19:11:00.814857: Epoch [280/1000] [140/183], total loss: 0.00096, regularization loss: 0.29020, contrastive loss: 0.00096, Loss positive: 0.00000, Loss negative: 0.00096
2018-10-22 19:11:11.048326: Epoch [280/1000] [160/183], total loss: 0.00007, regularization loss: 0.29020, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 19:11:21.303286: Epoch [280/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
Recall@1: 0.27262
Recall@2: 0.39534
Recall@4: 0.52650
Recall@8: 0.66239
Recall@16: 0.78038
Recall@32: 0.87762
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 75.  90.  36.  66.  65.  84.  52.  84.  45. 114.  58.  40.  90.  77.
  22.  58.  80.  71.  44.  66.  50.  35.  75.  70.  57.  56.  67.  48.
  43.  32.  72.  59.  47.  39.  62.  50.  42.  60.  69.  41.  61.  48.
 115.  79.  36.  28.  51.  22.  35.  65.  68.  76.  58.  64.  58.  75.
  69.  52.  73.  80.  70.  57.  70.  48.  26.  66.  54.  45. 122.  66.
  85.  69.  25.  78.  40.  62.  73.  54.  27.  31.  92.  29.  32.  95.
  39.  62.  81.  65.  61.  38.  89.  36.  56.  18.  71.  53.  38.  85.
  56.  56.]
Purity is 0.240
count_cross = [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 2. 0.]
 [0. 1. 1. ... 2. 0. 0.]]
Mutual information is 2.06399
5924.0
5924
Entropy cluster is 4.54316
Entropy class is 4.60444
normalized_mutual_information is 0.45126
tp_and_fp = 193931.0
tp = 20676.0
fp is 173255.0
fn is 152074.0
RI is 0.9814563171322086
Precision is 0.10661523944083205
Recall is 0.11968740955137482
F_1 is 0.11277377338885844

normalized_mutual_information = 0.45126387924136
RI = 0.9814563171322086
F_1 = 0.11277377338885844

The NN is 0.27262
The FT is 0.14454
The ST is 0.22412
The DCG is 0.52937
The E is 0.12145
The MAP 0.11823

2018-10-22 19:12:58.646053: Epoch [281/1000] [ 20/183], total loss: 0.00174, regularization loss: 0.29020, contrastive loss: 0.00174, Loss positive: 0.00000, Loss negative: 0.00174
2018-10-22 19:13:08.750327: Epoch [281/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:13:18.962848: Epoch [281/1000] [ 60/183], total loss: 0.01722, regularization loss: 0.29020, contrastive loss: 0.01722, Loss positive: 0.01565, Loss negative: 0.00157
2018-10-22 19:13:29.142580: Epoch [281/1000] [ 80/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 19:13:39.358487: Epoch [281/1000] [100/183], total loss: 0.00084, regularization loss: 0.29020, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 19:13:49.603515: Epoch [281/1000] [120/183], total loss: 0.00361, regularization loss: 0.29020, contrastive loss: 0.00361, Loss positive: 0.00000, Loss negative: 0.00361
2018-10-22 19:13:59.988400: Epoch [281/1000] [140/183], total loss: 0.00098, regularization loss: 0.29020, contrastive loss: 0.00098, Loss positive: 0.00000, Loss negative: 0.00098
2018-10-22 19:14:10.450292: Epoch [281/1000] [160/183], total loss: 0.00692, regularization loss: 0.29020, contrastive loss: 0.00692, Loss positive: 0.00692, Loss negative: 0.00000
2018-10-22 19:14:20.977434: Epoch [281/1000] [180/183], total loss: 0.01332, regularization loss: 0.29020, contrastive loss: 0.01332, Loss positive: 0.01213, Loss negative: 0.00119
2018-10-22 19:14:45.594381: Epoch [282/1000] [ 20/183], total loss: 0.00025, regularization loss: 0.29020, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 19:14:55.833567: Epoch [282/1000] [ 40/183], total loss: 0.02898, regularization loss: 0.29020, contrastive loss: 0.02898, Loss positive: 0.02893, Loss negative: 0.00005
2018-10-22 19:15:06.075881: Epoch [282/1000] [ 60/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 19:15:16.346309: Epoch [282/1000] [ 80/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 19:15:26.572123: Epoch [282/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:15:36.828793: Epoch [282/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:15:47.145629: Epoch [282/1000] [140/183], total loss: 0.00183, regularization loss: 0.29020, contrastive loss: 0.00183, Loss positive: 0.00000, Loss negative: 0.00183
2018-10-22 19:15:57.374569: Epoch [282/1000] [160/183], total loss: 0.00943, regularization loss: 0.29020, contrastive loss: 0.00943, Loss positive: 0.00883, Loss negative: 0.00060
2018-10-22 19:16:07.616912: Epoch [282/1000] [180/183], total loss: 0.04648, regularization loss: 0.29020, contrastive loss: 0.04648, Loss positive: 0.04606, Loss negative: 0.00042
2018-10-22 19:16:30.370180: Epoch [283/1000] [ 20/183], total loss: 0.00354, regularization loss: 0.29020, contrastive loss: 0.00354, Loss positive: 0.00000, Loss negative: 0.00354
2018-10-22 19:16:40.571559: Epoch [283/1000] [ 40/183], total loss: 0.00784, regularization loss: 0.29020, contrastive loss: 0.00784, Loss positive: 0.00000, Loss negative: 0.00784
2018-10-22 19:16:50.728807: Epoch [283/1000] [ 60/183], total loss: 0.02194, regularization loss: 0.29020, contrastive loss: 0.02194, Loss positive: 0.01759, Loss negative: 0.00434
2018-10-22 19:17:00.960975: Epoch [283/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:17:11.197256: Epoch [283/1000] [100/183], total loss: 0.01290, regularization loss: 0.29020, contrastive loss: 0.01290, Loss positive: 0.01259, Loss negative: 0.00031
2018-10-22 19:17:21.438873: Epoch [283/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:17:31.675901: Epoch [283/1000] [140/183], total loss: 0.00150, regularization loss: 0.29020, contrastive loss: 0.00150, Loss positive: 0.00000, Loss negative: 0.00150
2018-10-22 19:17:41.939318: Epoch [283/1000] [160/183], total loss: 0.00151, regularization loss: 0.29020, contrastive loss: 0.00151, Loss positive: 0.00000, Loss negative: 0.00151
2018-10-22 19:17:52.325627: Epoch [283/1000] [180/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:18:13.908007: Epoch [284/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:18:24.076055: Epoch [284/1000] [ 40/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 19:18:34.274527: Epoch [284/1000] [ 60/183], total loss: 0.02043, regularization loss: 0.29020, contrastive loss: 0.02043, Loss positive: 0.01921, Loss negative: 0.00122
2018-10-22 19:18:44.491966: Epoch [284/1000] [ 80/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:18:54.778083: Epoch [284/1000] [100/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 19:19:05.082933: Epoch [284/1000] [120/183], total loss: 0.00985, regularization loss: 0.29020, contrastive loss: 0.00985, Loss positive: 0.00000, Loss negative: 0.00985
2018-10-22 19:19:15.333102: Epoch [284/1000] [140/183], total loss: 0.00247, regularization loss: 0.29020, contrastive loss: 0.00247, Loss positive: 0.00000, Loss negative: 0.00247
2018-10-22 19:19:25.626359: Epoch [284/1000] [160/183], total loss: 0.02111, regularization loss: 0.29020, contrastive loss: 0.02111, Loss positive: 0.02111, Loss negative: 0.00000
2018-10-22 19:19:36.306739: Epoch [284/1000] [180/183], total loss: 0.01046, regularization loss: 0.29020, contrastive loss: 0.01046, Loss positive: 0.00000, Loss negative: 0.01046
2018-10-22 19:19:57.582813: Epoch [285/1000] [ 20/183], total loss: 0.00292, regularization loss: 0.29019, contrastive loss: 0.00292, Loss positive: 0.00000, Loss negative: 0.00292
2018-10-22 19:20:07.765885: Epoch [285/1000] [ 40/183], total loss: 0.00566, regularization loss: 0.29019, contrastive loss: 0.00566, Loss positive: 0.00000, Loss negative: 0.00566
2018-10-22 19:20:17.964813: Epoch [285/1000] [ 60/183], total loss: 0.00542, regularization loss: 0.29019, contrastive loss: 0.00542, Loss positive: 0.00000, Loss negative: 0.00542
2018-10-22 19:20:28.344131: Epoch [285/1000] [ 80/183], total loss: 0.00131, regularization loss: 0.29020, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 19:20:38.581411: Epoch [285/1000] [100/183], total loss: 0.00235, regularization loss: 0.29020, contrastive loss: 0.00235, Loss positive: 0.00000, Loss negative: 0.00235
2018-10-22 19:20:48.805620: Epoch [285/1000] [120/183], total loss: 0.00103, regularization loss: 0.29020, contrastive loss: 0.00103, Loss positive: 0.00000, Loss negative: 0.00103
2018-10-22 19:20:59.036127: Epoch [285/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:21:09.289094: Epoch [285/1000] [160/183], total loss: 0.00991, regularization loss: 0.29020, contrastive loss: 0.00991, Loss positive: 0.00000, Loss negative: 0.00991
2018-10-22 19:21:19.544193: Epoch [285/1000] [180/183], total loss: 0.00090, regularization loss: 0.29020, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 19:21:42.392448: Epoch [286/1000] [ 20/183], total loss: 0.00243, regularization loss: 0.29019, contrastive loss: 0.00243, Loss positive: 0.00000, Loss negative: 0.00243
2018-10-22 19:21:52.605100: Epoch [286/1000] [ 40/183], total loss: 0.00013, regularization loss: 0.29019, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 19:22:02.815955: Epoch [286/1000] [ 60/183], total loss: 0.00320, regularization loss: 0.29020, contrastive loss: 0.00320, Loss positive: 0.00000, Loss negative: 0.00320
2018-10-22 19:22:13.199981: Epoch [286/1000] [ 80/183], total loss: 0.00815, regularization loss: 0.29020, contrastive loss: 0.00815, Loss positive: 0.00000, Loss negative: 0.00815
2018-10-22 19:22:23.436715: Epoch [286/1000] [100/183], total loss: 0.01547, regularization loss: 0.29020, contrastive loss: 0.01547, Loss positive: 0.01537, Loss negative: 0.00010
2018-10-22 19:22:33.652811: Epoch [286/1000] [120/183], total loss: 0.01724, regularization loss: 0.29020, contrastive loss: 0.01724, Loss positive: 0.00000, Loss negative: 0.01724
2018-10-22 19:22:43.917719: Epoch [286/1000] [140/183], total loss: 0.01562, regularization loss: 0.29020, contrastive loss: 0.01562, Loss positive: 0.01426, Loss negative: 0.00136
2018-10-22 19:22:54.629555: Epoch [286/1000] [160/183], total loss: 0.01856, regularization loss: 0.29020, contrastive loss: 0.01856, Loss positive: 0.01710, Loss negative: 0.00146
2018-10-22 19:23:04.933277: Epoch [286/1000] [180/183], total loss: 0.00178, regularization loss: 0.29020, contrastive loss: 0.00178, Loss positive: 0.00000, Loss negative: 0.00178
2018-10-22 19:23:29.443464: Epoch [287/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:23:39.667931: Epoch [287/1000] [ 40/183], total loss: 0.00041, regularization loss: 0.29020, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 19:23:49.921251: Epoch [287/1000] [ 60/183], total loss: 0.00372, regularization loss: 0.29020, contrastive loss: 0.00372, Loss positive: 0.00000, Loss negative: 0.00372
2018-10-22 19:24:00.258650: Epoch [287/1000] [ 80/183], total loss: 0.00126, regularization loss: 0.29020, contrastive loss: 0.00126, Loss positive: 0.00000, Loss negative: 0.00126
2018-10-22 19:24:10.751441: Epoch [287/1000] [100/183], total loss: 0.00180, regularization loss: 0.29020, contrastive loss: 0.00180, Loss positive: 0.00000, Loss negative: 0.00180
2018-10-22 19:24:21.022682: Epoch [287/1000] [120/183], total loss: 0.00254, regularization loss: 0.29020, contrastive loss: 0.00254, Loss positive: 0.00000, Loss negative: 0.00254
2018-10-22 19:24:31.507372: Epoch [287/1000] [140/183], total loss: 0.00123, regularization loss: 0.29020, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 19:24:42.227348: Epoch [287/1000] [160/183], total loss: 0.00752, regularization loss: 0.29020, contrastive loss: 0.00752, Loss positive: 0.00000, Loss negative: 0.00752
2018-10-22 19:24:52.690113: Epoch [287/1000] [180/183], total loss: 0.00051, regularization loss: 0.29020, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 19:25:17.393448: Epoch [288/1000] [ 20/183], total loss: 0.00580, regularization loss: 0.29020, contrastive loss: 0.00580, Loss positive: 0.00562, Loss negative: 0.00018
2018-10-22 19:25:27.602174: Epoch [288/1000] [ 40/183], total loss: 0.00245, regularization loss: 0.29020, contrastive loss: 0.00245, Loss positive: 0.00000, Loss negative: 0.00245
2018-10-22 19:25:37.766134: Epoch [288/1000] [ 60/183], total loss: 0.00017, regularization loss: 0.29020, contrastive loss: 0.00017, Loss positive: 0.00000, Loss negative: 0.00017
2018-10-22 19:25:47.965910: Epoch [288/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:25:58.177138: Epoch [288/1000] [100/183], total loss: 0.03028, regularization loss: 0.29020, contrastive loss: 0.03028, Loss positive: 0.02887, Loss negative: 0.00141
2018-10-22 19:26:08.411791: Epoch [288/1000] [120/183], total loss: 0.01102, regularization loss: 0.29020, contrastive loss: 0.01102, Loss positive: 0.01000, Loss negative: 0.00103
2018-10-22 19:26:18.663607: Epoch [288/1000] [140/183], total loss: 0.02079, regularization loss: 0.29020, contrastive loss: 0.02079, Loss positive: 0.01921, Loss negative: 0.00158
2018-10-22 19:26:28.921783: Epoch [288/1000] [160/183], total loss: 0.02594, regularization loss: 0.29020, contrastive loss: 0.02594, Loss positive: 0.02572, Loss negative: 0.00022
2018-10-22 19:26:39.270507: Epoch [288/1000] [180/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 19:27:00.784064: Epoch [289/1000] [ 20/183], total loss: 0.00380, regularization loss: 0.29020, contrastive loss: 0.00380, Loss positive: 0.00000, Loss negative: 0.00380
2018-10-22 19:27:10.994014: Epoch [289/1000] [ 40/183], total loss: 0.00143, regularization loss: 0.29020, contrastive loss: 0.00143, Loss positive: 0.00000, Loss negative: 0.00143
2018-10-22 19:27:21.267096: Epoch [289/1000] [ 60/183], total loss: 0.00006, regularization loss: 0.29020, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 19:27:31.462251: Epoch [289/1000] [ 80/183], total loss: 0.01129, regularization loss: 0.29020, contrastive loss: 0.01129, Loss positive: 0.01129, Loss negative: 0.00000
2018-10-22 19:27:41.704757: Epoch [289/1000] [100/183], total loss: 0.00546, regularization loss: 0.29020, contrastive loss: 0.00546, Loss positive: 0.00381, Loss negative: 0.00165
2018-10-22 19:27:51.972969: Epoch [289/1000] [120/183], total loss: 0.00078, regularization loss: 0.29020, contrastive loss: 0.00078, Loss positive: 0.00000, Loss negative: 0.00078
2018-10-22 19:28:02.314933: Epoch [289/1000] [140/183], total loss: 0.00918, regularization loss: 0.29019, contrastive loss: 0.00918, Loss positive: 0.00755, Loss negative: 0.00162
2018-10-22 19:28:12.544728: Epoch [289/1000] [160/183], total loss: 0.00634, regularization loss: 0.29019, contrastive loss: 0.00634, Loss positive: 0.00000, Loss negative: 0.00634
2018-10-22 19:28:22.810062: Epoch [289/1000] [180/183], total loss: 0.00001, regularization loss: 0.29019, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:28:44.753181: Epoch [290/1000] [ 20/183], total loss: 0.00676, regularization loss: 0.29019, contrastive loss: 0.00676, Loss positive: 0.00617, Loss negative: 0.00058
2018-10-22 19:28:54.931647: Epoch [290/1000] [ 40/183], total loss: 0.01201, regularization loss: 0.29019, contrastive loss: 0.01201, Loss positive: 0.01099, Loss negative: 0.00103
2018-10-22 19:29:05.156578: Epoch [290/1000] [ 60/183], total loss: 0.00153, regularization loss: 0.29020, contrastive loss: 0.00153, Loss positive: 0.00000, Loss negative: 0.00153
2018-10-22 19:29:15.560694: Epoch [290/1000] [ 80/183], total loss: 0.00020, regularization loss: 0.29020, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 19:29:25.836808: Epoch [290/1000] [100/183], total loss: 0.00053, regularization loss: 0.29020, contrastive loss: 0.00053, Loss positive: 0.00000, Loss negative: 0.00053
2018-10-22 19:29:36.075434: Epoch [290/1000] [120/183], total loss: 0.02338, regularization loss: 0.29020, contrastive loss: 0.02338, Loss positive: 0.01659, Loss negative: 0.00679
2018-10-22 19:29:46.334620: Epoch [290/1000] [140/183], total loss: 0.01104, regularization loss: 0.29020, contrastive loss: 0.01104, Loss positive: 0.00000, Loss negative: 0.01104
2018-10-22 19:29:56.552505: Epoch [290/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:30:06.797625: Epoch [290/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
Recall@1: 0.26367
Recall@2: 0.37677
Recall@4: 0.51367
Recall@8: 0.65378
Recall@16: 0.78005
Recall@32: 0.87829
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 72.  65.  54.  50.  49.  60.  37.  69.  67.  25.  73.  60.  44.  87.
  67.  79.  36.  50.  64.  58.  27.  26.  37.  59. 109.  73.  54.  76.
  87.  75.  88.  56.  56.  46.  19.  93.  36.  65. 106.  57.  82.  66.
  28.  47.  83.  68.  36.  75.  84.  50.  29.  54.  75.  63.  85.  69.
  39.  89.  42.  55.  65.  45.  57.  20.  95.  77. 116.  64.  59.  46.
  29.  47.  51.  70.  67.  48.  44.  38.  67.  48.  65.  48.  62.  83.
  43.  91.  41.  44.  40.  40.  37.  54.  70.  66. 104.  51.  46.  57.
  62.  37.]
Purity is 0.248
count_cross = [[ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 ...
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  0. 16.]
 [ 0.  0.  0. ...  0.  0.  0.]]
Mutual information is 2.08716
5924.0
5924
Entropy cluster is 4.54641
Entropy class is 4.60444
normalized_mutual_information is 0.45617
tp_and_fp = 192980.0
tp = 20876.0
fp is 172104.0
fn is 151874.0
RI is 0.9815333238409693
Precision is 0.10817701316198569
Recall is 0.1208451519536903
F_1 is 0.11416071965657724

normalized_mutual_information = 0.45616754251332126
RI = 0.9815333238409693
F_1 = 0.11416071965657724

The NN is 0.26367
The FT is 0.14635
The ST is 0.22779
The DCG is 0.53080
The E is 0.12298
The MAP 0.11964

2018-10-22 19:31:38.357053: Epoch [291/1000] [ 20/183], total loss: 0.00755, regularization loss: 0.29020, contrastive loss: 0.00755, Loss positive: 0.00694, Loss negative: 0.00061
2018-10-22 19:31:48.475769: Epoch [291/1000] [ 40/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 19:31:58.602340: Epoch [291/1000] [ 60/183], total loss: 0.02540, regularization loss: 0.29020, contrastive loss: 0.02540, Loss positive: 0.02293, Loss negative: 0.00246
2018-10-22 19:32:08.718243: Epoch [291/1000] [ 80/183], total loss: 0.00014, regularization loss: 0.29019, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 19:32:18.948673: Epoch [291/1000] [100/183], total loss: 0.00030, regularization loss: 0.29019, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 19:32:29.349190: Epoch [291/1000] [120/183], total loss: 0.00764, regularization loss: 0.29020, contrastive loss: 0.00764, Loss positive: 0.00763, Loss negative: 0.00001
2018-10-22 19:32:39.642154: Epoch [291/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:32:50.081089: Epoch [291/1000] [160/183], total loss: 0.00314, regularization loss: 0.29020, contrastive loss: 0.00314, Loss positive: 0.00000, Loss negative: 0.00314
2018-10-22 19:33:00.739926: Epoch [291/1000] [180/183], total loss: 0.00742, regularization loss: 0.29020, contrastive loss: 0.00742, Loss positive: 0.00742, Loss negative: 0.00000
2018-10-22 19:33:25.190387: Epoch [292/1000] [ 20/183], total loss: 0.00457, regularization loss: 0.29019, contrastive loss: 0.00457, Loss positive: 0.00000, Loss negative: 0.00457
2018-10-22 19:33:35.404292: Epoch [292/1000] [ 40/183], total loss: 0.00135, regularization loss: 0.29019, contrastive loss: 0.00135, Loss positive: 0.00000, Loss negative: 0.00135
2018-10-22 19:33:45.656423: Epoch [292/1000] [ 60/183], total loss: 0.00040, regularization loss: 0.29019, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 19:33:55.901850: Epoch [292/1000] [ 80/183], total loss: 0.02365, regularization loss: 0.29020, contrastive loss: 0.02365, Loss positive: 0.02339, Loss negative: 0.00026
2018-10-22 19:34:06.250397: Epoch [292/1000] [100/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 19:34:16.726667: Epoch [292/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:34:27.130229: Epoch [292/1000] [140/183], total loss: 0.00022, regularization loss: 0.29020, contrastive loss: 0.00022, Loss positive: 0.00000, Loss negative: 0.00022
2018-10-22 19:34:37.386037: Epoch [292/1000] [160/183], total loss: 0.00028, regularization loss: 0.29020, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 19:34:47.936298: Epoch [292/1000] [180/183], total loss: 0.00202, regularization loss: 0.29020, contrastive loss: 0.00202, Loss positive: 0.00000, Loss negative: 0.00202
2018-10-22 19:35:12.438468: Epoch [293/1000] [ 20/183], total loss: 0.00035, regularization loss: 0.29020, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 19:35:22.639809: Epoch [293/1000] [ 40/183], total loss: 0.00158, regularization loss: 0.29020, contrastive loss: 0.00158, Loss positive: 0.00000, Loss negative: 0.00158
2018-10-22 19:35:32.818329: Epoch [293/1000] [ 60/183], total loss: 0.00052, regularization loss: 0.29020, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 19:35:43.007896: Epoch [293/1000] [ 80/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 19:35:53.255812: Epoch [293/1000] [100/183], total loss: 0.00054, regularization loss: 0.29020, contrastive loss: 0.00054, Loss positive: 0.00000, Loss negative: 0.00054
2018-10-22 19:36:03.510339: Epoch [293/1000] [120/183], total loss: 0.00079, regularization loss: 0.29020, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 19:36:13.952704: Epoch [293/1000] [140/183], total loss: 0.01315, regularization loss: 0.29020, contrastive loss: 0.01315, Loss positive: 0.01091, Loss negative: 0.00224
2018-10-22 19:36:24.378542: Epoch [293/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:36:34.610966: Epoch [293/1000] [180/183], total loss: 0.00167, regularization loss: 0.29020, contrastive loss: 0.00167, Loss positive: 0.00000, Loss negative: 0.00167
2018-10-22 19:36:56.082581: Epoch [294/1000] [ 20/183], total loss: 0.00175, regularization loss: 0.29020, contrastive loss: 0.00175, Loss positive: 0.00000, Loss negative: 0.00175
2018-10-22 19:37:06.242719: Epoch [294/1000] [ 40/183], total loss: 0.00378, regularization loss: 0.29020, contrastive loss: 0.00378, Loss positive: 0.00000, Loss negative: 0.00378
2018-10-22 19:37:16.476388: Epoch [294/1000] [ 60/183], total loss: 0.02357, regularization loss: 0.29020, contrastive loss: 0.02357, Loss positive: 0.02357, Loss negative: 0.00000
2018-10-22 19:37:26.830921: Epoch [294/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:37:37.058957: Epoch [294/1000] [100/183], total loss: 0.00124, regularization loss: 0.29019, contrastive loss: 0.00124, Loss positive: 0.00000, Loss negative: 0.00124
2018-10-22 19:37:47.302594: Epoch [294/1000] [120/183], total loss: 0.00000, regularization loss: 0.29019, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:37:57.559168: Epoch [294/1000] [140/183], total loss: 0.03526, regularization loss: 0.29019, contrastive loss: 0.03526, Loss positive: 0.03346, Loss negative: 0.00180
2018-10-22 19:38:07.803224: Epoch [294/1000] [160/183], total loss: 0.01113, regularization loss: 0.29019, contrastive loss: 0.01113, Loss positive: 0.00000, Loss negative: 0.01113
2018-10-22 19:38:18.033854: Epoch [294/1000] [180/183], total loss: 0.00030, regularization loss: 0.29020, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 19:38:39.749167: Epoch [295/1000] [ 20/183], total loss: 0.00092, regularization loss: 0.29019, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 19:38:49.943920: Epoch [295/1000] [ 40/183], total loss: 0.00055, regularization loss: 0.29020, contrastive loss: 0.00055, Loss positive: 0.00000, Loss negative: 0.00055
2018-10-22 19:39:00.143625: Epoch [295/1000] [ 60/183], total loss: 0.00363, regularization loss: 0.29020, contrastive loss: 0.00363, Loss positive: 0.00000, Loss negative: 0.00363
2018-10-22 19:39:10.368280: Epoch [295/1000] [ 80/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 19:39:20.608714: Epoch [295/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:39:30.861732: Epoch [295/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:39:41.098206: Epoch [295/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:39:51.700090: Epoch [295/1000] [160/183], total loss: 0.00626, regularization loss: 0.29020, contrastive loss: 0.00626, Loss positive: 0.00000, Loss negative: 0.00626
2018-10-22 19:40:02.067909: Epoch [295/1000] [180/183], total loss: 0.01601, regularization loss: 0.29020, contrastive loss: 0.01601, Loss positive: 0.01601, Loss negative: 0.00000
2018-10-22 19:40:23.403208: Epoch [296/1000] [ 20/183], total loss: 0.01097, regularization loss: 0.29019, contrastive loss: 0.01097, Loss positive: 0.01021, Loss negative: 0.00077
2018-10-22 19:40:33.620714: Epoch [296/1000] [ 40/183], total loss: 0.01762, regularization loss: 0.29019, contrastive loss: 0.01762, Loss positive: 0.01715, Loss negative: 0.00047
2018-10-22 19:40:43.832716: Epoch [296/1000] [ 60/183], total loss: 0.00087, regularization loss: 0.29019, contrastive loss: 0.00087, Loss positive: 0.00000, Loss negative: 0.00087
2018-10-22 19:40:54.200603: Epoch [296/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:41:04.429948: Epoch [296/1000] [100/183], total loss: 0.00061, regularization loss: 0.29020, contrastive loss: 0.00061, Loss positive: 0.00000, Loss negative: 0.00061
2018-10-22 19:41:14.683901: Epoch [296/1000] [120/183], total loss: 0.00564, regularization loss: 0.29020, contrastive loss: 0.00564, Loss positive: 0.00000, Loss negative: 0.00564
2018-10-22 19:41:24.919719: Epoch [296/1000] [140/183], total loss: 0.00298, regularization loss: 0.29020, contrastive loss: 0.00298, Loss positive: 0.00000, Loss negative: 0.00298
2018-10-22 19:41:35.187205: Epoch [296/1000] [160/183], total loss: 0.01348, regularization loss: 0.29020, contrastive loss: 0.01348, Loss positive: 0.01348, Loss negative: 0.00000
2018-10-22 19:41:45.634357: Epoch [296/1000] [180/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 19:42:08.354283: Epoch [297/1000] [ 20/183], total loss: 0.02734, regularization loss: 0.29019, contrastive loss: 0.02734, Loss positive: 0.01622, Loss negative: 0.01112
2018-10-22 19:42:18.564345: Epoch [297/1000] [ 40/183], total loss: 0.00653, regularization loss: 0.29019, contrastive loss: 0.00653, Loss positive: 0.00000, Loss negative: 0.00653
2018-10-22 19:42:28.786756: Epoch [297/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:42:39.152303: Epoch [297/1000] [ 80/183], total loss: 0.01419, regularization loss: 0.29020, contrastive loss: 0.01419, Loss positive: 0.00990, Loss negative: 0.00429
2018-10-22 19:42:49.421533: Epoch [297/1000] [100/183], total loss: 0.04204, regularization loss: 0.29020, contrastive loss: 0.04204, Loss positive: 0.04076, Loss negative: 0.00128
2018-10-22 19:42:59.677801: Epoch [297/1000] [120/183], total loss: 0.00902, regularization loss: 0.29020, contrastive loss: 0.00902, Loss positive: 0.00902, Loss negative: 0.00000
2018-10-22 19:43:10.138226: Epoch [297/1000] [140/183], total loss: 0.00437, regularization loss: 0.29020, contrastive loss: 0.00437, Loss positive: 0.00437, Loss negative: 0.00000
2018-10-22 19:43:20.752633: Epoch [297/1000] [160/183], total loss: 0.03321, regularization loss: 0.29019, contrastive loss: 0.03321, Loss positive: 0.03167, Loss negative: 0.00154
2018-10-22 19:43:31.082326: Epoch [297/1000] [180/183], total loss: 0.00057, regularization loss: 0.29020, contrastive loss: 0.00057, Loss positive: 0.00000, Loss negative: 0.00057
2018-10-22 19:43:55.489600: Epoch [298/1000] [ 20/183], total loss: 0.01303, regularization loss: 0.29020, contrastive loss: 0.01303, Loss positive: 0.01096, Loss negative: 0.00207
2018-10-22 19:44:05.748669: Epoch [298/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:44:16.010787: Epoch [298/1000] [ 60/183], total loss: 0.00735, regularization loss: 0.29020, contrastive loss: 0.00735, Loss positive: 0.00000, Loss negative: 0.00735
2018-10-22 19:44:26.280597: Epoch [298/1000] [ 80/183], total loss: 0.01103, regularization loss: 0.29020, contrastive loss: 0.01103, Loss positive: 0.00000, Loss negative: 0.01103
2018-10-22 19:44:36.631022: Epoch [298/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:44:46.974338: Epoch [298/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:44:57.249787: Epoch [298/1000] [140/183], total loss: 0.00449, regularization loss: 0.29020, contrastive loss: 0.00449, Loss positive: 0.00000, Loss negative: 0.00449
2018-10-22 19:45:07.758631: Epoch [298/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:45:18.113710: Epoch [298/1000] [180/183], total loss: 0.00479, regularization loss: 0.29020, contrastive loss: 0.00479, Loss positive: 0.00000, Loss negative: 0.00479
2018-10-22 19:45:40.817606: Epoch [299/1000] [ 20/183], total loss: 0.03585, regularization loss: 0.29020, contrastive loss: 0.03585, Loss positive: 0.03545, Loss negative: 0.00040
2018-10-22 19:45:51.033565: Epoch [299/1000] [ 40/183], total loss: 0.00025, regularization loss: 0.29020, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 19:46:01.240460: Epoch [299/1000] [ 60/183], total loss: 0.00073, regularization loss: 0.29020, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 19:46:11.443147: Epoch [299/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:46:21.690872: Epoch [299/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:46:31.901084: Epoch [299/1000] [120/183], total loss: 0.00052, regularization loss: 0.29020, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 19:46:42.177259: Epoch [299/1000] [140/183], total loss: 0.00088, regularization loss: 0.29020, contrastive loss: 0.00088, Loss positive: 0.00000, Loss negative: 0.00088
2018-10-22 19:46:52.413255: Epoch [299/1000] [160/183], total loss: 0.02321, regularization loss: 0.29020, contrastive loss: 0.02321, Loss positive: 0.00698, Loss negative: 0.01622
2018-10-22 19:47:02.656776: Epoch [299/1000] [180/183], total loss: 0.00049, regularization loss: 0.29020, contrastive loss: 0.00049, Loss positive: 0.00000, Loss negative: 0.00049
2018-10-22 19:47:24.215606: Epoch [300/1000] [ 20/183], total loss: 0.00785, regularization loss: 0.29020, contrastive loss: 0.00785, Loss positive: 0.00709, Loss negative: 0.00076
2018-10-22 19:47:34.412657: Epoch [300/1000] [ 40/183], total loss: 0.00055, regularization loss: 0.29020, contrastive loss: 0.00055, Loss positive: 0.00000, Loss negative: 0.00055
2018-10-22 19:47:44.613232: Epoch [300/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:47:54.856229: Epoch [300/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:48:05.097998: Epoch [300/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:48:15.348355: Epoch [300/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:48:25.606185: Epoch [300/1000] [140/183], total loss: 0.00213, regularization loss: 0.29020, contrastive loss: 0.00213, Loss positive: 0.00000, Loss negative: 0.00213
2018-10-22 19:48:35.836814: Epoch [300/1000] [160/183], total loss: 0.01247, regularization loss: 0.29020, contrastive loss: 0.01247, Loss positive: 0.01232, Loss negative: 0.00015
2018-10-22 19:48:46.081857: Epoch [300/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
Recall@1: 0.27768
Recall@2: 0.40007
Recall@4: 0.53342
Recall@8: 0.66155
Recall@16: 0.77650
Recall@32: 0.86901
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 87.  58.  53.  50.  38. 105.  65.  31.  41. 106.  57.  36.  59.  54.
  98. 102.  32.  46.  70.  51.  63.  61.  40.  93.  40.  48.  76.  91.
  81.  79.  69.  70.  80.  52.  39.  50.  59.  56.  49.  36.  31.  38.
  63.  31. 125.  52.  40.  60.  66.  47.  95.  49.  82.  58.  60.  27.
  62.  57.  56.  65.  83.  50.  49.  39.  43.  63.  52.  43.  94.  59.
  38.  53.  26.  88.  87.  38.  41.  76.  70.  86.  59.  60.  86.  52.
  94.  58.  47.  58.  71.  63.  62.  31.  58.  48.  72.  42.  86.  31.
  28.   5.]
Purity is 0.253
count_cross = [[0. 1. 3. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [2. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 1. 2. ... 0. 0. 0.]
 [5. 0. 0. ... 0. 0. 0.]]
Mutual information is 2.08839
5924.0
5924
Entropy cluster is 4.54029
Entropy class is 4.60444
normalized_mutual_information is 0.45674
tp_and_fp = 194999.0
tp = 21609.0
fp is 173390.0
fn is 151141.0
RI is 0.9815018029601812
Precision is 0.11081595290232257
Recall is 0.12508827785817656
F_1 is 0.11752037395071094

normalized_mutual_information = 0.45674129482093156
RI = 0.9815018029601812
F_1 = 0.11752037395071094

The NN is 0.27768
The FT is 0.14694
The ST is 0.22934
The DCG is 0.53238
The E is 0.12396
The MAP 0.12062

2018-10-22 19:50:16.412193: Epoch [301/1000] [ 20/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 19:50:26.515093: Epoch [301/1000] [ 40/183], total loss: 0.01277, regularization loss: 0.29020, contrastive loss: 0.01277, Loss positive: 0.00508, Loss negative: 0.00769
2018-10-22 19:50:36.622328: Epoch [301/1000] [ 60/183], total loss: 0.00003, regularization loss: 0.29020, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 19:50:46.746293: Epoch [301/1000] [ 80/183], total loss: 0.01484, regularization loss: 0.29020, contrastive loss: 0.01484, Loss positive: 0.01484, Loss negative: 0.00000
2018-10-22 19:50:56.864823: Epoch [301/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:51:07.090595: Epoch [301/1000] [120/183], total loss: 0.00995, regularization loss: 0.29020, contrastive loss: 0.00995, Loss positive: 0.00957, Loss negative: 0.00038
2018-10-22 19:51:17.306215: Epoch [301/1000] [140/183], total loss: 0.00125, regularization loss: 0.29020, contrastive loss: 0.00125, Loss positive: 0.00000, Loss negative: 0.00125
2018-10-22 19:51:27.547789: Epoch [301/1000] [160/183], total loss: 0.02758, regularization loss: 0.29020, contrastive loss: 0.02758, Loss positive: 0.02547, Loss negative: 0.00211
2018-10-22 19:51:37.806838: Epoch [301/1000] [180/183], total loss: 0.00035, regularization loss: 0.29020, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 19:52:00.889721: Epoch [302/1000] [ 20/183], total loss: 0.02713, regularization loss: 0.29020, contrastive loss: 0.02713, Loss positive: 0.02484, Loss negative: 0.00230
2018-10-22 19:52:11.090291: Epoch [302/1000] [ 40/183], total loss: 0.01635, regularization loss: 0.29020, contrastive loss: 0.01635, Loss positive: 0.01613, Loss negative: 0.00022
2018-10-22 19:52:21.305834: Epoch [302/1000] [ 60/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:52:31.672954: Epoch [302/1000] [ 80/183], total loss: 0.00228, regularization loss: 0.29020, contrastive loss: 0.00228, Loss positive: 0.00000, Loss negative: 0.00228
2018-10-22 19:52:41.903083: Epoch [302/1000] [100/183], total loss: 0.01457, regularization loss: 0.29020, contrastive loss: 0.01457, Loss positive: 0.01418, Loss negative: 0.00039
2018-10-22 19:52:52.193797: Epoch [302/1000] [120/183], total loss: 0.01274, regularization loss: 0.29020, contrastive loss: 0.01274, Loss positive: 0.01127, Loss negative: 0.00146
2018-10-22 19:53:02.808032: Epoch [302/1000] [140/183], total loss: 0.00612, regularization loss: 0.29020, contrastive loss: 0.00612, Loss positive: 0.00000, Loss negative: 0.00612
2018-10-22 19:53:13.476084: Epoch [302/1000] [160/183], total loss: 0.00597, regularization loss: 0.29020, contrastive loss: 0.00597, Loss positive: 0.00000, Loss negative: 0.00597
2018-10-22 19:53:23.985992: Epoch [302/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:53:48.442371: Epoch [303/1000] [ 20/183], total loss: 0.01980, regularization loss: 0.29020, contrastive loss: 0.01980, Loss positive: 0.01980, Loss negative: 0.00000
2018-10-22 19:53:58.699499: Epoch [303/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:54:08.902296: Epoch [303/1000] [ 60/183], total loss: 0.00015, regularization loss: 0.29020, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 19:54:19.174293: Epoch [303/1000] [ 80/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 19:54:29.438956: Epoch [303/1000] [100/183], total loss: 0.04076, regularization loss: 0.29020, contrastive loss: 0.04076, Loss positive: 0.04005, Loss negative: 0.00071
2018-10-22 19:54:39.713132: Epoch [303/1000] [120/183], total loss: 0.00051, regularization loss: 0.29020, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 19:54:50.252870: Epoch [303/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:55:00.761941: Epoch [303/1000] [160/183], total loss: 0.00151, regularization loss: 0.29020, contrastive loss: 0.00151, Loss positive: 0.00000, Loss negative: 0.00151
2018-10-22 19:55:11.152229: Epoch [303/1000] [180/183], total loss: 0.00006, regularization loss: 0.29020, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 19:55:34.376113: Epoch [304/1000] [ 20/183], total loss: 0.00118, regularization loss: 0.29020, contrastive loss: 0.00118, Loss positive: 0.00000, Loss negative: 0.00118
2018-10-22 19:55:44.583730: Epoch [304/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:55:54.803526: Epoch [304/1000] [ 60/183], total loss: 0.00081, regularization loss: 0.29020, contrastive loss: 0.00081, Loss positive: 0.00000, Loss negative: 0.00081
2018-10-22 19:56:05.009959: Epoch [304/1000] [ 80/183], total loss: 0.00163, regularization loss: 0.29020, contrastive loss: 0.00163, Loss positive: 0.00000, Loss negative: 0.00163
2018-10-22 19:56:15.505638: Epoch [304/1000] [100/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 19:56:25.784786: Epoch [304/1000] [120/183], total loss: 0.00212, regularization loss: 0.29020, contrastive loss: 0.00212, Loss positive: 0.00000, Loss negative: 0.00212
2018-10-22 19:56:35.995861: Epoch [304/1000] [140/183], total loss: 0.05369, regularization loss: 0.29020, contrastive loss: 0.05369, Loss positive: 0.05361, Loss negative: 0.00007
2018-10-22 19:56:46.252877: Epoch [304/1000] [160/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 19:56:56.502079: Epoch [304/1000] [180/183], total loss: 0.00669, regularization loss: 0.29020, contrastive loss: 0.00669, Loss positive: 0.00668, Loss negative: 0.00001
2018-10-22 19:57:18.079510: Epoch [305/1000] [ 20/183], total loss: 0.00259, regularization loss: 0.29020, contrastive loss: 0.00259, Loss positive: 0.00000, Loss negative: 0.00259
2018-10-22 19:57:28.268201: Epoch [305/1000] [ 40/183], total loss: 0.02284, regularization loss: 0.29020, contrastive loss: 0.02284, Loss positive: 0.02191, Loss negative: 0.00093
2018-10-22 19:57:38.460775: Epoch [305/1000] [ 60/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 19:57:48.706315: Epoch [305/1000] [ 80/183], total loss: 0.03317, regularization loss: 0.29020, contrastive loss: 0.03317, Loss positive: 0.03317, Loss negative: 0.00000
2018-10-22 19:57:58.943609: Epoch [305/1000] [100/183], total loss: 0.02015, regularization loss: 0.29020, contrastive loss: 0.02015, Loss positive: 0.02015, Loss negative: 0.00000
2018-10-22 19:58:09.183243: Epoch [305/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 19:58:19.412619: Epoch [305/1000] [140/183], total loss: 0.00411, regularization loss: 0.29020, contrastive loss: 0.00411, Loss positive: 0.00000, Loss negative: 0.00411
2018-10-22 19:58:29.664715: Epoch [305/1000] [160/183], total loss: 0.00003, regularization loss: 0.29020, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 19:58:39.917164: Epoch [305/1000] [180/183], total loss: 0.00569, regularization loss: 0.29020, contrastive loss: 0.00569, Loss positive: 0.00000, Loss negative: 0.00569
2018-10-22 19:59:01.563225: Epoch [306/1000] [ 20/183], total loss: 0.04365, regularization loss: 0.29020, contrastive loss: 0.04365, Loss positive: 0.04354, Loss negative: 0.00011
2018-10-22 19:59:11.746054: Epoch [306/1000] [ 40/183], total loss: 0.00096, regularization loss: 0.29020, contrastive loss: 0.00096, Loss positive: 0.00000, Loss negative: 0.00096
2018-10-22 19:59:21.967609: Epoch [306/1000] [ 60/183], total loss: 0.00290, regularization loss: 0.29020, contrastive loss: 0.00290, Loss positive: 0.00000, Loss negative: 0.00290
2018-10-22 19:59:32.226871: Epoch [306/1000] [ 80/183], total loss: 0.00015, regularization loss: 0.29020, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 19:59:42.459066: Epoch [306/1000] [100/183], total loss: 0.03531, regularization loss: 0.29020, contrastive loss: 0.03531, Loss positive: 0.03531, Loss negative: 0.00000
2018-10-22 19:59:52.735527: Epoch [306/1000] [120/183], total loss: 0.00068, regularization loss: 0.29020, contrastive loss: 0.00068, Loss positive: 0.00000, Loss negative: 0.00068
2018-10-22 20:00:02.961700: Epoch [306/1000] [140/183], total loss: 0.00309, regularization loss: 0.29020, contrastive loss: 0.00309, Loss positive: 0.00000, Loss negative: 0.00309
2018-10-22 20:00:13.222663: Epoch [306/1000] [160/183], total loss: 0.01218, regularization loss: 0.29020, contrastive loss: 0.01218, Loss positive: 0.00000, Loss negative: 0.01218
2018-10-22 20:00:23.453953: Epoch [306/1000] [180/183], total loss: 0.00557, regularization loss: 0.29020, contrastive loss: 0.00557, Loss positive: 0.00000, Loss negative: 0.00557
2018-10-22 20:00:45.088315: Epoch [307/1000] [ 20/183], total loss: 0.00231, regularization loss: 0.29020, contrastive loss: 0.00231, Loss positive: 0.00000, Loss negative: 0.00231
2018-10-22 20:00:55.309236: Epoch [307/1000] [ 40/183], total loss: 0.02996, regularization loss: 0.29020, contrastive loss: 0.02996, Loss positive: 0.02945, Loss negative: 0.00051
2018-10-22 20:01:05.467917: Epoch [307/1000] [ 60/183], total loss: 0.00715, regularization loss: 0.29020, contrastive loss: 0.00715, Loss positive: 0.00715, Loss negative: 0.00000
2018-10-22 20:01:15.761638: Epoch [307/1000] [ 80/183], total loss: 0.00211, regularization loss: 0.29020, contrastive loss: 0.00211, Loss positive: 0.00000, Loss negative: 0.00211
2018-10-22 20:01:26.001229: Epoch [307/1000] [100/183], total loss: 0.00206, regularization loss: 0.29020, contrastive loss: 0.00206, Loss positive: 0.00000, Loss negative: 0.00206
2018-10-22 20:01:36.274989: Epoch [307/1000] [120/183], total loss: 0.00235, regularization loss: 0.29020, contrastive loss: 0.00235, Loss positive: 0.00000, Loss negative: 0.00235
2018-10-22 20:01:46.530606: Epoch [307/1000] [140/183], total loss: 0.00028, regularization loss: 0.29020, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
2018-10-22 20:01:56.752385: Epoch [307/1000] [160/183], total loss: 0.00179, regularization loss: 0.29020, contrastive loss: 0.00179, Loss positive: 0.00000, Loss negative: 0.00179
2018-10-22 20:02:07.382427: Epoch [307/1000] [180/183], total loss: 0.00142, regularization loss: 0.29020, contrastive loss: 0.00142, Loss positive: 0.00000, Loss negative: 0.00142
2018-10-22 20:02:30.198091: Epoch [308/1000] [ 20/183], total loss: 0.00339, regularization loss: 0.29020, contrastive loss: 0.00339, Loss positive: 0.00000, Loss negative: 0.00339
2018-10-22 20:02:40.446261: Epoch [308/1000] [ 40/183], total loss: 0.00073, regularization loss: 0.29020, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 20:02:50.706278: Epoch [308/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:03:01.072450: Epoch [308/1000] [ 80/183], total loss: 0.01592, regularization loss: 0.29020, contrastive loss: 0.01592, Loss positive: 0.00000, Loss negative: 0.01592
2018-10-22 20:03:11.402538: Epoch [308/1000] [100/183], total loss: 0.03911, regularization loss: 0.29020, contrastive loss: 0.03911, Loss positive: 0.03895, Loss negative: 0.00016
2018-10-22 20:03:21.658722: Epoch [308/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:03:32.097395: Epoch [308/1000] [140/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 20:03:42.523974: Epoch [308/1000] [160/183], total loss: 0.00148, regularization loss: 0.29020, contrastive loss: 0.00148, Loss positive: 0.00000, Loss negative: 0.00148
2018-10-22 20:03:52.983045: Epoch [308/1000] [180/183], total loss: 0.02454, regularization loss: 0.29020, contrastive loss: 0.02454, Loss positive: 0.01993, Loss negative: 0.00462
2018-10-22 20:04:18.080160: Epoch [309/1000] [ 20/183], total loss: 0.01164, regularization loss: 0.29020, contrastive loss: 0.01164, Loss positive: 0.01144, Loss negative: 0.00020
2018-10-22 20:04:28.310863: Epoch [309/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:04:38.545444: Epoch [309/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:04:48.931954: Epoch [309/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:04:59.249743: Epoch [309/1000] [100/183], total loss: 0.01259, regularization loss: 0.29020, contrastive loss: 0.01259, Loss positive: 0.01234, Loss negative: 0.00025
2018-10-22 20:05:09.766219: Epoch [309/1000] [120/183], total loss: 0.00176, regularization loss: 0.29020, contrastive loss: 0.00176, Loss positive: 0.00000, Loss negative: 0.00176
2018-10-22 20:05:20.017463: Epoch [309/1000] [140/183], total loss: 0.00388, regularization loss: 0.29020, contrastive loss: 0.00388, Loss positive: 0.00000, Loss negative: 0.00388
2018-10-22 20:05:30.242365: Epoch [309/1000] [160/183], total loss: 0.00014, regularization loss: 0.29020, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 20:05:40.508869: Epoch [309/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:06:03.121621: Epoch [310/1000] [ 20/183], total loss: 0.01579, regularization loss: 0.29020, contrastive loss: 0.01579, Loss positive: 0.00000, Loss negative: 0.01579
2018-10-22 20:06:13.309705: Epoch [310/1000] [ 40/183], total loss: 0.00996, regularization loss: 0.29020, contrastive loss: 0.00996, Loss positive: 0.00939, Loss negative: 0.00057
2018-10-22 20:06:23.502299: Epoch [310/1000] [ 60/183], total loss: 0.00244, regularization loss: 0.29020, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 20:06:33.724737: Epoch [310/1000] [ 80/183], total loss: 0.00238, regularization loss: 0.29020, contrastive loss: 0.00238, Loss positive: 0.00000, Loss negative: 0.00238
2018-10-22 20:06:43.963919: Epoch [310/1000] [100/183], total loss: 0.02331, regularization loss: 0.29020, contrastive loss: 0.02331, Loss positive: 0.02131, Loss negative: 0.00201
2018-10-22 20:06:54.194075: Epoch [310/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:07:04.436919: Epoch [310/1000] [140/183], total loss: 0.01837, regularization loss: 0.29020, contrastive loss: 0.01837, Loss positive: 0.01596, Loss negative: 0.00241
2018-10-22 20:07:14.680439: Epoch [310/1000] [160/183], total loss: 0.02497, regularization loss: 0.29020, contrastive loss: 0.02497, Loss positive: 0.02433, Loss negative: 0.00063
2018-10-22 20:07:24.954763: Epoch [310/1000] [180/183], total loss: 0.00076, regularization loss: 0.29020, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
Recall@1: 0.28258
Recall@2: 0.39770
Recall@4: 0.52684
Recall@8: 0.65800
Recall@16: 0.77819
Recall@32: 0.87441
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 74.  81.  57.  71.  41.  56.  71.  49. 109.  56.  51.  54.  65.  45.
  54.  55.  46.  51.  71.  58.  62.  39.  44.  50.  60.  51.  71.  59.
  78.  50.  57. 128.  49.  41.  88.  29.  25.  65.  44.  86.  45.  89.
  46.  54.  61.  35.  56.  49.  80.  12.  36.  42.  76.  65.  72.  62.
  52.  46.  79.  80.  29.  56.  57.  85.  48. 100.  52.  41.  89.  67.
  84.  41.  58.  42.  70.  51.  78.  69.  48.  61.  66.  58.  93.  54.
  21.  46.  50.  32.  46.  64.  71.  69.  63.  45.  68.  55.  64.  79.
  64.  62.]
Purity is 0.262
count_cross = [[0. 1. 0. ... 7. 4. 0.]
 [0. 0. 0. ... 8. 4. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 2.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 6. 0.]]
Mutual information is 2.11843
5924.0
5924
Entropy cluster is 4.55699
Entropy class is 4.60444
normalized_mutual_information is 0.46247
tp_and_fp = 189370.0
tp = 22441.0
fp is 166929.0
fn is 150309.0
RI is 0.981917502388006
Precision is 0.11850345883719703
Recall is 0.12990448625180898
F_1 is 0.12394233955594831

normalized_mutual_information = 0.46246755895209263
RI = 0.981917502388006
F_1 = 0.12394233955594831

The NN is 0.28258
The FT is 0.14735
The ST is 0.22979
The DCG is 0.53313
The E is 0.12404
The MAP 0.12145

2018-10-22 20:08:55.068669: Epoch [311/1000] [ 20/183], total loss: 0.00283, regularization loss: 0.29020, contrastive loss: 0.00283, Loss positive: 0.00000, Loss negative: 0.00283
2018-10-22 20:09:05.165493: Epoch [311/1000] [ 40/183], total loss: 0.00156, regularization loss: 0.29020, contrastive loss: 0.00156, Loss positive: 0.00000, Loss negative: 0.00156
2018-10-22 20:09:15.288485: Epoch [311/1000] [ 60/183], total loss: 0.04725, regularization loss: 0.29020, contrastive loss: 0.04725, Loss positive: 0.04606, Loss negative: 0.00119
2018-10-22 20:09:25.431057: Epoch [311/1000] [ 80/183], total loss: 0.00010, regularization loss: 0.29020, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 20:09:35.559175: Epoch [311/1000] [100/183], total loss: 0.00273, regularization loss: 0.29020, contrastive loss: 0.00273, Loss positive: 0.00000, Loss negative: 0.00273
2018-10-22 20:09:45.775937: Epoch [311/1000] [120/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 20:09:56.088543: Epoch [311/1000] [140/183], total loss: 0.00125, regularization loss: 0.29020, contrastive loss: 0.00125, Loss positive: 0.00000, Loss negative: 0.00125
2018-10-22 20:10:06.312686: Epoch [311/1000] [160/183], total loss: 0.00025, regularization loss: 0.29020, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 20:10:16.575702: Epoch [311/1000] [180/183], total loss: 0.00579, regularization loss: 0.29020, contrastive loss: 0.00579, Loss positive: 0.00000, Loss negative: 0.00579
2018-10-22 20:10:37.978959: Epoch [312/1000] [ 20/183], total loss: 0.00067, regularization loss: 0.29020, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 20:10:48.132543: Epoch [312/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:10:58.336726: Epoch [312/1000] [ 60/183], total loss: 0.00140, regularization loss: 0.29020, contrastive loss: 0.00140, Loss positive: 0.00000, Loss negative: 0.00140
2018-10-22 20:11:08.594407: Epoch [312/1000] [ 80/183], total loss: 0.00853, regularization loss: 0.29020, contrastive loss: 0.00853, Loss positive: 0.00853, Loss negative: 0.00000
2018-10-22 20:11:18.804851: Epoch [312/1000] [100/183], total loss: 0.00527, regularization loss: 0.29020, contrastive loss: 0.00527, Loss positive: 0.00000, Loss negative: 0.00527
2018-10-22 20:11:29.070665: Epoch [312/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:11:39.320609: Epoch [312/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:11:49.564135: Epoch [312/1000] [160/183], total loss: 0.00121, regularization loss: 0.29020, contrastive loss: 0.00121, Loss positive: 0.00000, Loss negative: 0.00121
2018-10-22 20:11:59.916198: Epoch [312/1000] [180/183], total loss: 0.00366, regularization loss: 0.29020, contrastive loss: 0.00366, Loss positive: 0.00000, Loss negative: 0.00366
2018-10-22 20:12:23.020681: Epoch [313/1000] [ 20/183], total loss: 0.00341, regularization loss: 0.29020, contrastive loss: 0.00341, Loss positive: 0.00000, Loss negative: 0.00341
2018-10-22 20:12:33.214822: Epoch [313/1000] [ 40/183], total loss: 0.00031, regularization loss: 0.29020, contrastive loss: 0.00031, Loss positive: 0.00000, Loss negative: 0.00031
2018-10-22 20:12:43.426206: Epoch [313/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:12:53.748503: Epoch [313/1000] [ 80/183], total loss: 0.00538, regularization loss: 0.29020, contrastive loss: 0.00538, Loss positive: 0.00000, Loss negative: 0.00538
2018-10-22 20:13:04.238957: Epoch [313/1000] [100/183], total loss: 0.00014, regularization loss: 0.29020, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 20:13:14.857746: Epoch [313/1000] [120/183], total loss: 0.01488, regularization loss: 0.29020, contrastive loss: 0.01488, Loss positive: 0.01487, Loss negative: 0.00001
2018-10-22 20:13:25.120123: Epoch [313/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:13:35.655220: Epoch [313/1000] [160/183], total loss: 0.01542, regularization loss: 0.29020, contrastive loss: 0.01542, Loss positive: 0.01517, Loss negative: 0.00025
2018-10-22 20:13:46.191191: Epoch [313/1000] [180/183], total loss: 0.00366, regularization loss: 0.29020, contrastive loss: 0.00366, Loss positive: 0.00000, Loss negative: 0.00366
2018-10-22 20:14:10.743220: Epoch [314/1000] [ 20/183], total loss: 0.00010, regularization loss: 0.29020, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 20:14:21.009583: Epoch [314/1000] [ 40/183], total loss: 0.00059, regularization loss: 0.29020, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 20:14:31.250236: Epoch [314/1000] [ 60/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 20:14:41.478768: Epoch [314/1000] [ 80/183], total loss: 0.00033, regularization loss: 0.29020, contrastive loss: 0.00033, Loss positive: 0.00000, Loss negative: 0.00033
2018-10-22 20:14:51.743246: Epoch [314/1000] [100/183], total loss: 0.00079, regularization loss: 0.29020, contrastive loss: 0.00079, Loss positive: 0.00000, Loss negative: 0.00079
2018-10-22 20:15:02.088283: Epoch [314/1000] [120/183], total loss: 0.01344, regularization loss: 0.29020, contrastive loss: 0.01344, Loss positive: 0.00508, Loss negative: 0.00836
2018-10-22 20:15:12.436849: Epoch [314/1000] [140/183], total loss: 0.01626, regularization loss: 0.29020, contrastive loss: 0.01626, Loss positive: 0.01336, Loss negative: 0.00290
2018-10-22 20:15:22.738309: Epoch [314/1000] [160/183], total loss: 0.00189, regularization loss: 0.29020, contrastive loss: 0.00189, Loss positive: 0.00000, Loss negative: 0.00189
2018-10-22 20:15:33.006861: Epoch [314/1000] [180/183], total loss: 0.00357, regularization loss: 0.29020, contrastive loss: 0.00357, Loss positive: 0.00000, Loss negative: 0.00357
2018-10-22 20:15:55.637930: Epoch [315/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:16:05.846225: Epoch [315/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:16:16.072932: Epoch [315/1000] [ 60/183], total loss: 0.01876, regularization loss: 0.29020, contrastive loss: 0.01876, Loss positive: 0.01779, Loss negative: 0.00097
2018-10-22 20:16:26.307371: Epoch [315/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:16:36.580644: Epoch [315/1000] [100/183], total loss: 0.00488, regularization loss: 0.29020, contrastive loss: 0.00488, Loss positive: 0.00000, Loss negative: 0.00488
2018-10-22 20:16:46.812474: Epoch [315/1000] [120/183], total loss: 0.00038, regularization loss: 0.29020, contrastive loss: 0.00038, Loss positive: 0.00000, Loss negative: 0.00038
2018-10-22 20:16:57.047449: Epoch [315/1000] [140/183], total loss: 0.00197, regularization loss: 0.29020, contrastive loss: 0.00197, Loss positive: 0.00000, Loss negative: 0.00197
2018-10-22 20:17:07.287443: Epoch [315/1000] [160/183], total loss: 0.00611, regularization loss: 0.29020, contrastive loss: 0.00611, Loss positive: 0.00000, Loss negative: 0.00611
2018-10-22 20:17:17.514217: Epoch [315/1000] [180/183], total loss: 0.01931, regularization loss: 0.29020, contrastive loss: 0.01931, Loss positive: 0.01931, Loss negative: 0.00000
2018-10-22 20:17:39.149383: Epoch [316/1000] [ 20/183], total loss: 0.00006, regularization loss: 0.29020, contrastive loss: 0.00006, Loss positive: 0.00000, Loss negative: 0.00006
2018-10-22 20:17:49.377644: Epoch [316/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:17:59.558517: Epoch [316/1000] [ 60/183], total loss: 0.00154, regularization loss: 0.29020, contrastive loss: 0.00154, Loss positive: 0.00000, Loss negative: 0.00154
2018-10-22 20:18:09.765907: Epoch [316/1000] [ 80/183], total loss: 0.04037, regularization loss: 0.29020, contrastive loss: 0.04037, Loss positive: 0.04037, Loss negative: 0.00000
2018-10-22 20:18:19.989674: Epoch [316/1000] [100/183], total loss: 0.00237, regularization loss: 0.29020, contrastive loss: 0.00237, Loss positive: 0.00000, Loss negative: 0.00237
2018-10-22 20:18:30.276043: Epoch [316/1000] [120/183], total loss: 0.00749, regularization loss: 0.29020, contrastive loss: 0.00749, Loss positive: 0.00000, Loss negative: 0.00749
2018-10-22 20:18:40.474711: Epoch [316/1000] [140/183], total loss: 0.00119, regularization loss: 0.29020, contrastive loss: 0.00119, Loss positive: 0.00000, Loss negative: 0.00119
2018-10-22 20:18:50.712608: Epoch [316/1000] [160/183], total loss: 0.00218, regularization loss: 0.29020, contrastive loss: 0.00218, Loss positive: 0.00000, Loss negative: 0.00218
2018-10-22 20:19:00.969963: Epoch [316/1000] [180/183], total loss: 0.00417, regularization loss: 0.29020, contrastive loss: 0.00417, Loss positive: 0.00000, Loss negative: 0.00417
2018-10-22 20:19:22.604770: Epoch [317/1000] [ 20/183], total loss: 0.01252, regularization loss: 0.29020, contrastive loss: 0.01252, Loss positive: 0.01238, Loss negative: 0.00015
2018-10-22 20:19:32.782416: Epoch [317/1000] [ 40/183], total loss: 0.03946, regularization loss: 0.29020, contrastive loss: 0.03946, Loss positive: 0.03946, Loss negative: 0.00000
2018-10-22 20:19:42.984924: Epoch [317/1000] [ 60/183], total loss: 0.00051, regularization loss: 0.29020, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 20:19:53.199490: Epoch [317/1000] [ 80/183], total loss: 0.01871, regularization loss: 0.29020, contrastive loss: 0.01871, Loss positive: 0.01680, Loss negative: 0.00191
2018-10-22 20:20:03.412581: Epoch [317/1000] [100/183], total loss: 0.02128, regularization loss: 0.29020, contrastive loss: 0.02128, Loss positive: 0.02128, Loss negative: 0.00000
2018-10-22 20:20:13.658246: Epoch [317/1000] [120/183], total loss: 0.00134, regularization loss: 0.29020, contrastive loss: 0.00134, Loss positive: 0.00000, Loss negative: 0.00134
2018-10-22 20:20:23.895892: Epoch [317/1000] [140/183], total loss: 0.00612, regularization loss: 0.29020, contrastive loss: 0.00612, Loss positive: 0.00000, Loss negative: 0.00612
2018-10-22 20:20:34.128342: Epoch [317/1000] [160/183], total loss: 0.00052, regularization loss: 0.29020, contrastive loss: 0.00052, Loss positive: 0.00000, Loss negative: 0.00052
2018-10-22 20:20:44.390506: Epoch [317/1000] [180/183], total loss: 0.00056, regularization loss: 0.29020, contrastive loss: 0.00056, Loss positive: 0.00000, Loss negative: 0.00056
2018-10-22 20:21:05.908882: Epoch [318/1000] [ 20/183], total loss: 0.01188, regularization loss: 0.29020, contrastive loss: 0.01188, Loss positive: 0.01106, Loss negative: 0.00082
2018-10-22 20:21:16.122395: Epoch [318/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:21:26.307420: Epoch [318/1000] [ 60/183], total loss: 0.00391, regularization loss: 0.29020, contrastive loss: 0.00391, Loss positive: 0.00000, Loss negative: 0.00391
2018-10-22 20:21:36.542180: Epoch [318/1000] [ 80/183], total loss: 0.00181, regularization loss: 0.29020, contrastive loss: 0.00181, Loss positive: 0.00000, Loss negative: 0.00181
2018-10-22 20:21:46.955350: Epoch [318/1000] [100/183], total loss: 0.00137, regularization loss: 0.29020, contrastive loss: 0.00137, Loss positive: 0.00000, Loss negative: 0.00137
2018-10-22 20:21:57.222529: Epoch [318/1000] [120/183], total loss: 0.01127, regularization loss: 0.29020, contrastive loss: 0.01127, Loss positive: 0.01127, Loss negative: 0.00000
2018-10-22 20:22:07.479336: Epoch [318/1000] [140/183], total loss: 0.00084, regularization loss: 0.29020, contrastive loss: 0.00084, Loss positive: 0.00000, Loss negative: 0.00084
2018-10-22 20:22:17.868491: Epoch [318/1000] [160/183], total loss: 0.02175, regularization loss: 0.29020, contrastive loss: 0.02175, Loss positive: 0.02022, Loss negative: 0.00153
2018-10-22 20:22:28.127703: Epoch [318/1000] [180/183], total loss: 0.01569, regularization loss: 0.29020, contrastive loss: 0.01569, Loss positive: 0.01253, Loss negative: 0.00316
2018-10-22 20:22:50.900646: Epoch [319/1000] [ 20/183], total loss: 0.00110, regularization loss: 0.29020, contrastive loss: 0.00110, Loss positive: 0.00000, Loss negative: 0.00110
2018-10-22 20:23:01.116854: Epoch [319/1000] [ 40/183], total loss: 0.00046, regularization loss: 0.29020, contrastive loss: 0.00046, Loss positive: 0.00000, Loss negative: 0.00046
2018-10-22 20:23:11.384463: Epoch [319/1000] [ 60/183], total loss: 0.00065, regularization loss: 0.29020, contrastive loss: 0.00065, Loss positive: 0.00000, Loss negative: 0.00065
2018-10-22 20:23:21.795051: Epoch [319/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:23:32.259347: Epoch [319/1000] [100/183], total loss: 0.00043, regularization loss: 0.29020, contrastive loss: 0.00043, Loss positive: 0.00000, Loss negative: 0.00043
2018-10-22 20:23:42.575034: Epoch [319/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:23:52.947963: Epoch [319/1000] [140/183], total loss: 0.00032, regularization loss: 0.29020, contrastive loss: 0.00032, Loss positive: 0.00000, Loss negative: 0.00032
2018-10-22 20:24:03.277487: Epoch [319/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:24:13.601129: Epoch [319/1000] [180/183], total loss: 0.00350, regularization loss: 0.29020, contrastive loss: 0.00350, Loss positive: 0.00000, Loss negative: 0.00350
2018-10-22 20:24:38.019015: Epoch [320/1000] [ 20/183], total loss: 0.01867, regularization loss: 0.29020, contrastive loss: 0.01867, Loss positive: 0.01867, Loss negative: 0.00000
2018-10-22 20:24:48.244594: Epoch [320/1000] [ 40/183], total loss: 0.00060, regularization loss: 0.29020, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 20:24:58.521418: Epoch [320/1000] [ 60/183], total loss: 0.00020, regularization loss: 0.29020, contrastive loss: 0.00020, Loss positive: 0.00000, Loss negative: 0.00020
2018-10-22 20:25:08.806436: Epoch [320/1000] [ 80/183], total loss: 0.00013, regularization loss: 0.29020, contrastive loss: 0.00013, Loss positive: 0.00000, Loss negative: 0.00013
2018-10-22 20:25:19.233088: Epoch [320/1000] [100/183], total loss: 0.02633, regularization loss: 0.29020, contrastive loss: 0.02633, Loss positive: 0.02595, Loss negative: 0.00038
2018-10-22 20:25:29.703006: Epoch [320/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:25:40.020425: Epoch [320/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:25:50.268220: Epoch [320/1000] [160/183], total loss: 0.01641, regularization loss: 0.29020, contrastive loss: 0.01641, Loss positive: 0.01083, Loss negative: 0.00559
2018-10-22 20:26:00.589676: Epoch [320/1000] [180/183], total loss: 0.00028, regularization loss: 0.29020, contrastive loss: 0.00028, Loss positive: 0.00000, Loss negative: 0.00028
Recall@1: 0.27465
Recall@2: 0.39298
Recall@4: 0.52515
Recall@8: 0.65901
Recall@16: 0.77583
Recall@32: 0.86901
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 73.  55.  77.  45.  49.  93.  38.  49.  29.  59.  80.  42.  68.  91.
 112.  40.  39.  38.  54.  73.  42.  42.  41.  83.  88.  54.  65.  31.
  10.  66.  75.  86.  59.  49.  64.  83.  44.  51.  69.  55.  86.  44.
  37.  74.  48.  86.  79.  92.  91.  47.  28.  41.  31.  60.  71.  87.
  48.  36.  98.  32.  43.  76.  84.  38.  57.  90.  49.  65.  47.  46.
  46.  61.  33.  37.  42.  51.  53.  61.  30.  55.  81.  73.  52.  47.
  62.  43.  46.  59.  66.  52.  88.  46.  97.  67.  69.  71.  78.  73.
  71.  42.]
Purity is 0.247
count_cross = [[ 0.  0.  1. ...  0.  0.  1.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  0.  0. ...  0.  1.  1.]
 ...
 [ 0.  3.  2. ... 10.  1.  0.]
 [ 0.  0.  0. ...  0.  3.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]]
Mutual information is 2.10050
5924.0
5924
Entropy cluster is 4.54856
Entropy class is 4.60444
normalized_mutual_information is 0.45898
tp_and_fp = 192021.0
tp = 21162.0
fp is 170859.0
fn is 151588.0
RI is 0.9816205905109266
Precision is 0.1102066961426094
Recall is 0.12250072358900145
F_1 is 0.11602896063557684

normalized_mutual_information = 0.4589759049669906
RI = 0.9816205905109266
F_1 = 0.11602896063557684

The NN is 0.27465
The FT is 0.15051
The ST is 0.23296
The DCG is 0.53569
The E is 0.12701
The MAP 0.12463

2018-10-22 20:27:31.321458: Epoch [321/1000] [ 20/183], total loss: 0.00034, regularization loss: 0.29020, contrastive loss: 0.00034, Loss positive: 0.00000, Loss negative: 0.00034
2018-10-22 20:27:41.394077: Epoch [321/1000] [ 40/183], total loss: 0.00318, regularization loss: 0.29020, contrastive loss: 0.00318, Loss positive: 0.00000, Loss negative: 0.00318
2018-10-22 20:27:51.495495: Epoch [321/1000] [ 60/183], total loss: 0.00336, regularization loss: 0.29020, contrastive loss: 0.00336, Loss positive: 0.00000, Loss negative: 0.00336
2018-10-22 20:28:01.662300: Epoch [321/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:28:11.848571: Epoch [321/1000] [100/183], total loss: 0.01534, regularization loss: 0.29020, contrastive loss: 0.01534, Loss positive: 0.01534, Loss negative: 0.00000
2018-10-22 20:28:22.023827: Epoch [321/1000] [120/183], total loss: 0.04441, regularization loss: 0.29020, contrastive loss: 0.04441, Loss positive: 0.04226, Loss negative: 0.00215
2018-10-22 20:28:32.244229: Epoch [321/1000] [140/183], total loss: 0.00244, regularization loss: 0.29020, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 20:28:42.515003: Epoch [321/1000] [160/183], total loss: 0.00145, regularization loss: 0.29020, contrastive loss: 0.00145, Loss positive: 0.00000, Loss negative: 0.00145
2018-10-22 20:28:52.759149: Epoch [321/1000] [180/183], total loss: 0.00616, regularization loss: 0.29020, contrastive loss: 0.00616, Loss positive: 0.00000, Loss negative: 0.00616
2018-10-22 20:29:14.410864: Epoch [322/1000] [ 20/183], total loss: 0.01252, regularization loss: 0.29020, contrastive loss: 0.01252, Loss positive: 0.01243, Loss negative: 0.00009
2018-10-22 20:29:24.595343: Epoch [322/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:29:34.747223: Epoch [322/1000] [ 60/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 20:29:44.972038: Epoch [322/1000] [ 80/183], total loss: 0.00185, regularization loss: 0.29020, contrastive loss: 0.00185, Loss positive: 0.00000, Loss negative: 0.00185
2018-10-22 20:29:55.211449: Epoch [322/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:30:05.645237: Epoch [322/1000] [120/183], total loss: 0.00878, regularization loss: 0.29020, contrastive loss: 0.00878, Loss positive: 0.00000, Loss negative: 0.00878
2018-10-22 20:30:15.921459: Epoch [322/1000] [140/183], total loss: 0.01632, regularization loss: 0.29020, contrastive loss: 0.01632, Loss positive: 0.01623, Loss negative: 0.00009
2018-10-22 20:30:26.179361: Epoch [322/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:30:36.402264: Epoch [322/1000] [180/183], total loss: 0.00035, regularization loss: 0.29020, contrastive loss: 0.00035, Loss positive: 0.00000, Loss negative: 0.00035
2018-10-22 20:30:57.891585: Epoch [323/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:31:08.095645: Epoch [323/1000] [ 40/183], total loss: 0.04253, regularization loss: 0.29020, contrastive loss: 0.04253, Loss positive: 0.04194, Loss negative: 0.00059
2018-10-22 20:31:18.294798: Epoch [323/1000] [ 60/183], total loss: 0.03581, regularization loss: 0.29020, contrastive loss: 0.03581, Loss positive: 0.03581, Loss negative: 0.00000
2018-10-22 20:31:28.491841: Epoch [323/1000] [ 80/183], total loss: 0.01651, regularization loss: 0.29020, contrastive loss: 0.01651, Loss positive: 0.01618, Loss negative: 0.00033
2018-10-22 20:31:38.742539: Epoch [323/1000] [100/183], total loss: 0.00030, regularization loss: 0.29020, contrastive loss: 0.00030, Loss positive: 0.00000, Loss negative: 0.00030
2018-10-22 20:31:48.987654: Epoch [323/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:31:59.229850: Epoch [323/1000] [140/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 20:32:09.604058: Epoch [323/1000] [160/183], total loss: 0.00019, regularization loss: 0.29020, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 20:32:20.231980: Epoch [323/1000] [180/183], total loss: 0.00473, regularization loss: 0.29020, contrastive loss: 0.00473, Loss positive: 0.00000, Loss negative: 0.00473
2018-10-22 20:32:42.909650: Epoch [324/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:32:53.167876: Epoch [324/1000] [ 40/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 20:33:03.440401: Epoch [324/1000] [ 60/183], total loss: 0.00060, regularization loss: 0.29020, contrastive loss: 0.00060, Loss positive: 0.00000, Loss negative: 0.00060
2018-10-22 20:33:13.689734: Epoch [324/1000] [ 80/183], total loss: 0.02057, regularization loss: 0.29020, contrastive loss: 0.02057, Loss positive: 0.02057, Loss negative: 0.00000
2018-10-22 20:33:24.026244: Epoch [324/1000] [100/183], total loss: 0.01842, regularization loss: 0.29020, contrastive loss: 0.01842, Loss positive: 0.01842, Loss negative: 0.00000
2018-10-22 20:33:34.458485: Epoch [324/1000] [120/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 20:33:44.848554: Epoch [324/1000] [140/183], total loss: 0.00496, regularization loss: 0.29020, contrastive loss: 0.00496, Loss positive: 0.00000, Loss negative: 0.00496
2018-10-22 20:33:55.201923: Epoch [324/1000] [160/183], total loss: 0.01681, regularization loss: 0.29020, contrastive loss: 0.01681, Loss positive: 0.01651, Loss negative: 0.00031
2018-10-22 20:34:05.874345: Epoch [324/1000] [180/183], total loss: 0.00618, regularization loss: 0.29020, contrastive loss: 0.00618, Loss positive: 0.00589, Loss negative: 0.00029
2018-10-22 20:34:30.719468: Epoch [325/1000] [ 20/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:34:40.954210: Epoch [325/1000] [ 40/183], total loss: 0.00076, regularization loss: 0.29020, contrastive loss: 0.00076, Loss positive: 0.00000, Loss negative: 0.00076
2018-10-22 20:34:51.195815: Epoch [325/1000] [ 60/183], total loss: 0.00318, regularization loss: 0.29020, contrastive loss: 0.00318, Loss positive: 0.00000, Loss negative: 0.00318
2018-10-22 20:35:01.492068: Epoch [325/1000] [ 80/183], total loss: 0.00646, regularization loss: 0.29020, contrastive loss: 0.00646, Loss positive: 0.00627, Loss negative: 0.00019
2018-10-22 20:35:12.100873: Epoch [325/1000] [100/183], total loss: 0.00097, regularization loss: 0.29020, contrastive loss: 0.00097, Loss positive: 0.00000, Loss negative: 0.00097
2018-10-22 20:35:22.518359: Epoch [325/1000] [120/183], total loss: 0.00348, regularization loss: 0.29020, contrastive loss: 0.00348, Loss positive: 0.00000, Loss negative: 0.00348
2018-10-22 20:35:32.760905: Epoch [325/1000] [140/183], total loss: 0.00067, regularization loss: 0.29020, contrastive loss: 0.00067, Loss positive: 0.00000, Loss negative: 0.00067
2018-10-22 20:35:43.176240: Epoch [325/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:35:53.432302: Epoch [325/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:36:16.043929: Epoch [326/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:36:26.226302: Epoch [326/1000] [ 40/183], total loss: 0.01183, regularization loss: 0.29020, contrastive loss: 0.01183, Loss positive: 0.00794, Loss negative: 0.00388
2018-10-22 20:36:36.428155: Epoch [326/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:36:46.653460: Epoch [326/1000] [ 80/183], total loss: 0.05360, regularization loss: 0.29020, contrastive loss: 0.05360, Loss positive: 0.05352, Loss negative: 0.00008
2018-10-22 20:36:57.007376: Epoch [326/1000] [100/183], total loss: 0.00562, regularization loss: 0.29020, contrastive loss: 0.00562, Loss positive: 0.00000, Loss negative: 0.00562
2018-10-22 20:37:07.247563: Epoch [326/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:37:17.583938: Epoch [326/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:37:27.824909: Epoch [326/1000] [160/183], total loss: 0.00800, regularization loss: 0.29020, contrastive loss: 0.00800, Loss positive: 0.00000, Loss negative: 0.00800
2018-10-22 20:37:38.058785: Epoch [326/1000] [180/183], total loss: 0.00455, regularization loss: 0.29020, contrastive loss: 0.00455, Loss positive: 0.00000, Loss negative: 0.00455
2018-10-22 20:37:59.693663: Epoch [327/1000] [ 20/183], total loss: 0.02488, regularization loss: 0.29020, contrastive loss: 0.02488, Loss positive: 0.02474, Loss negative: 0.00015
2018-10-22 20:38:09.890184: Epoch [327/1000] [ 40/183], total loss: 0.00461, regularization loss: 0.29020, contrastive loss: 0.00461, Loss positive: 0.00455, Loss negative: 0.00006
2018-10-22 20:38:20.080560: Epoch [327/1000] [ 60/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 20:38:30.304230: Epoch [327/1000] [ 80/183], total loss: 0.00089, regularization loss: 0.29020, contrastive loss: 0.00089, Loss positive: 0.00000, Loss negative: 0.00089
2018-10-22 20:38:40.583097: Epoch [327/1000] [100/183], total loss: 0.00092, regularization loss: 0.29020, contrastive loss: 0.00092, Loss positive: 0.00000, Loss negative: 0.00092
2018-10-22 20:38:50.824876: Epoch [327/1000] [120/183], total loss: 0.00868, regularization loss: 0.29020, contrastive loss: 0.00868, Loss positive: 0.00868, Loss negative: 0.00000
2018-10-22 20:39:01.259289: Epoch [327/1000] [140/183], total loss: 0.00331, regularization loss: 0.29020, contrastive loss: 0.00331, Loss positive: 0.00000, Loss negative: 0.00331
2018-10-22 20:39:11.481141: Epoch [327/1000] [160/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 20:39:21.729600: Epoch [327/1000] [180/183], total loss: 0.02861, regularization loss: 0.29020, contrastive loss: 0.02861, Loss positive: 0.02857, Loss negative: 0.00004
2018-10-22 20:39:43.309961: Epoch [328/1000] [ 20/183], total loss: 0.00988, regularization loss: 0.29020, contrastive loss: 0.00988, Loss positive: 0.00950, Loss negative: 0.00038
2018-10-22 20:39:53.500540: Epoch [328/1000] [ 40/183], total loss: 0.00377, regularization loss: 0.29020, contrastive loss: 0.00377, Loss positive: 0.00000, Loss negative: 0.00377
2018-10-22 20:40:03.717593: Epoch [328/1000] [ 60/183], total loss: 0.00126, regularization loss: 0.29020, contrastive loss: 0.00126, Loss positive: 0.00000, Loss negative: 0.00126
2018-10-22 20:40:13.951143: Epoch [328/1000] [ 80/183], total loss: 0.00048, regularization loss: 0.29020, contrastive loss: 0.00048, Loss positive: 0.00000, Loss negative: 0.00048
2018-10-22 20:40:24.165922: Epoch [328/1000] [100/183], total loss: 0.01078, regularization loss: 0.29020, contrastive loss: 0.01078, Loss positive: 0.00000, Loss negative: 0.01078
2018-10-22 20:40:34.402065: Epoch [328/1000] [120/183], total loss: 0.00099, regularization loss: 0.29020, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 20:40:44.650145: Epoch [328/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:40:54.881419: Epoch [328/1000] [160/183], total loss: 0.00201, regularization loss: 0.29020, contrastive loss: 0.00201, Loss positive: 0.00000, Loss negative: 0.00201
2018-10-22 20:41:05.143052: Epoch [328/1000] [180/183], total loss: 0.00083, regularization loss: 0.29020, contrastive loss: 0.00083, Loss positive: 0.00000, Loss negative: 0.00083
2018-10-22 20:41:26.770386: Epoch [329/1000] [ 20/183], total loss: 0.00072, regularization loss: 0.29020, contrastive loss: 0.00072, Loss positive: 0.00000, Loss negative: 0.00072
2018-10-22 20:41:37.002037: Epoch [329/1000] [ 40/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:41:47.191241: Epoch [329/1000] [ 60/183], total loss: 0.00086, regularization loss: 0.29020, contrastive loss: 0.00086, Loss positive: 0.00000, Loss negative: 0.00086
2018-10-22 20:41:57.450443: Epoch [329/1000] [ 80/183], total loss: 0.00059, regularization loss: 0.29020, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 20:42:07.690859: Epoch [329/1000] [100/183], total loss: 0.01519, regularization loss: 0.29020, contrastive loss: 0.01519, Loss positive: 0.01519, Loss negative: 0.00000
2018-10-22 20:42:17.943677: Epoch [329/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:42:28.244511: Epoch [329/1000] [140/183], total loss: 0.00937, regularization loss: 0.29020, contrastive loss: 0.00937, Loss positive: 0.00884, Loss negative: 0.00053
2018-10-22 20:42:38.763150: Epoch [329/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:42:49.106419: Epoch [329/1000] [180/183], total loss: 0.00950, regularization loss: 0.29020, contrastive loss: 0.00950, Loss positive: 0.00802, Loss negative: 0.00148
2018-10-22 20:43:13.701660: Epoch [330/1000] [ 20/183], total loss: 0.00224, regularization loss: 0.29020, contrastive loss: 0.00224, Loss positive: 0.00000, Loss negative: 0.00224
2018-10-22 20:43:23.966624: Epoch [330/1000] [ 40/183], total loss: 0.00184, regularization loss: 0.29020, contrastive loss: 0.00184, Loss positive: 0.00000, Loss negative: 0.00184
2018-10-22 20:43:34.215850: Epoch [330/1000] [ 60/183], total loss: 0.00670, regularization loss: 0.29020, contrastive loss: 0.00670, Loss positive: 0.00633, Loss negative: 0.00038
2018-10-22 20:43:44.453872: Epoch [330/1000] [ 80/183], total loss: 0.00227, regularization loss: 0.29020, contrastive loss: 0.00227, Loss positive: 0.00000, Loss negative: 0.00227
2018-10-22 20:43:54.750594: Epoch [330/1000] [100/183], total loss: 0.00762, regularization loss: 0.29020, contrastive loss: 0.00762, Loss positive: 0.00000, Loss negative: 0.00762
2018-10-22 20:44:05.191791: Epoch [330/1000] [120/183], total loss: 0.00015, regularization loss: 0.29020, contrastive loss: 0.00015, Loss positive: 0.00000, Loss negative: 0.00015
2018-10-22 20:44:15.526724: Epoch [330/1000] [140/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 20:44:25.870552: Epoch [330/1000] [160/183], total loss: 0.02362, regularization loss: 0.29020, contrastive loss: 0.02362, Loss positive: 0.00000, Loss negative: 0.02362
2018-10-22 20:44:36.471403: Epoch [330/1000] [180/183], total loss: 0.00478, regularization loss: 0.29020, contrastive loss: 0.00478, Loss positive: 0.00000, Loss negative: 0.00478
Recall@1: 0.26519
Recall@2: 0.38352
Recall@4: 0.52887
Recall@8: 0.65935
Recall@16: 0.77633
Recall@32: 0.87086
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 70.  23.  54.  62.  58.  64.  72.  42.  31.  40.  75.  38. 115.  50.
  93.  43.  70.  35.  73.  86.  49.  76.  77.  24.  50.  40.  59.  35.
  43.  87. 120.  97.  44.  23.  75.  69.  89.  61.  76.  64.  54.  83.
  57.  56.  45.  57.  87.  65. 105.  16.  75.  55.  48.  36.  53.  55.
  80.  35.  54.  64.  68.  45.  54.  55.  62.  69.  55.  42.  79.  87.
  33.  61.  56.  41.  68.  64.  80.  71.  21.  39. 107. 138.  38.  37.
  82.  48.  95.  26.  40.  34.  77.  55.  64.  49.  54.  51.  36.  29.
  23.  59.]
Purity is 0.255
count_cross = [[0. 0. 0. ... 1. 5. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [1. 1. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 3. 2. ... 5. 1. 0.]]
Mutual information is 2.11671
5924.0
5924
Entropy cluster is 4.53189
Entropy class is 4.60444
normalized_mutual_information is 0.46336
tp_and_fp = 198583.0
tp = 22339.0
fp is 176244.0
fn is 150411.0
RI is 0.9813807354180587
Precision is 0.11249200586152894
Recall is 0.12931403762662808
F_1 is 0.1203178817934307

normalized_mutual_information = 0.46336026841074573
RI = 0.9813807354180587
F_1 = 0.1203178817934307

The NN is 0.26519
The FT is 0.15219
The ST is 0.23583
The DCG is 0.53696
The E is 0.12770
The MAP 0.12669

2018-10-22 20:46:14.307454: Epoch [331/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:46:24.414723: Epoch [331/1000] [ 40/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:46:34.475588: Epoch [331/1000] [ 60/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:46:44.583073: Epoch [331/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:46:54.746532: Epoch [331/1000] [100/183], total loss: 0.02423, regularization loss: 0.29020, contrastive loss: 0.02423, Loss positive: 0.02290, Loss negative: 0.00134
2018-10-22 20:47:04.961853: Epoch [331/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:47:15.281525: Epoch [331/1000] [140/183], total loss: 0.00280, regularization loss: 0.29020, contrastive loss: 0.00280, Loss positive: 0.00000, Loss negative: 0.00280
2018-10-22 20:47:25.564156: Epoch [331/1000] [160/183], total loss: 0.01624, regularization loss: 0.29020, contrastive loss: 0.01624, Loss positive: 0.01599, Loss negative: 0.00025
2018-10-22 20:47:35.802898: Epoch [331/1000] [180/183], total loss: 0.00212, regularization loss: 0.29020, contrastive loss: 0.00212, Loss positive: 0.00000, Loss negative: 0.00212
2018-10-22 20:47:57.169408: Epoch [332/1000] [ 20/183], total loss: 0.01030, regularization loss: 0.29020, contrastive loss: 0.01030, Loss positive: 0.00726, Loss negative: 0.00303
2018-10-22 20:48:07.312343: Epoch [332/1000] [ 40/183], total loss: 0.02481, regularization loss: 0.29020, contrastive loss: 0.02481, Loss positive: 0.02399, Loss negative: 0.00082
2018-10-22 20:48:17.472457: Epoch [332/1000] [ 60/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:48:27.818289: Epoch [332/1000] [ 80/183], total loss: 0.02212, regularization loss: 0.29020, contrastive loss: 0.02212, Loss positive: 0.01889, Loss negative: 0.00323
2018-10-22 20:48:38.187914: Epoch [332/1000] [100/183], total loss: 0.00093, regularization loss: 0.29020, contrastive loss: 0.00093, Loss positive: 0.00000, Loss negative: 0.00093
2018-10-22 20:48:48.415831: Epoch [332/1000] [120/183], total loss: 0.00345, regularization loss: 0.29020, contrastive loss: 0.00345, Loss positive: 0.00345, Loss negative: 0.00000
2018-10-22 20:48:58.666296: Epoch [332/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:49:08.922292: Epoch [332/1000] [160/183], total loss: 0.00715, regularization loss: 0.29020, contrastive loss: 0.00715, Loss positive: 0.00000, Loss negative: 0.00715
2018-10-22 20:49:19.159248: Epoch [332/1000] [180/183], total loss: 0.01682, regularization loss: 0.29020, contrastive loss: 0.01682, Loss positive: 0.01682, Loss negative: 0.00000
2018-10-22 20:49:40.806435: Epoch [333/1000] [ 20/183], total loss: 0.01822, regularization loss: 0.29020, contrastive loss: 0.01822, Loss positive: 0.01557, Loss negative: 0.00265
2018-10-22 20:49:50.972336: Epoch [333/1000] [ 40/183], total loss: 0.00441, regularization loss: 0.29020, contrastive loss: 0.00441, Loss positive: 0.00000, Loss negative: 0.00441
2018-10-22 20:50:01.187016: Epoch [333/1000] [ 60/183], total loss: 0.00368, regularization loss: 0.29020, contrastive loss: 0.00368, Loss positive: 0.00221, Loss negative: 0.00147
2018-10-22 20:50:11.445849: Epoch [333/1000] [ 80/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 20:50:21.681429: Epoch [333/1000] [100/183], total loss: 0.02282, regularization loss: 0.29020, contrastive loss: 0.02282, Loss positive: 0.02137, Loss negative: 0.00145
2018-10-22 20:50:31.943803: Epoch [333/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:50:42.189496: Epoch [333/1000] [140/183], total loss: 0.00111, regularization loss: 0.29020, contrastive loss: 0.00111, Loss positive: 0.00000, Loss negative: 0.00111
2018-10-22 20:50:52.414203: Epoch [333/1000] [160/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 20:51:02.682572: Epoch [333/1000] [180/183], total loss: 0.00999, regularization loss: 0.29020, contrastive loss: 0.00999, Loss positive: 0.00000, Loss negative: 0.00999
2018-10-22 20:51:24.231757: Epoch [334/1000] [ 20/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 20:51:34.414075: Epoch [334/1000] [ 40/183], total loss: 0.02838, regularization loss: 0.29020, contrastive loss: 0.02838, Loss positive: 0.02445, Loss negative: 0.00394
2018-10-22 20:51:44.626300: Epoch [334/1000] [ 60/183], total loss: 0.00047, regularization loss: 0.29020, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 20:51:55.167519: Epoch [334/1000] [ 80/183], total loss: 0.02294, regularization loss: 0.29020, contrastive loss: 0.02294, Loss positive: 0.01927, Loss negative: 0.00367
2018-10-22 20:52:05.532726: Epoch [334/1000] [100/183], total loss: 0.00358, regularization loss: 0.29020, contrastive loss: 0.00358, Loss positive: 0.00000, Loss negative: 0.00358
2018-10-22 20:52:15.779524: Epoch [334/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:52:26.054488: Epoch [334/1000] [140/183], total loss: 0.00041, regularization loss: 0.29020, contrastive loss: 0.00041, Loss positive: 0.00000, Loss negative: 0.00041
2018-10-22 20:52:36.301513: Epoch [334/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:52:46.746682: Epoch [334/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:53:11.275278: Epoch [335/1000] [ 20/183], total loss: 0.00007, regularization loss: 0.29020, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 20:53:21.498762: Epoch [335/1000] [ 40/183], total loss: 0.01965, regularization loss: 0.29020, contrastive loss: 0.01965, Loss positive: 0.01637, Loss negative: 0.00328
2018-10-22 20:53:31.737819: Epoch [335/1000] [ 60/183], total loss: 0.00050, regularization loss: 0.29020, contrastive loss: 0.00050, Loss positive: 0.00000, Loss negative: 0.00050
2018-10-22 20:53:41.975886: Epoch [335/1000] [ 80/183], total loss: 0.00003, regularization loss: 0.29020, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 20:53:52.259189: Epoch [335/1000] [100/183], total loss: 0.00575, regularization loss: 0.29020, contrastive loss: 0.00575, Loss positive: 0.00000, Loss negative: 0.00575
2018-10-22 20:54:02.525739: Epoch [335/1000] [120/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 20:54:13.026344: Epoch [335/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:54:23.310320: Epoch [335/1000] [160/183], total loss: 0.00522, regularization loss: 0.29020, contrastive loss: 0.00522, Loss positive: 0.00000, Loss negative: 0.00522
2018-10-22 20:54:33.726305: Epoch [335/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:54:58.418325: Epoch [336/1000] [ 20/183], total loss: 0.00099, regularization loss: 0.29020, contrastive loss: 0.00099, Loss positive: 0.00000, Loss negative: 0.00099
2018-10-22 20:55:08.674939: Epoch [336/1000] [ 40/183], total loss: 0.00010, regularization loss: 0.29020, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 20:55:18.931189: Epoch [336/1000] [ 60/183], total loss: 0.02594, regularization loss: 0.29020, contrastive loss: 0.02594, Loss positive: 0.02594, Loss negative: 0.00000
2018-10-22 20:55:29.158151: Epoch [336/1000] [ 80/183], total loss: 0.02709, regularization loss: 0.29020, contrastive loss: 0.02709, Loss positive: 0.02602, Loss negative: 0.00107
2018-10-22 20:55:39.372959: Epoch [336/1000] [100/183], total loss: 0.00500, regularization loss: 0.29020, contrastive loss: 0.00500, Loss positive: 0.00000, Loss negative: 0.00500
2018-10-22 20:55:49.694193: Epoch [336/1000] [120/183], total loss: 0.00253, regularization loss: 0.29020, contrastive loss: 0.00253, Loss positive: 0.00000, Loss negative: 0.00253
2018-10-22 20:55:59.959723: Epoch [336/1000] [140/183], total loss: 0.00091, regularization loss: 0.29020, contrastive loss: 0.00091, Loss positive: 0.00000, Loss negative: 0.00091
2018-10-22 20:56:10.192150: Epoch [336/1000] [160/183], total loss: 0.00284, regularization loss: 0.29020, contrastive loss: 0.00284, Loss positive: 0.00000, Loss negative: 0.00284
2018-10-22 20:56:20.426234: Epoch [336/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:56:42.626233: Epoch [337/1000] [ 20/183], total loss: 0.00197, regularization loss: 0.29020, contrastive loss: 0.00197, Loss positive: 0.00000, Loss negative: 0.00197
2018-10-22 20:56:52.767613: Epoch [337/1000] [ 40/183], total loss: 0.00359, regularization loss: 0.29020, contrastive loss: 0.00359, Loss positive: 0.00000, Loss negative: 0.00359
2018-10-22 20:57:02.943568: Epoch [337/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:57:13.162175: Epoch [337/1000] [ 80/183], total loss: 0.00584, regularization loss: 0.29020, contrastive loss: 0.00584, Loss positive: 0.00584, Loss negative: 0.00000
2018-10-22 20:57:23.497722: Epoch [337/1000] [100/183], total loss: 0.00021, regularization loss: 0.29020, contrastive loss: 0.00021, Loss positive: 0.00000, Loss negative: 0.00021
2018-10-22 20:57:33.720558: Epoch [337/1000] [120/183], total loss: 0.00276, regularization loss: 0.29020, contrastive loss: 0.00276, Loss positive: 0.00000, Loss negative: 0.00276
2018-10-22 20:57:43.980459: Epoch [337/1000] [140/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:57:54.225537: Epoch [337/1000] [160/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
2018-10-22 20:58:04.450400: Epoch [337/1000] [180/183], total loss: 0.02586, regularization loss: 0.29020, contrastive loss: 0.02586, Loss positive: 0.01893, Loss negative: 0.00693
2018-10-22 20:58:26.067457: Epoch [338/1000] [ 20/183], total loss: 0.00077, regularization loss: 0.29020, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 20:58:36.220928: Epoch [338/1000] [ 40/183], total loss: 0.03384, regularization loss: 0.29020, contrastive loss: 0.03384, Loss positive: 0.02978, Loss negative: 0.00406
2018-10-22 20:58:46.640838: Epoch [338/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:58:56.813856: Epoch [338/1000] [ 80/183], total loss: 0.00923, regularization loss: 0.29020, contrastive loss: 0.00923, Loss positive: 0.00773, Loss negative: 0.00150
2018-10-22 20:59:07.013096: Epoch [338/1000] [100/183], total loss: 0.01948, regularization loss: 0.29020, contrastive loss: 0.01948, Loss positive: 0.01918, Loss negative: 0.00030
2018-10-22 20:59:17.253026: Epoch [338/1000] [120/183], total loss: 0.01315, regularization loss: 0.29020, contrastive loss: 0.01315, Loss positive: 0.01315, Loss negative: 0.00000
2018-10-22 20:59:27.522968: Epoch [338/1000] [140/183], total loss: 0.00260, regularization loss: 0.29020, contrastive loss: 0.00260, Loss positive: 0.00000, Loss negative: 0.00260
2018-10-22 20:59:37.743323: Epoch [338/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 20:59:47.993319: Epoch [338/1000] [180/183], total loss: 0.00806, regularization loss: 0.29020, contrastive loss: 0.00806, Loss positive: 0.00709, Loss negative: 0.00097
2018-10-22 21:00:09.566470: Epoch [339/1000] [ 20/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:00:19.737675: Epoch [339/1000] [ 40/183], total loss: 0.00448, regularization loss: 0.29020, contrastive loss: 0.00448, Loss positive: 0.00000, Loss negative: 0.00448
2018-10-22 21:00:29.971971: Epoch [339/1000] [ 60/183], total loss: 0.00198, regularization loss: 0.29020, contrastive loss: 0.00198, Loss positive: 0.00000, Loss negative: 0.00198
2018-10-22 21:00:40.201949: Epoch [339/1000] [ 80/183], total loss: 0.00077, regularization loss: 0.29020, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 21:00:50.467862: Epoch [339/1000] [100/183], total loss: 0.00438, regularization loss: 0.29020, contrastive loss: 0.00438, Loss positive: 0.00000, Loss negative: 0.00438
2018-10-22 21:01:00.708416: Epoch [339/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:01:10.948765: Epoch [339/1000] [140/183], total loss: 0.00113, regularization loss: 0.29020, contrastive loss: 0.00113, Loss positive: 0.00000, Loss negative: 0.00113
2018-10-22 21:01:21.184051: Epoch [339/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:01:31.445099: Epoch [339/1000] [180/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 21:01:54.368870: Epoch [340/1000] [ 20/183], total loss: 0.00095, regularization loss: 0.29020, contrastive loss: 0.00095, Loss positive: 0.00000, Loss negative: 0.00095
2018-10-22 21:02:04.533142: Epoch [340/1000] [ 40/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 21:02:14.738418: Epoch [340/1000] [ 60/183], total loss: 0.00044, regularization loss: 0.29020, contrastive loss: 0.00044, Loss positive: 0.00000, Loss negative: 0.00044
2018-10-22 21:02:25.068866: Epoch [340/1000] [ 80/183], total loss: 0.00527, regularization loss: 0.29020, contrastive loss: 0.00527, Loss positive: 0.00000, Loss negative: 0.00527
2018-10-22 21:02:35.306183: Epoch [340/1000] [100/183], total loss: 0.00385, regularization loss: 0.29020, contrastive loss: 0.00385, Loss positive: 0.00000, Loss negative: 0.00385
2018-10-22 21:02:45.578574: Epoch [340/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:02:56.072901: Epoch [340/1000] [140/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 21:03:06.573977: Epoch [340/1000] [160/183], total loss: 0.00583, regularization loss: 0.29020, contrastive loss: 0.00583, Loss positive: 0.00000, Loss negative: 0.00583
2018-10-22 21:03:16.999643: Epoch [340/1000] [180/183], total loss: 0.00005, regularization loss: 0.29020, contrastive loss: 0.00005, Loss positive: 0.00000, Loss negative: 0.00005
Recall@1: 0.26907
Recall@2: 0.38910
Recall@4: 0.51553
Recall@8: 0.65614
Recall@16: 0.77937
Recall@32: 0.88015
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 63.  77.  41.  75.  56.  36.  54.  92.  65.  73.  48.  46.  85.  79.
  75.  35.  39.  91.  58.  50.  44.  56.  69.  87.  73.  50.  48.  10.
  53.  78.  51.  91.  77.  68.  40. 135.  44.  56.  63.  53.  40.  58.
  85.  39.  73.  46.  71.  41.  71.  54.  59.  58.  75.  54.  73.  49.
  20.  64.  47. 116.  74.  42.  54.  32.  17.  87.  50.  39.  47.  70.
  50.  66.  94.  58.  32. 105.  49.  74.  54.  60.  96.  47.  61.  28.
  69.  65.  36.  45.  52.  58.  94.  44. 106.  43.  40.  36.  64.  38.
  33.  38.]
Purity is 0.257
count_cross = [[0. 3. 1. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 ...
 [1. 0. 0. ... 0. 0. 0.]
 [0. 1. 2. ... 2. 0. 0.]
 [0. 0. 0. ... 0. 0. 3.]]
Mutual information is 2.11012
5924.0
5924
Entropy cluster is 4.54075
Entropy class is 4.60444
normalized_mutual_information is 0.46147
tp_and_fp = 195297.0
tp = 21799.0
fp is 173498.0
fn is 150951.0
RI is 0.9815064769425043
Precision is 0.11161973814241898
Recall is 0.12618813314037627
F_1 is 0.11845769697891845

normalized_mutual_information = 0.4614707368442059
RI = 0.9815064769425043
F_1 = 0.11845769697891845

The NN is 0.26907
The FT is 0.14885
The ST is 0.23016
The DCG is 0.53411
The E is 0.12443
The MAP 0.12329

2018-10-22 21:05:02.423820: Epoch [341/1000] [ 20/183], total loss: 0.01774, regularization loss: 0.29020, contrastive loss: 0.01774, Loss positive: 0.01620, Loss negative: 0.00154
2018-10-22 21:05:12.592242: Epoch [341/1000] [ 40/183], total loss: 0.01184, regularization loss: 0.29020, contrastive loss: 0.01184, Loss positive: 0.01182, Loss negative: 0.00002
2018-10-22 21:05:22.720978: Epoch [341/1000] [ 60/183], total loss: 0.01550, regularization loss: 0.29020, contrastive loss: 0.01550, Loss positive: 0.01543, Loss negative: 0.00007
2018-10-22 21:05:32.869955: Epoch [341/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:05:43.079338: Epoch [341/1000] [100/183], total loss: 0.00472, regularization loss: 0.29020, contrastive loss: 0.00472, Loss positive: 0.00000, Loss negative: 0.00472
2018-10-22 21:05:53.252315: Epoch [341/1000] [120/183], total loss: 0.00355, regularization loss: 0.29020, contrastive loss: 0.00355, Loss positive: 0.00000, Loss negative: 0.00355
2018-10-22 21:06:03.629640: Epoch [341/1000] [140/183], total loss: 0.00062, regularization loss: 0.29020, contrastive loss: 0.00062, Loss positive: 0.00000, Loss negative: 0.00062
2018-10-22 21:06:13.852471: Epoch [341/1000] [160/183], total loss: 0.03284, regularization loss: 0.29020, contrastive loss: 0.03284, Loss positive: 0.03284, Loss negative: 0.00000
2018-10-22 21:06:24.095802: Epoch [341/1000] [180/183], total loss: 0.01191, regularization loss: 0.29020, contrastive loss: 0.01191, Loss positive: 0.01096, Loss negative: 0.00095
2018-10-22 21:06:45.636060: Epoch [342/1000] [ 20/183], total loss: 0.00042, regularization loss: 0.29020, contrastive loss: 0.00042, Loss positive: 0.00000, Loss negative: 0.00042
2018-10-22 21:06:55.770703: Epoch [342/1000] [ 40/183], total loss: 0.02098, regularization loss: 0.29020, contrastive loss: 0.02098, Loss positive: 0.01776, Loss negative: 0.00322
2018-10-22 21:07:05.919100: Epoch [342/1000] [ 60/183], total loss: 0.00350, regularization loss: 0.29020, contrastive loss: 0.00350, Loss positive: 0.00000, Loss negative: 0.00350
2018-10-22 21:07:16.132255: Epoch [342/1000] [ 80/183], total loss: 0.00094, regularization loss: 0.29020, contrastive loss: 0.00094, Loss positive: 0.00000, Loss negative: 0.00094
2018-10-22 21:07:26.349788: Epoch [342/1000] [100/183], total loss: 0.01260, regularization loss: 0.29020, contrastive loss: 0.01260, Loss positive: 0.01260, Loss negative: 0.00000
2018-10-22 21:07:36.587525: Epoch [342/1000] [120/183], total loss: 0.00273, regularization loss: 0.29020, contrastive loss: 0.00273, Loss positive: 0.00000, Loss negative: 0.00273
2018-10-22 21:07:46.856871: Epoch [342/1000] [140/183], total loss: 0.00104, regularization loss: 0.29020, contrastive loss: 0.00104, Loss positive: 0.00000, Loss negative: 0.00104
2018-10-22 21:07:57.065481: Epoch [342/1000] [160/183], total loss: 0.03371, regularization loss: 0.29020, contrastive loss: 0.03371, Loss positive: 0.03369, Loss negative: 0.00001
2018-10-22 21:08:07.305395: Epoch [342/1000] [180/183], total loss: 0.00350, regularization loss: 0.29020, contrastive loss: 0.00350, Loss positive: 0.00000, Loss negative: 0.00350
2018-10-22 21:08:28.885578: Epoch [343/1000] [ 20/183], total loss: 0.00762, regularization loss: 0.29020, contrastive loss: 0.00762, Loss positive: 0.00000, Loss negative: 0.00762
2018-10-22 21:08:39.077306: Epoch [343/1000] [ 40/183], total loss: 0.00344, regularization loss: 0.29020, contrastive loss: 0.00344, Loss positive: 0.00000, Loss negative: 0.00344
2018-10-22 21:08:49.269984: Epoch [343/1000] [ 60/183], total loss: 0.00040, regularization loss: 0.29020, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 21:08:59.445560: Epoch [343/1000] [ 80/183], total loss: 0.00002, regularization loss: 0.29020, contrastive loss: 0.00002, Loss positive: 0.00000, Loss negative: 0.00002
2018-10-22 21:09:09.682598: Epoch [343/1000] [100/183], total loss: 0.00073, regularization loss: 0.29020, contrastive loss: 0.00073, Loss positive: 0.00000, Loss negative: 0.00073
2018-10-22 21:09:19.930746: Epoch [343/1000] [120/183], total loss: 0.00159, regularization loss: 0.29020, contrastive loss: 0.00159, Loss positive: 0.00000, Loss negative: 0.00159
2018-10-22 21:09:30.160302: Epoch [343/1000] [140/183], total loss: 0.02858, regularization loss: 0.29020, contrastive loss: 0.02858, Loss positive: 0.02582, Loss negative: 0.00276
2018-10-22 21:09:40.388393: Epoch [343/1000] [160/183], total loss: 0.00287, regularization loss: 0.29020, contrastive loss: 0.00287, Loss positive: 0.00000, Loss negative: 0.00287
2018-10-22 21:09:50.638154: Epoch [343/1000] [180/183], total loss: 0.00077, regularization loss: 0.29020, contrastive loss: 0.00077, Loss positive: 0.00000, Loss negative: 0.00077
2018-10-22 21:10:12.208827: Epoch [344/1000] [ 20/183], total loss: 0.00310, regularization loss: 0.29020, contrastive loss: 0.00310, Loss positive: 0.00000, Loss negative: 0.00310
2018-10-22 21:10:22.412768: Epoch [344/1000] [ 40/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 21:10:32.590819: Epoch [344/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:10:42.942157: Epoch [344/1000] [ 80/183], total loss: 0.00007, regularization loss: 0.29020, contrastive loss: 0.00007, Loss positive: 0.00000, Loss negative: 0.00007
2018-10-22 21:10:53.200233: Epoch [344/1000] [100/183], total loss: 0.00047, regularization loss: 0.29020, contrastive loss: 0.00047, Loss positive: 0.00000, Loss negative: 0.00047
2018-10-22 21:11:03.467485: Epoch [344/1000] [120/183], total loss: 0.00905, regularization loss: 0.29020, contrastive loss: 0.00905, Loss positive: 0.00905, Loss negative: 0.00000
2018-10-22 21:11:13.703394: Epoch [344/1000] [140/183], total loss: 0.03208, regularization loss: 0.29020, contrastive loss: 0.03208, Loss positive: 0.03097, Loss negative: 0.00112
2018-10-22 21:11:23.953903: Epoch [344/1000] [160/183], total loss: 0.00933, regularization loss: 0.29020, contrastive loss: 0.00933, Loss positive: 0.00000, Loss negative: 0.00933
2018-10-22 21:11:34.210367: Epoch [344/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:11:56.819809: Epoch [345/1000] [ 20/183], total loss: 0.00010, regularization loss: 0.29020, contrastive loss: 0.00010, Loss positive: 0.00000, Loss negative: 0.00010
2018-10-22 21:12:07.023604: Epoch [345/1000] [ 40/183], total loss: 0.01403, regularization loss: 0.29020, contrastive loss: 0.01403, Loss positive: 0.00000, Loss negative: 0.01403
2018-10-22 21:12:17.233276: Epoch [345/1000] [ 60/183], total loss: 0.00025, regularization loss: 0.29020, contrastive loss: 0.00025, Loss positive: 0.00000, Loss negative: 0.00025
2018-10-22 21:12:27.454542: Epoch [345/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:12:37.858651: Epoch [345/1000] [100/183], total loss: 0.02309, regularization loss: 0.29020, contrastive loss: 0.02309, Loss positive: 0.02309, Loss negative: 0.00000
2018-10-22 21:12:48.120185: Epoch [345/1000] [120/183], total loss: 0.00614, regularization loss: 0.29020, contrastive loss: 0.00614, Loss positive: 0.00363, Loss negative: 0.00251
2018-10-22 21:12:58.498803: Epoch [345/1000] [140/183], total loss: 0.00023, regularization loss: 0.29020, contrastive loss: 0.00023, Loss positive: 0.00000, Loss negative: 0.00023
2018-10-22 21:13:09.050477: Epoch [345/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:13:19.499698: Epoch [345/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:13:44.178580: Epoch [346/1000] [ 20/183], total loss: 0.01450, regularization loss: 0.29020, contrastive loss: 0.01450, Loss positive: 0.01450, Loss negative: 0.00000
2018-10-22 21:13:54.400081: Epoch [346/1000] [ 40/183], total loss: 0.00036, regularization loss: 0.29020, contrastive loss: 0.00036, Loss positive: 0.00000, Loss negative: 0.00036
2018-10-22 21:14:04.618304: Epoch [346/1000] [ 60/183], total loss: 0.00256, regularization loss: 0.29020, contrastive loss: 0.00256, Loss positive: 0.00000, Loss negative: 0.00256
2018-10-22 21:14:14.852621: Epoch [346/1000] [ 80/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:14:25.130094: Epoch [346/1000] [100/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:14:35.603310: Epoch [346/1000] [120/183], total loss: 0.00225, regularization loss: 0.29020, contrastive loss: 0.00225, Loss positive: 0.00000, Loss negative: 0.00225
2018-10-22 21:14:46.107720: Epoch [346/1000] [140/183], total loss: 0.01610, regularization loss: 0.29020, contrastive loss: 0.01610, Loss positive: 0.01449, Loss negative: 0.00161
2018-10-22 21:14:56.430290: Epoch [346/1000] [160/183], total loss: 0.00019, regularization loss: 0.29020, contrastive loss: 0.00019, Loss positive: 0.00000, Loss negative: 0.00019
2018-10-22 21:15:06.781719: Epoch [346/1000] [180/183], total loss: 0.01183, regularization loss: 0.29020, contrastive loss: 0.01183, Loss positive: 0.00726, Loss negative: 0.00457
2018-10-22 21:15:31.139466: Epoch [347/1000] [ 20/183], total loss: 0.00094, regularization loss: 0.29020, contrastive loss: 0.00094, Loss positive: 0.00000, Loss negative: 0.00094
2018-10-22 21:15:41.318099: Epoch [347/1000] [ 40/183], total loss: 0.04665, regularization loss: 0.29020, contrastive loss: 0.04665, Loss positive: 0.03964, Loss negative: 0.00701
2018-10-22 21:15:51.516196: Epoch [347/1000] [ 60/183], total loss: 0.02118, regularization loss: 0.29020, contrastive loss: 0.02118, Loss positive: 0.02094, Loss negative: 0.00024
2018-10-22 21:16:01.732065: Epoch [347/1000] [ 80/183], total loss: 0.05437, regularization loss: 0.29020, contrastive loss: 0.05437, Loss positive: 0.05389, Loss negative: 0.00048
2018-10-22 21:16:11.938480: Epoch [347/1000] [100/183], total loss: 0.03558, regularization loss: 0.29020, contrastive loss: 0.03558, Loss positive: 0.03210, Loss negative: 0.00348
2018-10-22 21:16:22.352004: Epoch [347/1000] [120/183], total loss: 0.00004, regularization loss: 0.29020, contrastive loss: 0.00004, Loss positive: 0.00000, Loss negative: 0.00004
2018-10-22 21:16:32.577777: Epoch [347/1000] [140/183], total loss: 0.00987, regularization loss: 0.29020, contrastive loss: 0.00987, Loss positive: 0.00562, Loss negative: 0.00424
2018-10-22 21:16:42.828461: Epoch [347/1000] [160/183], total loss: 0.01531, regularization loss: 0.29020, contrastive loss: 0.01531, Loss positive: 0.00899, Loss negative: 0.00632
2018-10-22 21:16:53.073352: Epoch [347/1000] [180/183], total loss: 0.00039, regularization loss: 0.29020, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 21:17:14.895807: Epoch [348/1000] [ 20/183], total loss: 0.02626, regularization loss: 0.29020, contrastive loss: 0.02626, Loss positive: 0.02569, Loss negative: 0.00056
2018-10-22 21:17:25.080495: Epoch [348/1000] [ 40/183], total loss: 0.00442, regularization loss: 0.29020, contrastive loss: 0.00442, Loss positive: 0.00000, Loss negative: 0.00442
2018-10-22 21:17:35.274808: Epoch [348/1000] [ 60/183], total loss: 0.05094, regularization loss: 0.29020, contrastive loss: 0.05094, Loss positive: 0.04450, Loss negative: 0.00644
2018-10-22 21:17:45.454180: Epoch [348/1000] [ 80/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 21:17:55.869529: Epoch [348/1000] [100/183], total loss: 0.00090, regularization loss: 0.29020, contrastive loss: 0.00090, Loss positive: 0.00000, Loss negative: 0.00090
2018-10-22 21:18:06.059120: Epoch [348/1000] [120/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:18:16.291150: Epoch [348/1000] [140/183], total loss: 0.00014, regularization loss: 0.29020, contrastive loss: 0.00014, Loss positive: 0.00000, Loss negative: 0.00014
2018-10-22 21:18:26.553148: Epoch [348/1000] [160/183], total loss: 0.00839, regularization loss: 0.29020, contrastive loss: 0.00839, Loss positive: 0.00471, Loss negative: 0.00368
2018-10-22 21:18:36.783856: Epoch [348/1000] [180/183], total loss: 0.00568, regularization loss: 0.29020, contrastive loss: 0.00568, Loss positive: 0.00566, Loss negative: 0.00001
2018-10-22 21:18:58.275103: Epoch [349/1000] [ 20/183], total loss: 0.00244, regularization loss: 0.29020, contrastive loss: 0.00244, Loss positive: 0.00000, Loss negative: 0.00244
2018-10-22 21:19:08.486408: Epoch [349/1000] [ 40/183], total loss: 0.00024, regularization loss: 0.29020, contrastive loss: 0.00024, Loss positive: 0.00000, Loss negative: 0.00024
2018-10-22 21:19:18.693355: Epoch [349/1000] [ 60/183], total loss: 0.00018, regularization loss: 0.29020, contrastive loss: 0.00018, Loss positive: 0.00000, Loss negative: 0.00018
2018-10-22 21:19:28.904801: Epoch [349/1000] [ 80/183], total loss: 0.00039, regularization loss: 0.29020, contrastive loss: 0.00039, Loss positive: 0.00000, Loss negative: 0.00039
2018-10-22 21:19:39.155430: Epoch [349/1000] [100/183], total loss: 0.01872, regularization loss: 0.29020, contrastive loss: 0.01872, Loss positive: 0.00000, Loss negative: 0.01872
2018-10-22 21:19:49.422009: Epoch [349/1000] [120/183], total loss: 0.00001, regularization loss: 0.29020, contrastive loss: 0.00001, Loss positive: 0.00000, Loss negative: 0.00001
2018-10-22 21:19:59.670680: Epoch [349/1000] [140/183], total loss: 0.00161, regularization loss: 0.29020, contrastive loss: 0.00161, Loss positive: 0.00000, Loss negative: 0.00161
2018-10-22 21:20:09.898277: Epoch [349/1000] [160/183], total loss: 0.00200, regularization loss: 0.29020, contrastive loss: 0.00200, Loss positive: 0.00000, Loss negative: 0.00200
2018-10-22 21:20:20.101225: Epoch [349/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:20:41.656905: Epoch [350/1000] [ 20/183], total loss: 0.00069, regularization loss: 0.29020, contrastive loss: 0.00069, Loss positive: 0.00000, Loss negative: 0.00069
2018-10-22 21:20:51.840185: Epoch [350/1000] [ 40/183], total loss: 0.00795, regularization loss: 0.29020, contrastive loss: 0.00795, Loss positive: 0.00459, Loss negative: 0.00336
2018-10-22 21:21:02.009965: Epoch [350/1000] [ 60/183], total loss: 0.00147, regularization loss: 0.29020, contrastive loss: 0.00147, Loss positive: 0.00000, Loss negative: 0.00147
2018-10-22 21:21:12.230019: Epoch [350/1000] [ 80/183], total loss: 0.00051, regularization loss: 0.29020, contrastive loss: 0.00051, Loss positive: 0.00000, Loss negative: 0.00051
2018-10-22 21:21:22.452618: Epoch [350/1000] [100/183], total loss: 0.00045, regularization loss: 0.29020, contrastive loss: 0.00045, Loss positive: 0.00000, Loss negative: 0.00045
2018-10-22 21:21:32.735950: Epoch [350/1000] [120/183], total loss: 0.01019, regularization loss: 0.29020, contrastive loss: 0.01019, Loss positive: 0.00000, Loss negative: 0.01019
2018-10-22 21:21:42.990316: Epoch [350/1000] [140/183], total loss: 0.00062, regularization loss: 0.29020, contrastive loss: 0.00062, Loss positive: 0.00000, Loss negative: 0.00062
2018-10-22 21:21:53.443698: Epoch [350/1000] [160/183], total loss: 0.00121, regularization loss: 0.29020, contrastive loss: 0.00121, Loss positive: 0.00000, Loss negative: 0.00121
2018-10-22 21:22:03.746965: Epoch [350/1000] [180/183], total loss: 0.04068, regularization loss: 0.29020, contrastive loss: 0.04068, Loss positive: 0.04068, Loss negative: 0.00000
Recall@1: 0.26772
Recall@2: 0.38910
Recall@4: 0.52701
Recall@8: 0.65665
Recall@16: 0.77920
Recall@32: 0.87441
In side function n_clusters = 100
sampler_num_per_class = [50. 60. 60. 60. 49. 60. 59. 60. 60. 60. 60. 60. 50. 60. 59. 60. 59. 60.
 59. 60. 60. 60. 60. 59. 59. 59. 60. 60. 60. 60. 60. 60. 60. 60. 59. 60.
 60. 60. 60. 60. 58. 60. 60. 60. 60. 60. 60. 60. 59. 60. 51. 60. 59. 60.
 60. 60. 59. 60. 60. 59. 60. 60. 60. 60. 60. 59. 60. 59. 59. 60. 60. 60.
 60. 60. 60. 60. 60. 56. 59. 60. 59. 60. 60. 60. 60. 60. 50. 60. 60. 58.
 60. 60. 60. 60. 60. 59. 60. 60. 60. 60.]
sampler_num_per_cluster = [ 84.  67.  50.  68.  64.  33.  77.  92.  50.  52. 101.  43.  30.  42.
  54.  54.  25.  91.  41. 112.  42.  57.  75.  37.  57.  47.  82.  69.
  39.  79.  88.  58.  36.  63.  39.  54.  52. 106.  46.  51.  55.  93.
  38.  35.  44.  59.  75. 130.  63.  33.  28.  45.  82.  35.  84.  36.
  36.  42.  46.  80.  60.  62.  28.  41.  41.  85.  83.  55.  47.  63.
  50.  66.  66.  53.  66.  40.  34. 121.  59.  26.  79. 110.  42.  49.
  75. 107.  55.  52.  23.  46.  55.  50.  84.  63.  66.  22.  42.  86.
  64.  62.]
Purity is 0.252
count_cross = [[ 0.  0.  0. ...  0. 20.  0.]
 [ 0.  0.  0. ...  0.  0.  0.]
 [ 0.  5.  5. ...  7.  3.  0.]
 ...
 [ 0.  1.  0. ...  0.  1.  0.]
 [ 0.  0.  0. ...  0.  0.  4.]
 [ 0.  0.  1. ...  0.  0.  2.]]
Mutual information is 2.12572
5924.0
5924
Entropy cluster is 4.53445
Entropy class is 4.60444
normalized_mutual_information is 0.46520
tp_and_fp = 198347.0
tp = 22649.0
fp is 175698.0
fn is 150101.0
RI is 0.981429527233528
Precision is 0.11418877018558385
Recall is 0.13110853835021707
F_1 is 0.12206512044020834

normalized_mutual_information = 0.46520330223351414
RI = 0.981429527233528
F_1 = 0.12206512044020834

The NN is 0.26772
The FT is 0.15153
The ST is 0.23421
The DCG is 0.53666
The E is 0.12726
The MAP 0.12671

2018-10-22 21:23:45.898117: Epoch [351/1000] [ 20/183], total loss: 0.00059, regularization loss: 0.29020, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 21:23:55.987256: Epoch [351/1000] [ 40/183], total loss: 0.01911, regularization loss: 0.29020, contrastive loss: 0.01911, Loss positive: 0.01908, Loss negative: 0.00003
2018-10-22 21:24:06.160065: Epoch [351/1000] [ 60/183], total loss: 0.00059, regularization loss: 0.29020, contrastive loss: 0.00059, Loss positive: 0.00000, Loss negative: 0.00059
2018-10-22 21:24:16.303656: Epoch [351/1000] [ 80/183], total loss: 0.03789, regularization loss: 0.29020, contrastive loss: 0.03789, Loss positive: 0.03001, Loss negative: 0.00789
2018-10-22 21:24:26.513588: Epoch [351/1000] [100/183], total loss: 0.00009, regularization loss: 0.29020, contrastive loss: 0.00009, Loss positive: 0.00000, Loss negative: 0.00009
2018-10-22 21:24:36.758366: Epoch [351/1000] [120/183], total loss: 0.00114, regularization loss: 0.29020, contrastive loss: 0.00114, Loss positive: 0.00000, Loss negative: 0.00114
2018-10-22 21:24:47.038976: Epoch [351/1000] [140/183], total loss: 0.01497, regularization loss: 0.29020, contrastive loss: 0.01497, Loss positive: 0.01250, Loss negative: 0.00247
2018-10-22 21:24:57.344228: Epoch [351/1000] [160/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:25:07.786951: Epoch [351/1000] [180/183], total loss: 0.00003, regularization loss: 0.29020, contrastive loss: 0.00003, Loss positive: 0.00000, Loss negative: 0.00003
2018-10-22 21:25:31.704575: Epoch [352/1000] [ 20/183], total loss: 0.00713, regularization loss: 0.29020, contrastive loss: 0.00713, Loss positive: 0.00000, Loss negative: 0.00713
2018-10-22 21:25:41.851722: Epoch [352/1000] [ 40/183], total loss: 0.00396, regularization loss: 0.29020, contrastive loss: 0.00396, Loss positive: 0.00000, Loss negative: 0.00396
2018-10-22 21:25:51.979028: Epoch [352/1000] [ 60/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:26:02.157898: Epoch [352/1000] [ 80/183], total loss: 0.00207, regularization loss: 0.29020, contrastive loss: 0.00207, Loss positive: 0.00000, Loss negative: 0.00207
2018-10-22 21:26:12.375800: Epoch [352/1000] [100/183], total loss: 0.00040, regularization loss: 0.29020, contrastive loss: 0.00040, Loss positive: 0.00000, Loss negative: 0.00040
2018-10-22 21:26:22.722447: Epoch [352/1000] [120/183], total loss: 0.00129, regularization loss: 0.29020, contrastive loss: 0.00129, Loss positive: 0.00000, Loss negative: 0.00129
2018-10-22 21:26:33.072599: Epoch [352/1000] [140/183], total loss: 0.00195, regularization loss: 0.29020, contrastive loss: 0.00195, Loss positive: 0.00000, Loss negative: 0.00195
2018-10-22 21:26:43.330059: Epoch [352/1000] [160/183], total loss: 0.00008, regularization loss: 0.29020, contrastive loss: 0.00008, Loss positive: 0.00000, Loss negative: 0.00008
2018-10-22 21:26:53.552365: Epoch [352/1000] [180/183], total loss: 0.00000, regularization loss: 0.29020, contrastive loss: 0.00000, Loss positive: 0.00000, Loss negative: 0.00000
2018-10-22 21:27:15.005177: Epoch [353/1000] [ 20/183], total loss: 0.00368, regularization loss: 0.29020, contrastive loss: 0.00368, Loss positive: 0.00000, Loss negative: 0.00368
2018-10-22 21:27:25.158040: Epoch [353/1000] [ 40/183], total loss: 0.00123, regularization loss: 0.29020, contrastive loss: 0.00123, Loss positive: 0.00000, Loss negative: 0.00123
2018-10-22 21:27:35.326447: Epoch [353/1000] [ 60/183], total loss: 0.03320, regularization loss: 0.29020, contrastive loss: 0.03320, Loss positive: 0.03302, Loss negative: 0.00018
2018-10-22 21:27:45.488082: Epoch [353/1000] [ 80/183], total loss: 0.00131, regularization loss: 0.29020, contrastive loss: 0.00131, Loss positive: 0.00000, Loss negative: 0.00131
2018-10-22 21:27:55.699268: Epoch [353/1000] [100/183], total loss: 0.02566, regularization loss: 0.29020, contrastive loss: 0.02566, Loss positive: 0.02461, Loss negative: 0.00105
2018-10-22 21:28:05.975140: Epoch [353/1000] [120/183], total loss: 0.01696, regularization loss: 0.29020, contrastive loss: 0.01696, Loss positive: 0.01694, Loss negative: 0.00002
2018-10-22 21:28:16.202291: Epoch [353/1000] [140/183], total loss: 0.02467, regularization loss: 0.29020, contrastive loss: 0.02467, Loss positive: 0.02467, Loss negative: 0.00000
2018-10-22 21:28:26.468992: Epoch [353/1000] [160/183], total loss: 0.01398, regularization loss: 0.29020, contrastive loss: 0.01398, Loss positive: 0.00000, Loss negative: 0.01398running code on the uranus
running code on the uranus
running code on the uranus
./run.sh: line 73: 16908 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --mode $MODE --optimizer "rmsprop" --batch_size 32 --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 50 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
./run.sh: line 73: 22089 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --mode $MODE --optimizer "rmsprop" --batch_size 32 --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 50 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
running code on the uranus
./run.sh: line 73: 25805 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --mode $MODE --optimizer "rmsprop" --batch_size 32 --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 50 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
./run.sh: line 73: 10797 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --mode $MODE --optimizer "rmsprop" --batch_size 32 --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 50 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
running code on the uranus
running code on the uranus
running code on the uranus
running code on the uranus
running code on the uranus
./run.sh: line 73: 17105 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --mode $MODE --optimizer "momentum" --batch_size 32 --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 50 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 111:  2221 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
running code on 102
Activate anaconda
./run.sh: line 107: 10031 Terminated  ./run.sh: line 107:  2098 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
===============
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
running code on 102
Activate anaconda
./run.sh: line 107: 10991 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 107: 10517 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
./run.sh: line 133: 29133 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
./run.sh: line 107: 31903 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 156:  1359 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 156: 21250 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 157: 23368 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "momentum" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
====================
Namespace(aux_logits=False, ckpt_dir='./checkpoint', class_num=5, dataset_name='cub200', display_step=20, dropout_keep_prob=0.5, embedding_size=128, eval_step=5, evaluation=0, focal_decay_factor=1.0, gamma=0.98, height=512, image_txt='/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/images.txt', label_txt='/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/image_class_labels.txt', learning_rate=0.01, learning_rate2=0.0001, learning_rate_decay_type='fixed', loss_type='contrastive_loss', margin=1.0, mode='train', momentum=0.01, ngpu=2, num_epochs=10000, num_epochs_per_decay=2, num_workers=4, optimizer='momentum', pair_type='vector', pretrained=False, pretrained_model_path='./weights/inception_v3.ckpt', restore_ckpt=0, root_dir='/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/images', targetNum=1000, test_batch_size=32, train_batch_size=64, train_test_split_txt='/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/train_test_split.txt', weightFile='./models/my-model', weight_decay=0.0005, width=512, with_regularizer=False)
====================
pair_type = vector
128
learning_rate = 0.010
learning_rate2 = 0.000
dataset_name = cub200    
/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/images
/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/images.txt
/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/train_test_split.txt
/data/Guoxian_Dai/CUB_200_2011/CUB_200_2011/image_class_labels.txt
Traceback (most recent call last):
  File "main.py", line 218, in <module>
    train(args)
  File "main.py", line 172, in train
    dataset_train = CubDataset(root_dir, image_txt, train_test_split_txt, label_txt, transform=transform, is_train=True, offset=1)
  File "/home/gxdai/MMVC_LARGE/Guoxian_Dai/sourcecode/FCS_pytorch/dataset.py", line 27, in __init__
    self.label_txt, self.offset
  File "/home/gxdai/MMVC_LARGE/Guoxian_Dai/sourcecode/FCS_pytorch/dataset.py", line 106, in generate_half_split_list
    raise IOError("Please check the files or paths existence")
OSError: Please check the files or paths existence
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 158: 22627 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "momentum" --pair_type "vector" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 158: 15275 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 158: 22494 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "sgd" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 158:  7882 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "sgd" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
./run.sh: line 158: 18095 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "sgd" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 09:29:44 PST 2018.txt
./run.sh: line 159: 25905 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 64 --num_epochs_per_decay 5 > "${LOSS_TYPE}_$(date).txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 22:31:57 PST 2018.txt
./run.sh: line 159:  9128 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 64 --num_epochs_per_decay 5 > "${LOSS_TYPE}_$(date).txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 23:25:49 PST 2018.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 23:27:25 PST 2018.txt
pair_type = matrix
Init the last two layers with normal distribution
128
args.optimizer = rmsprop   
learning_rate = 0.00100
learning_rate2 = 0.00010
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
./run.sh: line 159:   367 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > "${LOSS_TYPE}_$(date).txt" 2>&1
./run.sh: line 159:  1776 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 23:27:47 PST 2018.txt
pair_type = matrix
Init the last two layers with normal distribution
128
args.optimizer = rmsprop   
learning_rate = 0.00100
learning_rate2 = 0.00010
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
./run.sh: line 159:  2215 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Mon Nov 12 23:28:10 PST 2018.txt
./run.sh: line 159:  2372 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 128 --num_epochs_per_decay 5 > "${LOSS_TYPE}_$(date).txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Tue Nov 13 09:17:34 PST 2018.txt
./run.sh: line 159: 31800 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Tue Nov 13 09:26:51 PST 2018.txt
./run.sh: line 158: 18608 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "sgd" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 1.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 1 --eval_step 10 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}.txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Tue Nov 13 11:38:23 PST 2018.txt
./run.sh: line 159:  3857 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 2.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Tue Nov 13 22:23:03 PST 2018.txt
./run.sh: line 159: 11538 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 5.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Tue Nov 13 23:40:51 PST 2018.txt
./run.sh: line 159: 28807 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 5.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 20 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 00:11:14 PST 2018.txt
./run.sh: line 159: 14191 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 10.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
./run.sh: line 159: 27911 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 02:04:12 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 02:04:20 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 02:17:37 PST 2018.txt
./run.sh: line 159: 16831 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
./run.sh: line 159: 21089 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 03:28:22 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 03:28:31 PST 2018.txt
./run.sh: line 161: 21905 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
./run.sh: line 161: 21960 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 03:47:29 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focalcontrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focalcontrastive_loss_Wed Nov 14 03:47:37 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focalcontrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focalcontrastive_loss_Wed Nov 14 03:47:43 PST 2018.txt
./run.sh: line 163: 31459 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --mean 7.5 --std 2.0 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
./run.sh: line 163: 31549 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --mean 7.5 --std 2.0 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 03:48:14 PST 2018.txt
./run.sh: line 163: 32204 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --mean 7.5 --std 2.0 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
./run.sh: line 163: 31402 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin 15.0 --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 5 --embedding_size 64 --mean 7.5 --std 2.0 --num_epochs_per_decay 5 > ${LOSS_TYPE}_$(date +%m_%d_%Y_%H_%M).txt 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 20:29:09 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_Loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_Loss_Wed Nov 14 20:29:31 PST 2018.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 20:32:34 PST 2018.txt
./run.sh: line 167: 18356 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 167: 20069 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 20:40:41 PST 2018.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 20:40:51 PST 2018.txt
./run.sh: line 167: 24400 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 167: 24323 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 20:46:49 PST 2018.txt
focal_contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 20:47:27 PST 2018.txt
contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
./run.sh: line 168: 27836 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168: 27345 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 20:53:56 PST 2018.txt
contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 20:54:01 PST 2018.txt
focal_contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
./run.sh: line 168: 31140 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168: 31195 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:03:39 PST 2018.txt
focal_contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:03:46 PST 2018.txt
contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
./run.sh: line 168:  3962 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168:  3904 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:10:49 PST 2018.txt
contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:10:54 PST 2018.txt
focal_contrastive_loss_margin_10_embedding_size_64_mean_5_std_2.txt
./run.sh: line 168:  8082 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168:  8134 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:17:51 PST 2018.txt
focal_contrastive_loss_margin_15_embedding_size_64_mean_7.5_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:17:56 PST 2018.txt
contrastive_loss_margin_15_embedding_size_64_mean_7.5_std_2.txt
./run.sh: line 168: 12315 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168: 12255 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:24:52 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:24:59 PST 2018.txt
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 168: 16483 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168: 16429 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:39:38 PST 2018.txt
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:39:46 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 168: 24679 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168: 24611 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_Wed Nov 14 21:58:19 PST 2018.txt
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_5.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 21:58:31 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_5.txt
./run.sh: line 168:  1996 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
./run.sh: line 168:  1910 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:09:15 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_50.txt
./run.sh: line 168:  7780 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:15:49 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_50.txt
./run.sh: line 168:  9918 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:21:49 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_50.txt
./run.sh: line 168: 11077 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:24:28 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_50.txt
./run.sh: line 168: 12200 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean $MEAN --std $STD --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN}_std_${STD}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:37:07 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_25.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:41:35 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_15.txt
./run.sh: line 168: 15699 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 17234 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:47:03 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_150.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:47:32 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_1.txt
./run.sh: line 166: 20338 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 19874 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_Wed Nov 14 22:52:47 PST 2018.txt
contrastive_loss_margin_20_embedding_size_64_mean_10_std_150.txt
./run.sh: line 166: 22972 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 166:  2169 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 167: unexpected EOF while looking for matching `"'
./run.sh: line 174: syntax error: unexpected end of file
./run.sh: line 166:  2609 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 167: unexpected EOF while looking for matching `"'
./run.sh: line 174: syntax error: unexpected end of file
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 166: 10631 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 10686 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 166: 16854 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 16781 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on 102
Activate anaconda
DATASET_NAME = cub200
contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
Traing Info:
GPU_ID = 5,6
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on 102
Activate anaconda
DATASET_NAME = cub200
focal_contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
./run.sh: line 166: 22998 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 23042 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_128_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_128_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_128_mean_10_std_2.txt
./run.sh: line 166: 29618 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 166:  8770 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 10210 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166:  1397 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166:  1469 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
./run.sh: line 166:  2133 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166:  2187 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_256_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_256_mean_10_std_2.txt
./run.sh: line 166:  5043 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166:  5115 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
./run.sh: line 166: 17584 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 166: 17641 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
./run.sh: line 174: 26255 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 174: 26922 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 174:  2835 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 174: 30356 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 174: 30419 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 178: 18789 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 178: 18864 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 184:  8530 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 184:  8465 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 184:  2878 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 185: unexpected EOF while looking for matching `"'
./run.sh: line 192: syntax error: unexpected end of file
./run.sh: line 184:  2948 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 185: unexpected EOF while looking for matching `"'
./run.sh: line 192: syntax error: unexpected end of file
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 193: 24395 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 193: 24475 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 193: 20071 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 193: 20141 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_3.txt
./run.sh: line 193:  2341 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_triplet_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_triplet_loss_margin_20_embedding_size_64_mean_10_std_2.txt
./run.sh: line 193:  4350 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 193:  4415 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_contrastive_loss_margin_20_embedding_size_128_mean_10_std_2.txt
./run.sh: line 193: 19280 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 193: 18845 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_contrastive_loss_margin_20_embedding_size_256_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_contrastive_loss_margin_20_embedding_size_256_mean_10_std_2.txt
./run.sh: line 193: 24765 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 193: 24830 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = car196
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
car196_focal_contrastive_loss_margin_20_embedding_size_512_mean_10_std_2.txt
./run.sh: line 219:  6685 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 219:  6633 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_4.txt
./run.sh: line 219: 25124 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 
LOSS_TYPE = 
DATASET_NAME = 
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
__margin_20_embedding_size_64_mean_10_std_4.txt
Traing Info:
GPU_ID = 
LOSS_TYPE = 
DATASET_NAME = 
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
__margin_20_embedding_size_64_mean_10_std_4.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_4.txt
margin = 20.00
manual_seed = 2222.00
mean_value = 10.00
std_value =  4.00
pair_type = matrix
loss_type = focal_contrastive_loss
mode = training
weight_file = checkpoint/car196/focal_contrastive_loss/64/model_24_.pth
model_dir = ./checkpoint/cub200/focal_contrastive_loss/64
Init the last two layers with normal distribution
64
args.optimizer = rmsprop   
learning_rate = 0.00100
learning_rate2 = 0.00010
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
Finish eval
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_8.txt
margin = 20.00
manual_seed = 2222.00
mean_value = 10.00
std_value =  8.00
pair_type = matrix
loss_type = focal_contrastive_loss
mode = training
weight_file = checkpoint/car196/focal_contrastive_loss/64/model_24_.pth
model_dir = ./checkpoint/cub200/focal_contrastive_loss/64
Init the last two layers with normal distribution
64
args.optimizer = rmsprop   
learning_rate = 0.00100
learning_rate2 = 0.00010
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
training image number is 5864
testing image number is 5924
len(self.train_img_list) =  5864
len(self.train_label_list) =  5864
len(self.test_img_list) =  5924
len(self.test_label_list) =  5924
Finish eval
Recall@1: 0.32225
Recall@2: 0.45037
Recall@4: 0.58356
Recall@8: 0.70594
Recall@16: 0.81465
Recall@32: 0.88977

normalized_mutual_information = 0.45489767490646427
RI = 0.9812642278586903
F_1 = 0.12332672781439115

The NN is 0.32225
The FT is 0.14565
The ST is 0.21604
The DCG is 0.52663
The E is 0.12615
The MAP 0.11117

Recall@1: 0.32225
Recall@2: 0.45037
Recall@4: 0.58356
Recall@8: 0.70594
Recall@16: 0.81465
Recall@32: 0.88977

normalized_mutual_information = 0.45489767490646427
RI = 0.9812642278586903
F_1 = 0.12332672781439115

The NN is 0.32225
The FT is 0.14565
The ST is 0.21604
The DCG is 0.52663
The E is 0.12615
The MAP 0.11117

2018-11-16 07:30:18.930608, Epoch [  0/10000], Iter [ 20/ 92], Loss: 8.34610, Positive loss: 6.51727, Negative loss: 1.82882
2018-11-16 07:30:30.399116, Epoch [  0/10000], Iter [ 40/ 92], Loss: 7.10807, Positive loss: 5.19853, Negative loss: 1.90954
2018-11-16 07:30:42.293898, Epoch [  0/10000], Iter [ 60/ 92], Loss: 5.19818, Positive loss: 3.52910, Negative loss: 1.66908
2018-11-16 07:30:54.448782, Epoch [  0/10000], Iter [ 80/ 92], Loss: 6.24031, Positive loss: 5.09934, Negative loss: 1.14097
Start evalution
2018-11-16 07:30:35.780169, Epoch [  0/10000], Iter [ 20/ 92], Loss: 6.35723, Positive loss: 3.40866, Negative loss: 2.94857
2018-11-16 07:30:47.383680, Epoch [  0/10000], Iter [ 40/ 92], Loss: 6.13507, Positive loss: 4.22074, Negative loss: 1.91433
2018-11-16 07:30:59.046004, Epoch [  0/10000], Iter [ 60/ 92], Loss: 5.93711, Positive loss: 4.40733, Negative loss: 1.52978
2018-11-16 07:31:10.461802, Epoch [  0/10000], Iter [ 80/ 92], Loss: 4.13174, Positive loss: 2.19132, Negative loss: 1.94042
Start evalution
Recall@1: 0.27718
Recall@2: 0.39500
Recall@4: 0.52971
Recall@8: 0.66999
Recall@16: 0.79288
Recall@32: 0.88065

normalized_mutual_information = 0.4639269651281954
RI = 0.9816493184022778
F_1 = 0.12416719942326264

The NN is 0.27718
The FT is 0.15174
The ST is 0.23781
The DCG is 0.53826
The E is 0.12612
The MAP 0.12876

Recall@1: 0.29895
Recall@2: 0.41762
Recall@4: 0.54676
Recall@8: 0.67505
Recall@16: 0.79541
Recall@32: 0.88521

normalized_mutual_information = 0.4783515827917813
RI = 0.9818391846842035
F_1 = 0.1372776258427879

The NN is 0.29895
The FT is 0.16642
The ST is 0.25811
The DCG is 0.55183
The E is 0.13977
The MAP 0.14406

2018-11-16 07:32:16.446085, Epoch [  1/10000], Iter [ 20/ 92], Loss: 3.92114, Positive loss: 2.98845, Negative loss: 0.93269
2018-11-16 07:32:28.056461, Epoch [  1/10000], Iter [ 40/ 92], Loss: 4.50804, Positive loss: 3.20307, Negative loss: 1.30497
2018-11-16 07:32:40.331228, Epoch [  1/10000], Iter [ 60/ 92], Loss: 3.42575, Positive loss: 2.60689, Negative loss: 0.81886
2018-11-16 07:32:52.681303, Epoch [  1/10000], Iter [ 80/ 92], Loss: 3.98899, Positive loss: 3.14605, Negative loss: 0.84294
2018-11-16 07:32:34.462588, Epoch [  1/10000], Iter [ 20/ 92], Loss: 3.52471, Positive loss: 2.29744, Negative loss: 1.22727
2018-11-16 07:32:46.082552, Epoch [  1/10000], Iter [ 40/ 92], Loss: 3.39865, Positive loss: 2.41457, Negative loss: 0.98408
2018-11-16 07:32:57.632592, Epoch [  1/10000], Iter [ 60/ 92], Loss: 3.87200, Positive loss: 3.13738, Negative loss: 0.73462
2018-11-16 07:33:09.586448, Epoch [  1/10000], Iter [ 80/ 92], Loss: 2.40268, Positive loss: 1.58380, Negative loss: 0.81888
2018-11-16 07:33:14.557216, Epoch [  2/10000], Iter [ 20/ 92], Loss: 2.72712, Positive loss: 2.20265, Negative loss: 0.52446
2018-11-16 07:33:27.054891, Epoch [  2/10000], Iter [ 40/ 92], Loss: 3.58994, Positive loss: 2.77583, Negative loss: 0.81411
2018-11-16 07:33:39.345108, Epoch [  2/10000], Iter [ 60/ 92], Loss: 2.38876, Positive loss: 1.68313, Negative loss: 0.70563
2018-11-16 07:33:51.651215, Epoch [  2/10000], Iter [ 80/ 92], Loss: 2.99208, Positive loss: 2.32782, Negative loss: 0.66426
2018-11-16 07:33:30.576570, Epoch [  2/10000], Iter [ 20/ 92], Loss: 2.82044, Positive loss: 2.00348, Negative loss: 0.81696
2018-11-16 07:33:42.849329, Epoch [  2/10000], Iter [ 40/ 92], Loss: 2.50568, Positive loss: 1.82697, Negative loss: 0.67872
2018-11-16 07:33:55.136638, Epoch [  2/10000], Iter [ 60/ 92], Loss: 2.66631, Positive loss: 1.95503, Negative loss: 0.71129
2018-11-16 07:34:07.483587, Epoch [  2/10000], Iter [ 80/ 92], Loss: 1.77301, Positive loss: 1.04406, Negative loss: 0.72895
2018-11-16 07:34:13.120014, Epoch [  3/10000], Iter [ 20/ 92], Loss: 2.04561, Positive loss: 1.65459, Negative loss: 0.39102
2018-11-16 07:34:25.420909, Epoch [  3/10000], Iter [ 40/ 92], Loss: 2.72047, Positive loss: 1.85927, Negative loss: 0.86120
2018-11-16 07:34:37.709235, Epoch [  3/10000], Iter [ 60/ 92], Loss: 2.13879, Positive loss: 1.50452, Negative loss: 0.63427
2018-11-16 07:34:49.988521, Epoch [  3/10000], Iter [ 80/ 92], Loss: 2.04173, Positive loss: 1.66987, Negative loss: 0.37186
Start evalution
2018-11-16 07:34:28.765145, Epoch [  3/10000], Iter [ 20/ 92], Loss: 1.90013, Positive loss: 1.39731, Negative loss: 0.50282
2018-11-16 07:34:41.049099, Epoch [  3/10000], Iter [ 40/ 92], Loss: 1.96087, Positive loss: 1.52949, Negative loss: 0.43138
2018-11-16 07:34:53.345917, Epoch [  3/10000], Iter [ 60/ 92], Loss: 2.18437, Positive loss: 1.72865, Negative loss: 0.45572
2018-11-16 07:35:05.590621, Epoch [  3/10000], Iter [ 80/ 92], Loss: 1.34334, Positive loss: 0.94758, Negative loss: 0.39576
Start evalution
Recall@1: 0.30284
Recall@2: 0.42100
Recall@4: 0.55655
Recall@8: 0.69058
Recall@16: 0.80638
Recall@32: 0.89045

normalized_mutual_information = 0.4924254765601501
RI = 0.9821323915752951
F_1 = 0.14520694375514703

The NN is 0.30284
The FT is 0.17146
The ST is 0.26616
The DCG is 0.55779
The E is 0.14250
The MAP 0.14975

Recall@1: 0.29321
Recall@2: 0.40766
Recall@4: 0.54878
Recall@8: 0.68248
Recall@16: 0.79507
Recall@32: 0.88774

normalized_mutual_information = 0.49128885191161525
RI = 0.9823548047341285
F_1 = 0.1465286699713824

The NN is 0.29321
The FT is 0.17310
The ST is 0.26867
The DCG is 0.55748
The E is 0.14372
The MAP 0.15174

2018-11-16 07:36:11.899544, Epoch [  4/10000], Iter [ 20/ 92], Loss: 1.46088, Positive loss: 1.13463, Negative loss: 0.32625
2018-11-16 07:36:24.008804, Epoch [  4/10000], Iter [ 40/ 92], Loss: 2.15659, Positive loss: 1.72572, Negative loss: 0.43087
2018-11-16 07:36:36.335883, Epoch [  4/10000], Iter [ 60/ 92], Loss: 1.89511, Positive loss: 1.27557, Negative loss: 0.61954
2018-11-16 07:36:48.565932, Epoch [  4/10000], Iter [ 80/ 92], Loss: 1.82661, Positive loss: 1.33631, Negative loss: 0.49030
2018-11-16 07:36:29.886012, Epoch [  4/10000], Iter [ 20/ 92], Loss: 1.63131, Positive loss: 1.21035, Negative loss: 0.42096
2018-11-16 07:36:41.515645, Epoch [  4/10000], Iter [ 40/ 92], Loss: 1.79451, Positive loss: 1.32375, Negative loss: 0.47076
2018-11-16 07:36:53.547062, Epoch [  4/10000], Iter [ 60/ 92], Loss: 2.17534, Positive loss: 1.86590, Negative loss: 0.30943
2018-11-16 07:37:05.846322, Epoch [  4/10000], Iter [ 80/ 92], Loss: 1.13338, Positive loss: 0.84576, Negative loss: 0.28762
2018-11-16 07:37:10.052759, Epoch [  5/10000], Iter [ 20/ 92], Loss: 1.18202, Positive loss: 0.85912, Negative loss: 0.32290
2018-11-16 07:37:22.361007, Epoch [  5/10000], Iter [ 40/ 92], Loss: 1.63909, Positive loss: 1.25589, Negative loss: 0.38320
2018-11-16 07:37:34.776701, Epoch [  5/10000], Iter [ 60/ 92], Loss: 1.37111, Positive loss: 0.89999, Negative loss: 0.47112
2018-11-16 07:37:47.172023, Epoch [  5/10000], Iter [ 80/ 92], Loss: 1.39064, Positive loss: 1.11091, Negative loss: 0.27973
2018-11-16 07:37:27.311018, Epoch [  5/10000], Iter [ 20/ 92], Loss: 1.46340, Positive loss: 1.11190, Negative loss: 0.35150
2018-11-16 07:37:39.635022, Epoch [  5/10000], Iter [ 40/ 92], Loss: 1.42006, Positive loss: 0.94411, Negative loss: 0.47595
2018-11-16 07:37:51.970901, Epoch [  5/10000], Iter [ 60/ 92], Loss: 1.61886, Positive loss: 1.21056, Negative loss: 0.40830
2018-11-16 07:38:04.227265, Epoch [  5/10000], Iter [ 80/ 92], Loss: 1.14744, Positive loss: 0.69393, Negative loss: 0.45352
2018-11-16 07:38:08.659876, Epoch [  6/10000], Iter [ 20/ 92], Loss: 1.20876, Positive loss: 0.93823, Negative loss: 0.27053
2018-11-16 07:38:20.955537, Epoch [  6/10000], Iter [ 40/ 92], Loss: 1.70436, Positive loss: 1.15335, Negative loss: 0.55102
2018-11-16 07:38:33.421029, Epoch [  6/10000], Iter [ 60/ 92], Loss: 1.02824, Positive loss: 0.67909, Negative loss: 0.34915
2018-11-16 07:38:45.940214, Epoch [  6/10000], Iter [ 80/ 92], Loss: 1.43281, Positive loss: 1.10966, Negative loss: 0.32314
Start evalution
2018-11-16 07:38:25.533055, Epoch [  6/10000], Iter [ 20/ 92], Loss: 1.18194, Positive loss: 0.70342, Negative loss: 0.47852
2018-11-16 07:38:37.840374, Epoch [  6/10000], Iter [ 40/ 92], Loss: 1.22753, Positive loss: 0.90617, Negative loss: 0.32135
2018-11-16 07:38:50.095656, Epoch [  6/10000], Iter [ 60/ 92], Loss: 1.58986, Positive loss: 1.29528, Negative loss: 0.29458
2018-11-16 07:39:02.342564, Epoch [  6/10000], Iter [ 80/ 92], Loss: 0.91620, Positive loss: 0.59779, Negative loss: 0.31841
Start evalution
Recall@1: 0.31685
Recall@2: 0.43704
Recall@4: 0.56668
Recall@8: 0.70003
Recall@16: 0.80908
Recall@32: 0.89551

normalized_mutual_information = 0.5057893202180515
RI = 0.9825381730406295
F_1 = 0.15471976116306083

The NN is 0.31685
The FT is 0.18336
The ST is 0.28300
The DCG is 0.56807
The E is 0.15325
The MAP 0.16140

Recall@1: 0.30030
Recall@2: 0.41492
Recall@4: 0.54608
Recall@8: 0.68248
Recall@16: 0.80182
Recall@32: 0.88707

normalized_mutual_information = 0.500511025101817
RI = 0.9821781054024054
F_1 = 0.15092249118786016

The NN is 0.30030
The FT is 0.17849
The ST is 0.27488
The DCG is 0.56204
The E is 0.14859
The MAP 0.15700

2018-11-16 07:40:07.636691, Epoch [  7/10000], Iter [ 20/ 92], Loss: 1.10975, Positive loss: 0.73592, Negative loss: 0.37383
2018-11-16 07:40:19.936417, Epoch [  7/10000], Iter [ 40/ 92], Loss: 1.45871, Positive loss: 0.97169, Negative loss: 0.48702
2018-11-16 07:40:32.232754, Epoch [  7/10000], Iter [ 60/ 92], Loss: 1.13507, Positive loss: 0.72021, Negative loss: 0.41486
2018-11-16 07:40:44.544053, Epoch [  7/10000], Iter [ 80/ 92], Loss: 1.13312, Positive loss: 0.88135, Negative loss: 0.25177
2018-11-16 07:40:24.812548, Epoch [  7/10000], Iter [ 20/ 92], Loss: 0.90083, Positive loss: 0.56888, Negative loss: 0.33196
2018-11-16 07:40:36.582798, Epoch [  7/10000], Iter [ 40/ 92], Loss: 1.00747, Positive loss: 0.79371, Negative loss: 0.21376
2018-11-16 07:40:48.650243, Epoch [  7/10000], Iter [ 60/ 92], Loss: 1.34986, Positive loss: 1.03734, Negative loss: 0.31252
2018-11-16 07:41:00.971191, Epoch [  7/10000], Iter [ 80/ 92], Loss: 1.10352, Positive loss: 0.84951, Negative loss: 0.25401
2018-11-16 07:41:06.030683, Epoch [  8/10000], Iter [ 20/ 92], Loss: 0.84292, Positive loss: 0.61811, Negative loss: 0.22481
2018-11-16 07:41:18.297372, Epoch [  8/10000], Iter [ 40/ 92], Loss: 1.57355, Positive loss: 1.14497, Negative loss: 0.42858
2018-11-16 07:41:30.610985, Epoch [  8/10000], Iter [ 60/ 92], Loss: 1.23744, Positive loss: 1.04034, Negative loss: 0.19710
2018-11-16 07:41:42.883839, Epoch [  8/10000], Iter [ 80/ 92], Loss: 0.94793, Positive loss: 0.76028, Negative loss: 0.18765
2018-11-16 07:41:22.452013, Epoch [  8/10000], Iter [ 20/ 92], Loss: 0.91435, Positive loss: 0.54896, Negative loss: 0.36539
2018-11-16 07:41:34.738144, Epoch [  8/10000], Iter [ 40/ 92], Loss: 1.05205, Positive loss: 0.85301, Negative loss: 0.19904
2018-11-16 07:41:47.066048, Epoch [  8/10000], Iter [ 60/ 92], Loss: 1.12567, Positive loss: 0.91948, Negative loss: 0.20619
2018-11-16 07:41:59.308523, Epoch [  8/10000], Iter [ 80/ 92], Loss: 0.91448, Positive loss: 0.69530, Negative loss: 0.21918
2018-11-16 07:42:04.400985, Epoch [  9/10000], Iter [ 20/ 92], Loss: 0.96528, Positive loss: 0.80199, Negative loss: 0.16330
2018-11-16 07:42:16.910555, Epoch [  9/10000], Iter [ 40/ 92], Loss: 1.24322, Positive loss: 0.95322, Negative loss: 0.29000
2018-11-16 07:42:29.389466, Epoch [  9/10000], Iter [ 60/ 92], Loss: 1.17769, Positive loss: 0.85145, Negative loss: 0.32624
2018-11-16 07:42:41.693434, Epoch [  9/10000], Iter [ 80/ 92], Loss: 1.12409, Positive loss: 0.95382, Negative loss: 0.17028
Start evalution
2018-11-16 07:42:21.258599, Epoch [  9/10000], Iter [ 20/ 92], Loss: 0.76566, Positive loss: 0.56031, Negative loss: 0.20535
2018-11-16 07:42:33.522658, Epoch [  9/10000], Iter [ 40/ 92], Loss: 0.79142, Positive loss: 0.55808, Negative loss: 0.23334
2018-11-16 07:42:45.803606, Epoch [  9/10000], Iter [ 60/ 92], Loss: 1.04338, Positive loss: 0.66332, Negative loss: 0.38007
2018-11-16 07:42:58.105269, Epoch [  9/10000], Iter [ 80/ 92], Loss: 1.00228, Positive loss: 0.72456, Negative loss: 0.27772
Start evalution
Recall@1: 0.30976
Recall@2: 0.43062
Recall@4: 0.56009
Recall@8: 0.70054
Recall@16: 0.81043
Recall@32: 0.89095

normalized_mutual_information = 0.5113002750368121
RI = 0.9822072322922475
F_1 = 0.1595700858589802

The NN is 0.30976
The FT is 0.18170
The ST is 0.28090
The DCG is 0.56699
The E is 0.15116
The MAP 0.15960

Recall@1: 0.31685
Recall@2: 0.44007
Recall@4: 0.57259
Recall@8: 0.69953
Recall@16: 0.81094
Recall@32: 0.88572

normalized_mutual_information = 0.5095575513926243
RI = 0.9818780015373982
F_1 = 0.1550612979267933

The NN is 0.31685
The FT is 0.18370
The ST is 0.28264
The DCG is 0.56756
The E is 0.15387
The MAP 0.16197

2018-11-16 07:44:03.811231, Epoch [ 10/10000], Iter [ 20/ 92], Loss: 0.76708, Positive loss: 0.66585, Negative loss: 0.10123
2018-11-16 07:44:15.680948, Epoch [ 10/10000], Iter [ 40/ 92], Loss: 0.91211, Positive loss: 0.65419, Negative loss: 0.25793
2018-11-16 07:44:27.963465, Epoch [ 10/10000], Iter [ 60/ 92], Loss: 0.77029, Positive loss: 0.44520, Negative loss: 0.32509
2018-11-16 07:44:40.291731, Epoch [ 10/10000], Iter [ 80/ 92], Loss: 1.11883, Positive loss: 0.96562, Negative loss: 0.15322
2018-11-16 07:44:21.711715, Epoch [ 10/10000], Iter [ 20/ 92], Loss: 0.70916, Positive loss: 0.60398, Negative loss: 0.10518
2018-11-16 07:44:33.409751, Epoch [ 10/10000], Iter [ 40/ 92], Loss: 0.92631, Positive loss: 0.71779, Negative loss: 0.20852
2018-11-16 07:44:45.617566, Epoch [ 10/10000], Iter [ 60/ 92], Loss: 0.90379, Positive loss: 0.63921, Negative loss: 0.26458
2018-11-16 07:44:57.908086, Epoch [ 10/10000], Iter [ 80/ 92], Loss: 0.80916, Positive loss: 0.62073, Negative loss: 0.18843
2018-11-16 07:45:01.770688, Epoch [ 11/10000], Iter [ 20/ 92], Loss: 0.96054, Positive loss: 0.57938, Negative loss: 0.38116
2018-11-16 07:45:14.029109, Epoch [ 11/10000], Iter [ 40/ 92], Loss: 1.08214, Positive loss: 0.97510, Negative loss: 0.10704
2018-11-16 07:45:26.319941, Epoch [ 11/10000], Iter [ 60/ 92], Loss: 0.62799, Positive loss: 0.42601, Negative loss: 0.20197
2018-11-16 07:45:38.833781, Epoch [ 11/10000], Iter [ 80/ 92], Loss: 1.07988, Positive loss: 0.90462, Negative loss: 0.17525
2018-11-16 07:45:19.185257, Epoch [ 11/10000], Iter [ 20/ 92], Loss: 0.69143, Positive loss: 0.49044, Negative loss: 0.20099
2018-11-16 07:45:31.479155, Epoch [ 11/10000], Iter [ 40/ 92], Loss: 0.90473, Positive loss: 0.51648, Negative loss: 0.38824
2018-11-16 07:45:43.785742, Epoch [ 11/10000], Iter [ 60/ 92], Loss: 0.64536, Positive loss: 0.49716, Negative loss: 0.14820
2018-11-16 07:45:56.346716, Epoch [ 11/10000], Iter [ 80/ 92], Loss: 0.65328, Positive loss: 0.39274, Negative loss: 0.26053
2018-11-16 07:46:00.147590, Epoch [ 12/10000], Iter [ 20/ 92], Loss: 0.69245, Positive loss: 0.56369, Negative loss: 0.12876
2018-11-16 07:46:12.437341, Epoch [ 12/10000], Iter [ 40/ 92], Loss: 1.04119, Positive loss: 0.80235, Negative loss: 0.23883
2018-11-16 07:46:24.713299, Epoch [ 12/10000], Iter [ 60/ 92], Loss: 0.59814, Positive loss: 0.36939, Negative loss: 0.22875
2018-11-16 07:46:36.993485, Epoch [ 12/10000], Iter [ 80/ 92], Loss: 0.83066, Positive loss: 0.69474, Negative loss: 0.13592
Start evalution
2018-11-16 07:46:17.573669, Epoch [ 12/10000], Iter [ 20/ 92], Loss: 0.65932, Positive loss: 0.46143, Negative loss: 0.19789
2018-11-16 07:46:29.923790, Epoch [ 12/10000], Iter [ 40/ 92], Loss: 0.65776, Positive loss: 0.55004, Negative loss: 0.10772
2018-11-16 07:46:42.116891, Epoch [ 12/10000], Iter [ 60/ 92], Loss: 0.59425, Positive loss: 0.48294, Negative loss: 0.11131
2018-11-16 07:46:54.441776, Epoch [ 12/10000], Iter [ 80/ 92], Loss: 0.61905, Positive loss: 0.39280, Negative loss: 0.22625
Start evalution
Recall@1: 0.31296
Recall@2: 0.43332
Recall@4: 0.56077
Recall@8: 0.69733
Recall@16: 0.81128
Recall@32: 0.89551

normalized_mutual_information = 0.5059103905819095
RI = 0.9822246742262821
F_1 = 0.15920322892878616

The NN is 0.31296
The FT is 0.18385
The ST is 0.28419
The DCG is 0.56731
The E is 0.15237
The MAP 0.16298

Recall@1: 0.31685
Recall@2: 0.44294
Recall@4: 0.56853
Recall@8: 0.69548
Recall@16: 0.80486
Recall@32: 0.88454

normalized_mutual_information = 0.5096386122419398
RI = 0.981852066635484
F_1 = 0.15843478904436914

The NN is 0.31685
The FT is 0.18934
The ST is 0.28931
The DCG is 0.57086
The E is 0.15862
The MAP 0.16780

2018-11-16 07:47:57.877286, Epoch [ 13/10000], Iter [ 20/ 92], Loss: 0.56077, Positive loss: 0.50729, Negative loss: 0.05348
2018-11-16 07:48:09.974644, Epoch [ 13/10000], Iter [ 40/ 92], Loss: 0.88829, Positive loss: 0.82613, Negative loss: 0.06216
2018-11-16 07:48:22.455847, Epoch [ 13/10000], Iter [ 60/ 92], Loss: 0.63894, Positive loss: 0.25400, Negative loss: 0.38494
2018-11-16 07:48:34.773997, Epoch [ 13/10000], Iter [ 80/ 92], Loss: 0.61905, Positive loss: 0.49024, Negative loss: 0.12881
2018-11-16 07:48:16.400813, Epoch [ 13/10000], Iter [ 20/ 92], Loss: 0.42907, Positive loss: 0.37857, Negative loss: 0.05050
2018-11-16 07:48:28.122509, Epoch [ 13/10000], Iter [ 40/ 92], Loss: 0.53952, Positive loss: 0.42520, Negative loss: 0.11432
2018-11-16 07:48:40.328580, Epoch [ 13/10000], Iter [ 60/ 92], Loss: 0.74064, Positive loss: 0.62031, Negative loss: 0.12033
2018-11-16 07:48:52.603239, Epoch [ 13/10000], Iter [ 80/ 92], Loss: 0.56804, Positive loss: 0.49269, Negative loss: 0.07535
2018-11-16 07:48:56.454531, Epoch [ 14/10000], Iter [ 20/ 92], Loss: 0.57249, Positive loss: 0.38524, Negative loss: 0.18726
2018-11-16 07:49:08.719310, Epoch [ 14/10000], Iter [ 40/ 92], Loss: 0.79127, Positive loss: 0.62243, Negative loss: 0.16884
2018-11-16 07:49:21.067575, Epoch [ 14/10000], Iter [ 60/ 92], Loss: 0.61454, Positive loss: 0.28532, Negative loss: 0.32922
2018-11-16 07:49:33.399274, Epoch [ 14/10000], Iter [ 80/ 92], Loss: 0.43516, Positive loss: 0.38849, Negative loss: 0.04667
2018-11-16 07:49:14.127879, Epoch [ 14/10000], Iter [ 20/ 92], Loss: 0.40312, Positive loss: 0.32179, Negative loss: 0.08134
2018-11-16 07:49:26.417891, Epoch [ 14/10000], Iter [ 40/ 92], Loss: 0.54084, Positive loss: 0.44961, Negative loss: 0.09123
2018-11-16 07:49:38.740016, Epoch [ 14/10000], Iter [ 60/ 92], Loss: 0.75558, Positive loss: 0.55503, Negative loss: 0.20056
2018-11-16 07:49:51.060160, Epoch [ 14/10000], Iter [ 80/ 92], Loss: 0.51672, Positive loss: 0.32717, Negative loss: 0.18955
2018-11-16 07:49:55.038038, Epoch [ 15/10000], Iter [ 20/ 92], Loss: 0.40524, Positive loss: 0.29448, Negative loss: 0.11076
2018-11-16 07:50:07.333648, Epoch [ 15/10000], Iter [ 40/ 92], Loss: 0.57914, Positive loss: 0.47582, Negative loss: 0.10332
2018-11-16 07:50:19.623497, Epoch [ 15/10000], Iter [ 60/ 92], Loss: 0.42722, Positive loss: 0.28866, Negative loss: 0.13857
2018-11-16 07:50:31.903684, Epoch [ 15/10000], Iter [ 80/ 92], Loss: 0.38793, Positive loss: 0.34737, Negative loss: 0.04055
Start evalution
2018-11-16 07:50:12.594296, Epoch [ 15/10000], Iter [ 20/ 92], Loss: 0.41481, Positive loss: 0.27279, Negative loss: 0.14202
2018-11-16 07:50:25.039849, Epoch [ 15/10000], Iter [ 40/ 92], Loss: 0.62544, Positive loss: 0.48905, Negative loss: 0.13639
2018-11-16 07:50:37.314665, Epoch [ 15/10000], Iter [ 60/ 92], Loss: 0.66931, Positive loss: 0.49703, Negative loss: 0.17229
2018-11-16 07:50:49.537026, Epoch [ 15/10000], Iter [ 80/ 92], Loss: 0.39422, Positive loss: 0.32221, Negative loss: 0.07201
Start evalution
Recall@1: 0.32343
Recall@2: 0.44716
Recall@4: 0.58660
Recall@8: 0.70932
Recall@16: 0.81533
Recall@32: 0.89635

normalized_mutual_information = 0.5190178746526946
RI = 0.9825794408845546
F_1 = 0.16616420528798542

The NN is 0.32343
The FT is 0.19043
The ST is 0.29374
The DCG is 0.57457
The E is 0.15946
The MAP 0.17008

Recall@1: 0.32799
Recall@2: 0.44497
Recall@4: 0.56904
Recall@8: 0.69750
Recall@16: 0.80908
Recall@32: 0.89348

normalized_mutual_information = 0.5166959346079509
RI = 0.982239494170233
F_1 = 0.16712152275661502

The NN is 0.32799
The FT is 0.19212
The ST is 0.29289
The DCG is 0.57498
The E is 0.16146
The MAP 0.17266

2018-11-16 07:51:53.052975, Epoch [ 16/10000], Iter [ 20/ 92], Loss: 0.37196, Positive loss: 0.32864, Negative loss: 0.04332
2018-11-16 07:52:05.111114, Epoch [ 16/10000], Iter [ 40/ 92], Loss: 0.52516, Positive loss: 0.33624, Negative loss: 0.18892
2018-11-16 07:52:17.528005, Epoch [ 16/10000], Iter [ 60/ 92], Loss: 0.49070, Positive loss: 0.22294, Negative loss: 0.26776
2018-11-16 07:52:29.883318, Epoch [ 16/10000], Iter [ 80/ 92], Loss: 0.44320, Positive loss: 0.40286, Negative loss: 0.04033
2018-11-16 07:52:11.348870, Epoch [ 16/10000], Iter [ 20/ 92], Loss: 0.44477, Positive loss: 0.31682, Negative loss: 0.12796
2018-11-16 07:52:22.998333, Epoch [ 16/10000], Iter [ 40/ 92], Loss: 0.60298, Positive loss: 0.47363, Negative loss: 0.12935
2018-11-16 07:52:35.237926, Epoch [ 16/10000], Iter [ 60/ 92], Loss: 0.54930, Positive loss: 0.44881, Negative loss: 0.10049
2018-11-16 07:52:47.594140, Epoch [ 16/10000], Iter [ 80/ 92], Loss: 0.31536, Positive loss: 0.25226, Negative loss: 0.06310
2018-11-16 07:52:51.375749, Epoch [ 17/10000], Iter [ 20/ 92], Loss: 0.48199, Positive loss: 0.42692, Negative loss: 0.05506
2018-11-16 07:53:03.701458, Epoch [ 17/10000], Iter [ 40/ 92], Loss: 0.44562, Positive loss: 0.30271, Negative loss: 0.14291
2018-11-16 07:53:15.989326, Epoch [ 17/10000], Iter [ 60/ 92], Loss: 0.43451, Positive loss: 0.37919, Negative loss: 0.05532
2018-11-16 07:53:28.250553, Epoch [ 17/10000], Iter [ 80/ 92], Loss: 0.37628, Positive loss: 0.28040, Negative loss: 0.09589
2018-11-16 07:53:08.827421, Epoch [ 17/10000], Iter [ 20/ 92], Loss: 0.40377, Positive loss: 0.28601, Negative loss: 0.11777
2018-11-16 07:53:21.315032, Epoch [ 17/10000], Iter [ 40/ 92], Loss: 0.53246, Positive loss: 0.39471, Negative loss: 0.13775
2018-11-16 07:53:33.665481, Epoch [ 17/10000], Iter [ 60/ 92], Loss: 0.43093, Positive loss: 0.29829, Negative loss: 0.13263
2018-11-16 07:53:45.880307, Epoch [ 17/10000], Iter [ 80/ 92], Loss: 0.40019, Positive loss: 0.27721, Negative loss: 0.12298
2018-11-16 07:53:49.761517, Epoch [ 18/10000], Iter [ 20/ 92], Loss: 0.47714, Positive loss: 0.30969, Negative loss: 0.16745
2018-11-16 07:54:02.215885, Epoch [ 18/10000], Iter [ 40/ 92], Loss: 0.47492, Positive loss: 0.28757, Negative loss: 0.18735
2018-11-16 07:54:14.751618, Epoch [ 18/10000], Iter [ 60/ 92], Loss: 0.46717, Positive loss: 0.34310, Negative loss: 0.12407
2018-11-16 07:54:27.012953, Epoch [ 18/10000], Iter [ 80/ 92], Loss: 0.49982, Positive loss: 0.42786, Negative loss: 0.07196
Start evalution
2018-11-16 07:54:06.973691, Epoch [ 18/10000], Iter [ 20/ 92], Loss: 0.47126, Positive loss: 0.22673, Negative loss: 0.24453
2018-11-16 07:54:19.267914, Epoch [ 18/10000], Iter [ 40/ 92], Loss: 0.36621, Positive loss: 0.29630, Negative loss: 0.06990
2018-11-16 07:54:31.548798, Epoch [ 18/10000], Iter [ 60/ 92], Loss: 0.44773, Positive loss: 0.29932, Negative loss: 0.14841
2018-11-16 07:54:43.847425, Epoch [ 18/10000], Iter [ 80/ 92], Loss: 0.44283, Positive loss: 0.26673, Negative loss: 0.17610
Start evalution
Recall@1: 0.32478
Recall@2: 0.44480
Recall@4: 0.58373
Recall@8: 0.71236
Recall@16: 0.81482
Recall@32: 0.90007

normalized_mutual_information = 0.5267705608939661
RI = 0.9817958078482547
F_1 = 0.1695473208226181

The NN is 0.32478
The FT is 0.19890
The ST is 0.30341
The DCG is 0.57957
The E is 0.16638
The MAP 0.17759

Recall@1: 0.32731
Recall@2: 0.45054
Recall@4: 0.57698
Recall@8: 0.70510
Recall@16: 0.81128
Recall@32: 0.89213

normalized_mutual_information = 0.5216207796400837
RI = 0.9822354471855388
F_1 = 0.17129774889518776

The NN is 0.32731
The FT is 0.19639
The ST is 0.30141
The DCG is 0.57919
The E is 0.16448
The MAP 0.17603

2018-11-16 07:55:49.238742, Epoch [ 19/10000], Iter [ 20/ 92], Loss: 0.23560, Positive loss: 0.18515, Negative loss: 0.05045
2018-11-16 07:56:01.240397, Epoch [ 19/10000], Iter [ 40/ 92], Loss: 0.53214, Positive loss: 0.36899, Negative loss: 0.16316
2018-11-16 07:56:13.510307, Epoch [ 19/10000], Iter [ 60/ 92], Loss: 0.34846, Positive loss: 0.31020, Negative loss: 0.03826
2018-11-16 07:56:26.011712, Epoch [ 19/10000], Iter [ 80/ 92], Loss: 0.48322, Positive loss: 0.42009, Negative loss: 0.06313
2018-11-16 07:56:06.764850, Epoch [ 19/10000], Iter [ 20/ 92], Loss: 0.43099, Positive loss: 0.28801, Negative loss: 0.14297
2018-11-16 07:56:18.406232, Epoch [ 19/10000], Iter [ 40/ 92], Loss: 0.36147, Positive loss: 0.28659, Negative loss: 0.07488
2018-11-16 07:56:30.551348, Epoch [ 19/10000], Iter [ 60/ 92], Loss: 0.42728, Positive loss: 0.39200, Negative loss: 0.03527
2018-11-16 07:56:42.830757, Epoch [ 19/10000], Iter [ 80/ 92], Loss: 0.39264, Positive loss: 0.29100, Negative loss: 0.10163
2018-11-16 07:56:47.334412, Epoch [ 20/10000], Iter [ 20/ 92], Loss: 0.29233, Positive loss: 0.23141, Negative loss: 0.06093
2018-11-16 07:56:59.628536, Epoch [ 20/10000], Iter [ 40/ 92], Loss: 0.35849, Positive loss: 0.29097, Negative loss: 0.06752
2018-11-16 07:57:11.919529, Epoch [ 20/10000], Iter [ 60/ 92], Loss: 0.36064, Positive loss: 0.22279, Negative loss: 0.13785
2018-11-16 07:57:24.179134, Epoch [ 20/10000], Iter [ 80/ 92], Loss: 0.44778, Positive loss: 0.36409, Negative loss: 0.08368
2018-11-16 07:57:04.152289, Epoch [ 20/10000], Iter [ 20/ 92], Loss: 0.40317, Positive loss: 0.28322, Negative loss: 0.11995
2018-11-16 07:57:16.421885, Epoch [ 20/10000], Iter [ 40/ 92], Loss: 0.52837, Positive loss: 0.43501, Negative loss: 0.09336
2018-11-16 07:57:28.932442, Epoch [ 20/10000], Iter [ 60/ 92], Loss: 0.36479, Positive loss: 0.30716, Negative loss: 0.05763
2018-11-16 07:57:41.215420, Epoch [ 20/10000], Iter [ 80/ 92], Loss: 0.33675, Positive loss: 0.29466, Negative loss: 0.04209
2018-11-16 07:57:45.700074, Epoch [ 21/10000], Iter [ 20/ 92], Loss: 0.35034, Positive loss: 0.29748, Negative loss: 0.05285
2018-11-16 07:57:58.034458, Epoch [ 21/10000], Iter [ 40/ 92], Loss: 0.37383, Positive loss: 0.33740, Negative loss: 0.03643
2018-11-16 07:58:10.668462, Epoch [ 21/10000], Iter [ 60/ 92], Loss: 0.34916, Positive loss: 0.29551, Negative loss: 0.05365
2018-11-16 07:58:22.978653, Epoch [ 21/10000], Iter [ 80/ 92], Loss: 0.32984, Positive loss: 0.25355, Negative loss: 0.07628
Start evalution
2018-11-16 07:58:02.492222, Epoch [ 21/10000], Iter [ 20/ 92], Loss: 0.38117, Positive loss: 0.31371, Negative loss: 0.06745
2018-11-16 07:58:14.842499, Epoch [ 21/10000], Iter [ 40/ 92], Loss: 0.35377, Positive loss: 0.31190, Negative loss: 0.04187
2018-11-16 07:58:27.077982, Epoch [ 21/10000], Iter [ 60/ 92], Loss: 0.45972, Positive loss: 0.41446, Negative loss: 0.04526
2018-11-16 07:58:39.361633, Epoch [ 21/10000], Iter [ 80/ 92], Loss: 0.28338, Positive loss: 0.23188, Negative loss: 0.05150
Start evalution
Recall@1: 0.32714
Recall@2: 0.45611
Recall@4: 0.57816
Recall@8: 0.70493
Recall@16: 0.81634
Recall@32: 0.89196

normalized_mutual_information = 0.5207596628923217
RI = 0.9818587356102619
F_1 = 0.16644885195496345

The NN is 0.32714
The FT is 0.19780
The ST is 0.30062
The DCG is 0.57987
The E is 0.16566
The MAP 0.17782

Recall@1: 0.34048
Recall@2: 0.45645
Recall@4: 0.58204
Recall@8: 0.70138
Recall@16: 0.80756
Recall@32: 0.89500

normalized_mutual_information = 0.5280519099919722
RI = 0.9820946007182201
F_1 = 0.17039437371088417

The NN is 0.34048
The FT is 0.20052
The ST is 0.30613
The DCG is 0.58247
The E is 0.16833
The MAP 0.17986

2018-11-16 07:59:44.226035, Epoch [ 22/10000], Iter [ 20/ 92], Loss: 0.27914, Positive loss: 0.18102, Negative loss: 0.09813
2018-11-16 07:59:56.359711, Epoch [ 22/10000], Iter [ 40/ 92], Loss: 0.41604, Positive loss: 0.38630, Negative loss: 0.02974
2018-11-16 08:00:08.662114, Epoch [ 22/10000], Iter [ 60/ 92], Loss: 0.29880, Positive loss: 0.26090, Negative loss: 0.03790
2018-11-16 08:00:20.916783, Epoch [ 22/10000], Iter [ 80/ 92], Loss: 0.35111, Positive loss: 0.27296, Negative loss: 0.07815
2018-11-16 08:00:02.394550, Epoch [ 22/10000], Iter [ 20/ 92], Loss: 0.28643, Positive loss: 0.27213, Negative loss: 0.01429
2018-11-16 08:00:14.003092, Epoch [ 22/10000], Iter [ 40/ 92], Loss: 0.36573, Positive loss: 0.30048, Negative loss: 0.06525
2018-11-16 08:00:26.069259, Epoch [ 22/10000], Iter [ 60/ 92], Loss: 0.42559, Positive loss: 0.33746, Negative loss: 0.08813
2018-11-16 08:00:38.369618, Epoch [ 22/10000], Iter [ 80/ 92], Loss: 0.27635, Positive loss: 0.21886, Negative loss: 0.05749
2018-11-16 08:00:42.450438, Epoch [ 23/10000], Iter [ 20/ 92], Loss: 0.22437, Positive loss: 0.19109, Negative loss: 0.03328
2018-11-16 08:00:54.708400, Epoch [ 23/10000], Iter [ 40/ 92], Loss: 0.48610, Positive loss: 0.38152, Negative loss: 0.10458
2018-11-16 08:01:07.235061, Epoch [ 23/10000], Iter [ 60/ 92], Loss: 0.27152, Positive loss: 0.23214, Negative loss: 0.03938
2018-11-16 08:01:19.502967, Epoch [ 23/10000], Iter [ 80/ 92], Loss: 0.25245, Positive loss: 0.21896, Negative loss: 0.03349
2018-11-16 08:00:59.825468, Epoch [ 23/10000], Iter [ 20/ 92], Loss: 0.23635, Positive loss: 0.20336, Negative loss: 0.03299
2018-11-16 08:01:12.261298, Epoch [ 23/10000], Iter [ 40/ 92], Loss: 0.28195, Positive loss: 0.22627, Negative loss: 0.05569
2018-11-16 08:01:24.652711, Epoch [ 23/10000], Iter [ 60/ 92], Loss: 0.33760, Positive loss: 0.28734, Negative loss: 0.05026
2018-11-16 08:01:36.966145, Epoch [ 23/10000], Iter [ 80/ 92], Loss: 0.34926, Positive loss: 0.31941, Negative loss: 0.02985
./run.sh: line 233: 25885 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5
./run.sh: line 233: 25996 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_2.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_4.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_8.txt
./run.sh: line 232: 10786 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 232: 11129 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_4.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_8.txt
./run.sh: line 232: 12034 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 232: 12120 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_16.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_1.txt
./run.sh: line 232: 14563 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 232: 14651 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.5.txt
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.5.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.25.txt
./run.sh: line 232: 27489 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 232: 27309 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.1.txt
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.05.txt
./run.sh: line 232: 23549 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.00001.txt
./run.sh: line 232: 23470 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 1,3
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.01.txt
./run.sh: line 232: 27111 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
Traing Info:
GPU_ID = 0,2
LOSS_TYPE = focal_contrastive_loss
DATASET_NAME = cub200
running code on the uranus
==========================
Activate virutalenv
START >>>>>>>>>>>>>>>>>>>
cub200_focal_contrastive_loss_margin_20_embedding_size_64_mean_10_std_0.001.txt
./run.sh: line 232: 32048 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
./run.sh: line 232: 31454 Terminated              CUDA_VISIBLE_DEVICES=$GPU_ID $PYTHON main.py --dataset_name $DATASET_NAME --mode $MODE --weight_file $WEIGHT_FILE --manual_seed $MANUAL_SEED --optimizer "rmsprop" --pair_type "matrix" --train_batch_size $TRAIN_BATCH_SIZE --momentum 0.9 --learning_rate $LEARNING_RATE --learning_rate2 ${LEARNING_RATE2} --learning_rate_decay_type "fixed" --loss_type $LOSS_TYPE --margin $MARGIN --root_dir $ROOT_DIR --image_txt $IMAGE_TXT --train_test_split_txt $TRAIN_TEST_SPLIT_TXT --label_txt $LABEL_TXT --focal_decay_factor "1000000000.0" --display_step 20 --eval_step 3 --embedding_size $EMBEDDING_SIZE --mean_value $MEAN_VALUE --std_value $STD_VALUE --num_epochs_per_decay 5 > "${DATASET_NAME}_${LOSS_TYPE}_margin_${MARGIN}_embedding_size_${EMBEDDING_SIZE}_mean_${MEAN_VALUE}_std_${STD_VALUE}.txt" 2>&1
